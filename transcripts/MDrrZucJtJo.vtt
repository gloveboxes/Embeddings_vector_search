WEBVTT

00:00:11.109 --> 00:00:18.649
Caption: so it&#39;s now my absolute pleasure to

00:00:14.829 --> 00:00:22.219
Caption: welcome our amazing keynote speaker to

00:00:18.649 --> 00:00:23.689
Caption: this stage I think it&#39;s fair to say the

00:00:22.219 --> 00:00:27.139
Caption: entire committee was really really

00:00:23.690 --> 00:00:27.950
Caption: excited when we confirmed that Karolina

00:00:27.139 --> 00:00:31.849
Caption: stirrer

00:00:27.950 --> 00:00:33.440
Caption: I hope icon is gonna be our keynote

00:00:31.850 --> 00:00:35.450
Caption: speaker because she&#39;s someone that we&#39;ve

00:00:33.439 --> 00:00:38.209
Caption: all been watching and admiring for a

00:00:35.450 --> 00:00:39.529
Caption: long time and certainly a lot of the

00:00:38.209 --> 00:00:41.329
Caption: things that we&#39;ve done as a committee

00:00:39.529 --> 00:00:43.159
Caption: we&#39;ve sort of tried to look to her and

00:00:41.330 --> 00:00:44.480
Caption: the stuff that she talks about in

00:00:43.159 --> 00:00:47.738
Caption: organizing conferences that are

00:00:44.479 --> 00:00:49.639
Caption: inclusive as guidance so today

00:00:47.738 --> 00:00:52.430
Caption: Karolina is going to be talking about

00:00:49.639 --> 00:00:54.259
Caption: towards a welcoming web and I&#39;d like

00:00:52.430 --> 00:00:56.599
Caption: everyone to give her an enormous round

00:00:54.259 --> 00:00:57.930
Caption: of applause and she are to welcome her

00:00:56.599 --> 00:01:07.080
Caption: onto the stage

00:00:57.930 --> 00:01:11.369
Caption: [Applause]

00:01:07.080 --> 00:01:11.369
Caption: hello is this on

00:01:16.860 --> 00:01:26.399
Caption: hello Perth how are you that&#39;s a little

00:01:22.959 --> 00:01:28.478
Caption: bit quiet have you had any coffee today

00:01:26.399 --> 00:01:33.690
Caption: how about we go one more time

00:01:28.478 --> 00:01:33.689
Caption: hello Perth that&#39;s better

00:01:35.289 --> 00:01:40.990
Caption: it was really great that ddd / started

00:01:37.599 --> 00:01:42.849
Caption: with welcome to the country and i would

00:01:40.989 --> 00:01:45.489
Caption: like to personally acknowledge that this

00:01:42.849 --> 00:01:47.559
Caption: presentation is held on the traditional

00:01:45.489 --> 00:01:49.209
Caption: lands in the new garden nation and i do

00:01:47.559 --> 00:01:54.158
Caption: pay my respects to the elders past

00:01:49.209 --> 00:01:56.618
Caption: present and merging so today we&#39;re going

00:01:54.158 --> 00:01:59.889
Caption: to be talking about where the web is

00:01:56.619 --> 00:02:02.220
Caption: today and where is it gonna be in the

00:01:59.889 --> 00:02:02.220
Caption: future

00:02:02.940 --> 00:02:07.539
Caption: the demand for innovation automation

00:02:06.009 --> 00:02:10.208
Caption: true artificial intelligence and

00:02:07.539 --> 00:02:12.128
Caption: disruption has never been higher and in

00:02:10.208 --> 00:02:15.580
Caption: many ways technology has brought us

00:02:12.128 --> 00:02:18.069
Caption: incredible advancements it contributed

00:02:15.580 --> 00:02:21.128
Caption: to improving living standards rare

00:02:18.069 --> 00:02:24.759
Caption: defined communications and amplification

00:02:21.128 --> 00:02:27.160
Caption: of business opportunity and much more so

00:02:24.759 --> 00:02:29.619
Caption: what are we wanted or not technology is

00:02:27.160 --> 00:02:33.458
Caption: reshaping our world it&#39;s economics

00:02:29.619 --> 00:02:36.159
Caption: culture democracy and society as we

00:02:33.458 --> 00:02:38.378
Caption: speak and i think it&#39;s fair to say that

00:02:36.160 --> 00:02:42.010
Caption: every single one of us in this room has

00:02:38.378 --> 00:02:45.369
Caption: benefited for playing a part in this

00:02:42.009 --> 00:02:48.610
Caption: digital revolution but i&#39;m not here

00:02:45.369 --> 00:02:51.789
Caption: today to talk about that i&#39;m here to

00:02:48.610 --> 00:02:56.009
Caption: talk about the pitiful sort of

00:02:51.789 --> 00:02:59.559
Caption: technology and i&#39;m here to talk about

00:02:56.009 --> 00:03:01.389
Caption: the negatives and i&#39;m here to take a

00:02:59.559 --> 00:03:06.158
Caption: critical look at what technology

00:03:01.389 --> 00:03:09.330
Caption: presents today so we are here today to

00:03:06.158 --> 00:03:09.330
Caption: get a little bit uncomfortable

00:03:10.229 --> 00:03:16.319
Caption: the cause of said disruption has been

00:03:12.598 --> 00:03:17.039
Caption: significant we demonstrated that the web

00:03:16.319 --> 00:03:19.589
Caption: had failed

00:03:17.039 --> 00:03:21.689
Caption: instead of served humanity the

00:03:19.589 --> 00:03:24.690
Caption: increasing centralization has ended up

00:03:21.690 --> 00:03:28.348
Caption: producing a large scale phenomenon which

00:03:24.690 --> 00:03:30.449
Caption: is anti human and I&#39;m not gonna tell you

00:03:28.348 --> 00:03:31.949
Caption: who said that just yet but we are going

00:03:30.449 --> 00:03:33.110
Caption: to come back to this quote later within

00:03:31.949 --> 00:03:35.910
Caption: the talk

00:03:33.110 --> 00:03:37.860
Caption: so what&#39;s so unhuman about the web I

00:03:35.910 --> 00:03:40.260
Caption: mean we just said that it contributed to

00:03:37.860 --> 00:03:43.199
Caption: so many great things and we&#39;re all so

00:03:40.259 --> 00:03:46.429
Caption: excited and it surely brings a lot of

00:03:43.199 --> 00:03:51.899
Caption: positivity and innovation into the world

00:03:46.429 --> 00:03:55.949
Caption: what&#39;s so anti human well technology

00:03:51.899 --> 00:03:57.929
Caption: negatively effects mental health social

00:03:55.949 --> 00:03:59.910
Caption: media especially is known to negatively

00:03:57.929 --> 00:04:02.639
Caption: affect mental health through low self

00:03:59.910 --> 00:04:05.010
Caption: esteem anxiety depression FOMO

00:04:02.639 --> 00:04:06.509
Caption: feelings of loneliness which feels a bit

00:04:05.009 --> 00:04:09.029
Caption: weird because there are so many people

00:04:06.509 --> 00:04:11.759
Caption: that we can reach through technology and

00:04:09.029 --> 00:04:18.059
Caption: the more time we spend on social media

00:04:11.759 --> 00:04:19.738
Caption: the more unhappy we get technology

00:04:18.059 --> 00:04:22.979
Caption: especially social media has also become

00:04:19.739 --> 00:04:26.789
Caption: a platform for harassment victimizing

00:04:22.979 --> 00:04:29.489
Caption: shaming trolling doxxing and that has

00:04:26.789 --> 00:04:32.399
Caption: driven a lot of people away mainly from

00:04:29.489 --> 00:04:36.679
Caption: underrepresented groups and I think

00:04:32.399 --> 00:04:36.679
Caption: those examples speak for themselves

00:04:45.308 --> 00:04:53.018
Caption: and I could go on personal data has

00:04:50.648 --> 00:04:55.388
Caption: become currency sold to advertisers and

00:04:53.019 --> 00:04:57.579
Caption: sometimes even weaponized to alter

00:04:55.389 --> 00:04:59.619
Caption: election results like in famous

00:04:57.579 --> 00:05:02.499
Caption: Cambridge analytic on Facebook scandal

00:04:59.618 --> 00:05:06.698
Caption: in many cases if a product is free

00:05:02.498 --> 00:05:09.158
Caption: you as a user our product data preachers

00:05:06.699 --> 00:05:13.359
Caption: have become very frequent leaking

00:05:09.158 --> 00:05:17.648
Caption: billions of users information such as

00:05:13.359 --> 00:05:19.869
Caption: giants like Yahoo Equifax uber due to

00:05:17.648 --> 00:05:22.028
Caption: their poor security practices and that&#39;s

00:05:19.868 --> 00:05:25.748
Caption: not only specific for Silicon Valley it

00:05:22.028 --> 00:05:27.518
Caption: happens everywhere and some of those

00:05:25.748 --> 00:05:29.048
Caption: companies covered up those incidents

00:05:27.519 --> 00:05:32.110
Caption: without alerting people who are

00:05:29.049 --> 00:05:38.979
Caption: susceptible to those breaches until a

00:05:32.109 --> 00:05:41.018
Caption: year later they had no idea in many

00:05:38.979 --> 00:05:43.269
Caption: cases reality can be mended and

00:05:41.019 --> 00:05:47.049
Caption: manipulated towards commercial goals

00:05:43.269 --> 00:05:49.229
Caption: this is an example from an Australian GP

00:05:47.049 --> 00:05:53.049
Caption: booking platform called health engine

00:05:49.229 --> 00:05:55.539
Caption: that altered 20,000 patient reviews to

00:05:53.049 --> 00:05:58.449
Caption: sound positive and this is the original

00:05:55.539 --> 00:06:00.158
Caption: one the doctor I saw was terrible but

00:05:58.449 --> 00:06:02.619
Caption: luckily I received another option

00:06:00.158 --> 00:06:05.378
Caption: opinion and the receptionist nurse was

00:06:02.618 --> 00:06:08.488
Caption: lovely which is why I would go back but

00:06:05.378 --> 00:06:10.658
Caption: that became the receptionist was lovely

00:06:08.489 --> 00:06:15.489
Caption: so you see the level of manipulation

00:06:10.658 --> 00:06:17.229
Caption: this is only one example the tech

00:06:15.489 --> 00:06:19.269
Caption: industry is toxic culture is actively

00:06:17.229 --> 00:06:22.088
Caption: driving people away especially members

00:06:19.269 --> 00:06:23.499
Caption: of underrepresented groups and this

00:06:22.088 --> 00:06:25.328
Caption: isn&#39;t a problem that&#39;s unique to

00:06:23.498 --> 00:06:27.998
Caption: technology it&#39;s something that prevails

00:06:25.329 --> 00:06:29.559
Caption: everywhere but rampant and fairness is

00:06:27.998 --> 00:06:33.389
Caption: something that&#39;s reported way more in

00:06:29.558 --> 00:06:33.388
Caption: tech industry than non tech industries

00:06:33.748 --> 00:06:39.639
Caption: accordingly to tech leavers study 78% of

00:06:37.359 --> 00:06:44.708
Caption: tech workers report some form of

00:06:39.639 --> 00:06:48.128
Caption: unfairness towards them accordingly two

00:06:44.709 --> 00:06:51.939
Caption: new study by reveal on the point eight

00:06:48.128 --> 00:06:54.598
Caption: percent of black women hold jobs in the

00:06:51.938 --> 00:06:54.598
Caption: tech industry

00:06:55.028 --> 00:07:00.419
Caption: only 6% of Fortune 500 chief executives

00:06:58.269 --> 00:07:08.309
Caption: are women

00:07:00.419 --> 00:07:12.039
Caption: 98% of VCS are white or Asian men 29% is

00:07:08.308 --> 00:07:17.949
Caption: the average pay gap between men and

00:07:12.039 --> 00:07:24.699
Caption: women 60% of women report unwanted

00:07:17.949 --> 00:07:26.739
Caption: sexual advances in their workplace 30%

00:07:24.699 --> 00:07:30.360
Caption: of women of color were passed over for a

00:07:26.739 --> 00:07:30.360
Caption: promotion for no reason

00:07:30.479 --> 00:07:41.979
Caption: according to github surveys 95% of open

00:07:33.908 --> 00:07:46.748
Caption: source contributors are men so these are

00:07:41.979 --> 00:07:50.799
Caption: just few consequences of that rapid

00:07:46.748 --> 00:07:53.998
Caption: technological growth so in my opinion

00:07:50.799 --> 00:07:56.229
Caption: our industry is really facing a crisis

00:07:53.998 --> 00:08:00.778
Caption: trying to figure out well who takes

00:07:56.229 --> 00:08:00.778
Caption: responsibility for this who fixes this

00:08:00.898 --> 00:08:08.859
Caption: what should we do and one of the reasons

00:08:06.759 --> 00:08:11.408
Caption: why we got here is because we&#39;ve

00:08:08.859 --> 00:08:14.708
Caption: disregarded the negative impact of that

00:08:11.408 --> 00:08:17.049
Caption: fast-paced revolution we&#39;ve become

00:08:14.709 --> 00:08:20.050
Caption: complacent engaging in something that&#39;s

00:08:17.049 --> 00:08:21.698
Caption: called an act of cultural denial which

00:08:20.049 --> 00:08:23.828
Caption: is the process where humans avoid

00:08:21.699 --> 00:08:28.269
Caption: uncomfortable realities like poverty

00:08:23.829 --> 00:08:31.569
Caption: suffering injustice racial oppression so

00:08:28.269 --> 00:08:37.838
Caption: in some ways we&#39;ve decided to look but

00:08:31.569 --> 00:08:40.268
Caption: not really see a dangerous form of

00:08:37.838 --> 00:08:43.299
Caption: magical thinking often accompanies new

00:08:40.268 --> 00:08:45.518
Caption: technological developments a curious

00:08:43.299 --> 00:08:48.250
Caption: assurance that revolution in our tools

00:08:45.518 --> 00:08:50.758
Caption: inevitably wipes the slate of the past

00:08:48.250 --> 00:08:50.758
Caption: clean

00:08:53.969 --> 00:08:59.879
Caption: so this place of comfort and ignorance

00:08:57.099 --> 00:09:02.409
Caption: and complacency has led us where we are

00:08:59.880 --> 00:09:07.900
Caption: we forgot our historical and social

00:09:02.409 --> 00:09:11.369
Caption: context so it&#39;s necessary to have a hard

00:09:07.900 --> 00:09:13.960
Caption: look at how this neutral hacker culture

00:09:11.369 --> 00:09:17.619
Caption: has steered us toward a crisis of

00:09:13.960 --> 00:09:20.190
Caption: democracy and human rights so when I

00:09:17.619 --> 00:09:28.779
Caption: have a look at common ways of thinking

00:09:20.190 --> 00:09:30.640
Caption: that contributed to this crisis the

00:09:28.780 --> 00:09:32.800
Caption: first one is move fast and break things

00:09:30.640 --> 00:09:36.309
Caption: and I&#39;m sure a lot of you have heard

00:09:32.799 --> 00:09:39.070
Caption: that before up until recently that was

00:09:36.309 --> 00:09:41.140
Caption: the unofficial mode of social media

00:09:39.070 --> 00:09:44.250
Caption: giant Facebook one of the most

00:09:41.140 --> 00:09:46.570
Caption: profitable companies that exists

00:09:44.250 --> 00:09:49.539
Caption: recently it has changed to move fast

00:09:46.570 --> 00:09:51.660
Caption: with stable infra and sure in mystery

00:09:49.539 --> 00:09:54.789
Caption: first to approach the software and

00:09:51.659 --> 00:09:57.459
Caption: reliable infrastructure but in some ways

00:09:54.789 --> 00:09:59.409
Caption: it has become a celebration of privilege

00:09:57.460 --> 00:10:04.570
Caption: and lack of responsibility in the early

00:09:59.409 --> 00:10:06.519
Caption: hacker culture apparently Facebook

00:10:04.570 --> 00:10:08.890
Caption: admitted that sometimes it harms

00:10:06.520 --> 00:10:12.059
Caption: democracy but honestly they&#39;ve shown

00:10:08.890 --> 00:10:14.679
Caption: little responsibility for that as well

00:10:12.059 --> 00:10:16.570
Caption: lack of consideration for there was

00:10:14.679 --> 00:10:19.419
Caption: real-life repercussions of technology

00:10:16.570 --> 00:10:24.309
Caption: and hunger for more data an exponential

00:10:19.419 --> 00:10:26.500
Caption: growth has had catastrophic results

00:10:24.309 --> 00:10:29.020
Caption: because disruption isn&#39;t a free card to

00:10:26.500 --> 00:10:33.280
Caption: bypass laws freedom&#39;s moral compass

00:10:29.020 --> 00:10:36.869
Caption: civil rights human protections speed

00:10:33.280 --> 00:10:36.870
Caption: doesn&#39;t want lack of responsibility

00:10:37.739 --> 00:10:43.000
Caption: the second belief is saying that

00:10:40.059 --> 00:10:44.919
Caption: technology is neutral and we&#39;ve heard

00:10:43.000 --> 00:10:47.710
Caption: that multiple times technology&#39;s neutral

00:10:44.919 --> 00:10:50.859
Caption: we should avoid politics but

00:10:47.710 --> 00:10:53.740
Caption: technology&#39;s not never will be and never

00:10:50.859 --> 00:10:55.959
Caption: was neutral because people after

00:10:53.739 --> 00:11:00.159
Caption: software and they design it with

00:10:55.960 --> 00:11:03.000
Caption: conscious and unknown biases baked in so

00:11:00.159 --> 00:11:06.009
Caption: their prejudice lack of education

00:11:03.000 --> 00:11:08.559
Caption: inevitably will manifest itself in their

00:11:06.010 --> 00:11:16.270
Caption: we&#39;re and their design and it will

00:11:08.559 --> 00:11:19.690
Caption: impact people who use them this myopic

00:11:16.270 --> 00:11:22.420
Caption: focus on what new what&#39;s new leads us to

00:11:19.690 --> 00:11:24.220
Caption: miss the important ways that digital

00:11:22.419 --> 00:11:27.909
Caption: tools are embedded in old systems of

00:11:24.219 --> 00:11:31.629
Caption: privilege and power they reflect the

00:11:27.909 --> 00:11:36.579
Caption: current political climate classism

00:11:31.630 --> 00:11:39.400
Caption: racism homophobia and amplify them

00:11:36.580 --> 00:11:45.239
Caption: tenfold because the world wide web is a

00:11:39.400 --> 00:11:48.940
Caption: giant megaphone the third reason is

00:11:45.239 --> 00:11:50.469
Caption: lacking to ethical education because in

00:11:48.940 --> 00:11:52.570
Caption: many cases the technology oriented

00:11:50.469 --> 00:11:54.849
Caption: curriculum doesn&#39;t really include any

00:11:52.570 --> 00:11:57.960
Caption: kind of ethical training compared to

00:11:54.849 --> 00:12:00.969
Caption: other disciplines like law or medicine

00:11:57.960 --> 00:12:03.190
Caption: the focus that we have on technical

00:12:00.969 --> 00:12:05.549
Caption: skills the prior to psychology social

00:12:03.190 --> 00:12:08.849
Caption: studies ethics communication skills

00:12:05.549 --> 00:12:11.489
Caption: categorizing them as soft and

00:12:08.849 --> 00:12:14.079
Caption: unnecessary distraction and that

00:12:11.489 --> 00:12:16.270
Caption: troubling dynamic really feeds into the

00:12:14.080 --> 00:12:20.320
Caption: stereotype of an introvert hacker who

00:12:16.270 --> 00:12:23.950
Caption: just wants to code and forgets that

00:12:20.320 --> 00:12:29.760
Caption: human that technology is built by and

00:12:23.950 --> 00:12:33.760
Caption: for humans so I did a little survey and

00:12:29.760 --> 00:12:36.400
Caption: sure seven seven thousand responses

00:12:33.760 --> 00:12:38.350
Caption: isn&#39;t statistically significant but it

00:12:36.400 --> 00:12:40.420
Caption: highlights a pattern and a very

00:12:38.349 --> 00:12:42.969
Caption: troubling one most of the respondents

00:12:40.419 --> 00:12:46.529
Caption: highlighted their engineering or design

00:12:42.969 --> 00:12:49.959
Caption: degrees didn&#39;t include any ethics

00:12:46.530 --> 00:12:53.190
Caption: curriculum or subjects or they were

00:12:49.960 --> 00:12:58.240
Caption: actually opted in they were in mandatory

00:12:53.190 --> 00:13:00.070
Caption: 67% no ethics training whatsoever and

00:12:58.239 --> 00:13:02.440
Caption: it&#39;s hard to imagine making informed

00:13:00.070 --> 00:13:04.390
Caption: decisions and having conversations about

00:13:02.440 --> 00:13:06.929
Caption: ethical implications in design and

00:13:04.390 --> 00:13:09.460
Caption: software if we are lacking foundational

00:13:06.929 --> 00:13:14.349
Caption: understanding of that subject we have

00:13:09.460 --> 00:13:17.530
Caption: nothing to base on an ethical decision

00:13:14.349 --> 00:13:22.750
Caption: making is like a muscle that has to be X

00:13:17.530 --> 00:13:25.120
Caption: because it will atrophy so the above

00:13:22.750 --> 00:13:28.840
Caption: examples of culture and algorithmic

00:13:25.119 --> 00:13:30.789
Caption: pitfalls of Technology highlight that we

00:13:28.840 --> 00:13:33.070
Caption: really have to become fluent in ethical

00:13:30.789 --> 00:13:34.989
Caption: concerns to continue building that

00:13:33.070 --> 00:13:43.600
Caption: welcoming respectful web that we all

00:13:34.989 --> 00:13:47.789
Caption: want and we talked us about this a

00:13:43.599 --> 00:13:47.789
Caption: little bit already exclusion homogeneity

00:13:47.880 --> 00:13:55.059
Caption: racial diversity scarce not mentioning

00:13:52.270 --> 00:13:57.940
Caption: LGBTQ a representation or inclusion of

00:13:55.059 --> 00:13:59.469
Caption: people with disabilities and the

00:13:57.940 --> 00:14:01.510
Caption: statistics on this show that it&#39;s not

00:13:59.469 --> 00:14:03.399
Caption: getting any better despite all of the

00:14:01.510 --> 00:14:06.940
Caption: noise surrounding that subject and a lot

00:14:03.400 --> 00:14:09.369
Caption: of effort and we cannot succeed making

00:14:06.940 --> 00:14:12.820
Caption: the web that&#39;s welcoming towards

00:14:09.369 --> 00:14:14.440
Caption: everyone if the platform is built with

00:14:12.820 --> 00:14:16.690
Caption: only a few with enough power in

00:14:14.440 --> 00:14:19.539
Caption: privilege that&#39;s backed by structural

00:14:16.690 --> 00:14:22.030
Caption: inequality because it is impossible to

00:14:19.539 --> 00:14:25.299
Caption: empathize understand and design for

00:14:22.030 --> 00:14:27.040
Caption: experiences we never had never seen or

00:14:25.299 --> 00:14:28.899
Caption: never heard of because those people are

00:14:27.039 --> 00:14:30.699
Caption: just not around us they&#39;re not in our

00:14:28.900 --> 00:14:32.290
Caption: industry we didn&#39;t see them we don&#39;t

00:14:30.700 --> 00:14:36.130
Caption: hear from them that&#39;s why we&#39;re

00:14:32.289 --> 00:14:38.169
Caption: presentation matters and many design and

00:14:36.130 --> 00:14:41.890
Caption: engineering problems could be avoided if

00:14:38.169 --> 00:14:44.469
Caption: teams were more diverse diverse teams

00:14:41.890 --> 00:14:47.588
Caption: challenge biases are more innovative and

00:14:44.469 --> 00:14:50.319
Caption: productive and drive more positive

00:14:47.588 --> 00:14:52.119
Caption: business outcomes so until everyone no

00:14:50.320 --> 00:14:53.789
Caption: matter their ethnicity socioeconomic

00:14:52.119 --> 00:14:57.400
Caption: background gender sexual orientation

00:14:53.789 --> 00:15:00.940
Caption: ability can enter and prevail and tech

00:14:57.400 --> 00:15:04.180
Caption: and use the products of tech comfortably

00:15:00.940 --> 00:15:08.440
Caption: and reliably the web is not open and the

00:15:04.179 --> 00:15:15.000
Caption: web is not welcoming so my question

00:15:08.440 --> 00:15:17.940
Caption: today is how do we establish was ethical

00:15:15.000 --> 00:15:20.590
Caption: how do we build this welcoming web and

00:15:17.940 --> 00:15:25.239
Caption: what does ethic means these are pretty

00:15:20.590 --> 00:15:27.309
Caption: big questions to me ethics really

00:15:25.239 --> 00:15:31.899
Caption: focuses on answering a question of

00:15:27.309 --> 00:15:33.669
Caption: should we in ethics defense and

00:15:31.900 --> 00:15:36.580
Caption: recommends concepts of right and wrong

00:15:33.669 --> 00:15:39.549
Caption: conduct and there&#39;s no code of ethics

00:15:36.580 --> 00:15:41.950
Caption: for do web there have been attempts

00:15:39.549 --> 00:15:43.809
Caption: there have been people who tried to

00:15:41.950 --> 00:15:46.330
Caption: write something for a certain group or

00:15:43.809 --> 00:15:48.969
Caption: certain meetup a certain organization

00:15:46.330 --> 00:15:51.520
Caption: but there&#39;s nothing widely adopted it&#39;s

00:15:48.969 --> 00:15:53.729
Caption: a very complex problem and the web is so

00:15:51.520 --> 00:15:57.119
Caption: distributed

00:15:53.729 --> 00:15:59.739
Caption: there&#39;s nothing undressing this problem

00:15:57.119 --> 00:16:01.390
Caption: how do we figure this out how do we

00:15:59.739 --> 00:16:02.979
Caption: figure out what&#39;s ethical how do we

00:16:01.390 --> 00:16:06.510
Caption: figure out our values and guiding

00:16:02.979 --> 00:16:09.190
Caption: principles and what we built for the web

00:16:06.510 --> 00:16:11.679
Caption: well I spent some time researching and

00:16:09.190 --> 00:16:15.309
Caption: trying to figure out what would my

00:16:11.679 --> 00:16:17.770
Caption: ethical principles be and this is based

00:16:15.309 --> 00:16:19.539
Caption: on the actual Declaration of Human

00:16:17.770 --> 00:16:21.730
Caption: Rights and a few other resources that

00:16:19.539 --> 00:16:26.199
Caption: are relevant to our problems in the

00:16:21.729 --> 00:16:29.829
Caption: industry and I hope they can guide us

00:16:26.200 --> 00:16:31.929
Caption: towards a better welcoming web so the

00:16:29.830 --> 00:16:34.360
Caption: first ethical principle is human rights

00:16:31.929 --> 00:16:36.969
Caption: and democracy because it&#39;s not a

00:16:34.359 --> 00:16:40.029
Caption: question of when but how technology is

00:16:36.969 --> 00:16:41.829
Caption: affecting democracy and sure tech

00:16:40.030 --> 00:16:43.990
Caption: conserve democracy really well but it

00:16:41.830 --> 00:16:47.260
Caption: also has empowered authoritarian regimes

00:16:43.989 --> 00:16:49.599
Caption: to censor press or track them dissenting

00:16:47.260 --> 00:16:51.790
Caption: citizens and actually enact legal

00:16:49.599 --> 00:16:55.389
Caption: repercussions on them for example in

00:16:51.789 --> 00:16:57.940
Caption: Turkey Twitter users were trialed for

00:16:55.390 --> 00:17:00.940
Caption: sending some critical tweets about the

00:16:57.940 --> 00:17:02.890
Caption: current prime minister Facebook

00:17:00.940 --> 00:17:07.109
Caption: discovered thousands of innocent ads

00:17:02.890 --> 00:17:10.328
Caption: that were affiliated with the Russian

00:17:07.109 --> 00:17:12.520
Caption: with the election 2016 that came from

00:17:10.328 --> 00:17:18.160
Caption: Russia and they were meant to spread

00:17:12.520 --> 00:17:20.440
Caption: misinformation many algorithms including

00:17:18.160 --> 00:17:23.020
Caption: Facebook and Google helped creating so

00:17:20.439 --> 00:17:26.509
Caption: called filter bubbles so we&#39;ll never see

00:17:23.020 --> 00:17:28.969
Caption: content that&#39;s disagreeable

00:17:26.510 --> 00:17:31.640
Caption: which feeds in phenomenon that&#39;s called

00:17:28.969 --> 00:17:33.500
Caption: confirmation bias and it furthers our

00:17:31.640 --> 00:17:38.119
Caption: inability to make informed choices a

00:17:33.500 --> 00:17:40.250
Caption: citizen&#39;s challenge fake content no

00:17:38.119 --> 00:17:42.890
Caption: surprise we live in a post fact era just

00:17:40.250 --> 00:17:44.839
Caption: a few days ago Google announced that it

00:17:42.890 --> 00:17:47.719
Caption: will be launching a censored version of

00:17:44.839 --> 00:17:50.000
Caption: their search engine in China where it

00:17:47.719 --> 00:17:52.400
Caption: previously had no presence so they will

00:17:50.000 --> 00:17:55.510
Caption: be collaborating with the government in

00:17:52.400 --> 00:17:57.979
Caption: oppression of the Chinese citizens

00:17:55.510 --> 00:17:59.540
Caption: Amazon just promised unwavering

00:17:57.979 --> 00:18:02.178
Caption: commitment in providing facial

00:17:59.540 --> 00:18:04.460
Caption: recognition technology to US government

00:18:02.178 --> 00:18:06.859
Caption: after many other companies denied

00:18:04.459 --> 00:18:11.659
Caption: providing such services knowing the

00:18:06.859 --> 00:18:14.630
Caption: repercussions of that because tech its

00:18:11.660 --> 00:18:16.760
Caption: support it&#39;s supposed to support and

00:18:14.630 --> 00:18:21.500
Caption: improve the Civic processes on which

00:18:16.760 --> 00:18:22.579
Caption: democratic societies depends so what are

00:18:21.500 --> 00:18:24.048
Caption: the rules what are the guiding

00:18:22.579 --> 00:18:25.399
Caption: principles for human rights and

00:18:24.048 --> 00:18:28.668
Caption: democracy and technology

00:18:25.400 --> 00:18:32.299
Caption: well ethical technology should respect

00:18:28.668 --> 00:18:35.019
Caption: and extend human rights it should serve

00:18:32.298 --> 00:18:36.859
Caption: and support democracy not challenge it

00:18:35.020 --> 00:18:38.929
Caption: fight against the spread of

00:18:36.859 --> 00:18:42.770
Caption: misinformation which is pretty rampant

00:18:38.928 --> 00:18:45.069
Caption: these days and encourage civic

00:18:42.770 --> 00:18:45.070
Caption: engagement

00:18:48.560 --> 00:18:56.480
Caption: the second ethical principle is

00:18:50.479 --> 00:18:59.169
Caption: well-being we talked about social media

00:18:56.479 --> 00:19:01.400
Caption: and affecting our mental health and

00:18:59.170 --> 00:19:04.090
Caption: there are more studies that show that

00:19:01.400 --> 00:19:06.859
Caption: our cognitive capacity decreases as

00:19:04.089 --> 00:19:10.309
Caption: hyper connectivity Foster&#39;s attention

00:19:06.859 --> 00:19:12.619
Caption: deficit disorders and often those social

00:19:10.310 --> 00:19:16.820
Caption: media services tap into toes creating

00:19:12.619 --> 00:19:19.489
Caption: addiction social alienation anxiety

00:19:16.819 --> 00:19:22.219
Caption: depression stress trouble sleeping

00:19:19.489 --> 00:19:28.280
Caption: our current side effects of this digital

00:19:22.219 --> 00:19:30.859
Caption: advancement but that shouldn&#39;t be the

00:19:28.280 --> 00:19:34.430
Caption: case technology should be mindful quiet

00:19:30.859 --> 00:19:36.579
Caption: companion to our lives helping us learn

00:19:34.430 --> 00:19:39.260
Caption: than being an overbearing disrupter

00:19:36.579 --> 00:19:42.469
Caption: hijacking our attention to complex

00:19:39.260 --> 00:19:46.790
Caption: interfaces and tricks and addiction

00:19:42.469 --> 00:19:48.169
Caption: patterns and if you&#39;re interested in the

00:19:46.790 --> 00:19:50.570
Caption: concept of calm technology I would

00:19:48.170 --> 00:19:53.660
Caption: strongly recommend amber cases book

00:19:50.569 --> 00:19:55.279
Caption: entitled calm technology and center for

00:19:53.660 --> 00:19:59.900
Caption: human technology because they speak

00:19:55.280 --> 00:20:02.780
Caption: about that subject so ethical technology

00:19:59.900 --> 00:20:05.540
Caption: in the spectrum of well-being should

00:20:02.780 --> 00:20:07.580
Caption: require minimum attention again we want

00:20:05.540 --> 00:20:10.359
Caption: technology that&#39;s helping us and then

00:20:07.579 --> 00:20:12.949
Caption: moving out of our way

00:20:10.359 --> 00:20:16.280
Caption: ethical technologies should inform and

00:20:12.949 --> 00:20:18.679
Caption: create calm should work in the

00:20:16.280 --> 00:20:20.810
Caption: background again not hijacking our

00:20:18.680 --> 00:20:25.450
Caption: attention in our time that we might to

00:20:20.810 --> 00:20:32.330
Caption: want to choose to spend otherwise and

00:20:25.449 --> 00:20:34.669
Caption: respect societal norms deferred ethical

00:20:32.329 --> 00:20:36.589
Caption: principle is security and safety and

00:20:34.670 --> 00:20:40.010
Caption: that can span over multiple dimensions

00:20:36.589 --> 00:20:42.079
Caption: from reliability of software against

00:20:40.010 --> 00:20:43.760
Caption: hacking and ransomware to actual

00:20:42.079 --> 00:20:47.409
Caption: physical and emotional safety of its

00:20:43.760 --> 00:20:50.260
Caption: users data bridges have catastrophic

00:20:47.410 --> 00:20:55.369
Caption: consequences and just in Australia alone

00:20:50.260 --> 00:21:01.770
Caption: 2014 2015 over 700,000 people were

00:20:55.369 --> 00:21:04.219
Caption: affected by data breaches 700,000 people

00:21:01.770 --> 00:21:07.140
Caption: and those details that were leaked and

00:21:04.219 --> 00:21:11.789
Caption: stolen and obtained can be used to open

00:21:07.140 --> 00:21:13.349
Caption: bank accounts get mortgages put the

00:21:11.790 --> 00:21:14.910
Caption: victims of those breaches at the risk of

00:21:13.349 --> 00:21:17.489
Caption: police raids immigration issues

00:21:14.910 --> 00:21:20.940
Caption: financial bottlenecks of different sorts

00:21:17.489 --> 00:21:23.369
Caption: for years to come and in other scenarios

00:21:20.939 --> 00:21:25.399
Caption: that stolen data might put victims of

00:21:23.369 --> 00:21:27.839
Caption: harassment and domestic abuse or

00:21:25.400 --> 00:21:34.050
Caption: activists or whisperers an incredible

00:21:27.839 --> 00:21:36.619
Caption: risk of daxing so ethical technology in

00:21:34.050 --> 00:21:40.550
Caption: the spectrum of safety and security

00:21:36.619 --> 00:21:44.150
Caption: should respond promptly to crisis

00:21:40.550 --> 00:21:46.440
Caption: eliminate single points of failure

00:21:44.150 --> 00:21:50.550
Caption: invest in cryptography and security

00:21:46.439 --> 00:21:58.500
Caption: practices and most of all protect the

00:21:50.550 --> 00:21:59.970
Caption: people who are most vulnerable the next

00:21:58.500 --> 00:22:05.010
Caption: principle is responsibility and

00:21:59.969 --> 00:22:06.989
Caption: accountability tech workers and

00:22:05.010 --> 00:22:08.880
Caption: organizations have to be accountable for

00:22:06.989 --> 00:22:10.890
Caption: their creations and they have to be

00:22:08.880 --> 00:22:12.959
Caption: liable for the potential damages they

00:22:10.890 --> 00:22:16.290
Caption: make on society and this hasn&#39;t been the

00:22:12.959 --> 00:22:18.239
Caption: case and that social impact should be

00:22:16.290 --> 00:22:21.000
Caption: estimated and measured with the same

00:22:18.239 --> 00:22:23.189
Caption: level of scrutiny and level of

00:22:21.000 --> 00:22:24.780
Caption: importance as delivering to your

00:22:23.189 --> 00:22:26.669
Caption: investors or pushing new feature

00:22:24.780 --> 00:22:29.190
Caption: releases or releasing a new product and

00:22:26.670 --> 00:22:35.700
Caption: has to be done on continuous basis as

00:22:29.189 --> 00:22:38.729
Caption: well sure significant of algorithms and

00:22:35.699 --> 00:22:41.760
Caption: AI rely on machine learning but it is

00:22:38.729 --> 00:22:44.819
Caption: humans who put that system in place and

00:22:41.760 --> 00:22:48.660
Caption: designer so it&#39;s us who should take

00:22:44.819 --> 00:22:50.729
Caption: ownership of those systemic flaws and I

00:22:48.660 --> 00:22:53.369
Caption: think it&#39;s really important to realize

00:22:50.729 --> 00:22:55.500
Caption: that fundamental distinction and the

00:22:53.369 --> 00:23:00.329
Caption: fact that we can&#39;t put the blame on the

00:22:55.500 --> 00:23:02.040
Caption: software recently Margaret Stewart who&#39;s

00:23:00.329 --> 00:23:04.260
Caption: the VP of design at face book spoke

00:23:02.040 --> 00:23:07.380
Caption: about the four quadrants of design

00:23:04.260 --> 00:23:10.290
Caption: responsibility and she explained how the

00:23:07.380 --> 00:23:13.349
Caption: liability of our designs rapidly grows

00:23:10.290 --> 00:23:14.900
Caption: as products turned into ecosystems used

00:23:13.349 --> 00:23:16.640
Caption: by thousands if not millions

00:23:14.900 --> 00:23:19.550
Caption: which is very relevant to the Facebook

00:23:16.640 --> 00:23:21.940
Caption: use case but no matter where your work

00:23:19.550 --> 00:23:24.140
Caption: Falls and those four quadrants is

00:23:21.939 --> 00:23:31.849
Caption: necessary to evaluate the potential

00:23:24.140 --> 00:23:34.359
Caption: impact so ethical technology is aware of

00:23:31.849 --> 00:23:37.489
Caption: diverse social and cultural norms

00:23:34.359 --> 00:23:40.369
Caption: creates policies for algorithmic

00:23:37.489 --> 00:23:44.150
Caption: accountability how this responsibilities

00:23:40.369 --> 00:23:47.329
Caption: look like collaborates with lawmakers to

00:23:44.150 --> 00:23:48.800
Caption: advance regulations it is so hard to

00:23:47.329 --> 00:23:52.399
Caption: create regulations for this age of

00:23:48.800 --> 00:23:53.600
Caption: technology and advancement the people

00:23:52.400 --> 00:23:55.910
Caption: who are creating those regulations

00:23:53.599 --> 00:23:58.400
Caption: really need our help and understanding

00:23:55.910 --> 00:24:01.450
Caption: technology in writing laws for

00:23:58.400 --> 00:24:03.950
Caption: technology to protect people and

00:24:01.449 --> 00:24:11.739
Caption: complies with national and international

00:24:03.949 --> 00:24:14.000
Caption: guidelines data protection and privacy

00:24:11.739 --> 00:24:18.739
Caption: something really close to security and

00:24:14.000 --> 00:24:21.010
Caption: safety the right to protection and

00:24:18.739 --> 00:24:23.989
Caption: control over our personal data is

00:24:21.010 --> 00:24:25.579
Caption: continually challenged one of the

00:24:23.989 --> 00:24:29.030
Caption: examples of that as a service called

00:24:25.579 --> 00:24:31.309
Caption: just deleteme which is a directory of

00:24:29.030 --> 00:24:34.700
Caption: web services and those services are

00:24:31.310 --> 00:24:36.170
Caption: categorized by how hard it is to the

00:24:34.699 --> 00:24:37.969
Caption: leader account or whether it&#39;s even

00:24:36.170 --> 00:24:42.020
Caption: possible to leave your account and your

00:24:37.969 --> 00:24:44.179
Caption: data associated with it open and closed

00:24:42.020 --> 00:24:46.670
Caption: our surveillance has become the new norm

00:24:44.180 --> 00:24:47.710
Caption: and it&#39;s challenging human rights even

00:24:46.670 --> 00:24:50.359
Caption: in Australia

00:24:47.709 --> 00:24:52.909
Caption: just recently Peter Dutton was calling

00:24:50.359 --> 00:24:55.549
Caption: for right to access emails bank records

00:24:52.910 --> 00:24:57.830
Caption: and text messages of citizens without a

00:24:55.550 --> 00:25:01.489
Caption: warrant and without the knowledge of the

00:24:57.829 --> 00:25:02.899
Caption: citizens and that&#39;s very troubling the

00:25:01.489 --> 00:25:04.939
Caption: data can be used to enhance user

00:25:02.900 --> 00:25:07.430
Caption: experience it can be used for good and

00:25:04.939 --> 00:25:10.239
Caption: it can provide useful services but can

00:25:07.430 --> 00:25:12.970
Caption: certainly be weaponized by algorithms

00:25:10.239 --> 00:25:15.589
Caption: authoritarian regimes and governments as

00:25:12.969 --> 00:25:17.689
Caption: I mentioned before even creators of

00:25:15.589 --> 00:25:19.669
Caption: facial recognition technology are well

00:25:17.689 --> 00:25:22.339
Caption: aware of those repercussions and they

00:25:19.670 --> 00:25:25.220
Caption: did decline to provide services to

00:25:22.339 --> 00:25:28.739
Caption: governments because they know and they

00:25:25.219 --> 00:25:32.048
Caption: understand what could happen

00:25:28.739 --> 00:25:34.359
Caption: sensitive data should be accessible to

00:25:32.048 --> 00:25:38.128
Caption: the users you should be able to modify

00:25:34.359 --> 00:25:43.359
Caption: restrict access export and delete and

00:25:38.129 --> 00:25:44.859
Caption: much it&#39;s AG web tsuki talks about data

00:25:43.359 --> 00:25:47.199
Caption: collection in his talk

00:25:44.859 --> 00:25:49.599
Caption: hunted by data in a really great way

00:25:47.199 --> 00:25:51.548
Caption: don&#39;t collect it if you have to collect

00:25:49.599 --> 00:25:53.829
Caption: it don&#39;t start it if you have to store

00:25:51.548 --> 00:25:55.689
Caption: it don&#39;t keep it and he calls for those

00:25:53.829 --> 00:25:59.888
Caption: minimal approaches to data collection

00:25:55.689 --> 00:26:02.229
Caption: and mindful short storage times so

00:25:59.889 --> 00:26:04.690
Caption: ethical technology should only collect

00:26:02.229 --> 00:26:06.339
Caption: data that&#39;s necessary for operation do

00:26:04.689 --> 00:26:07.778
Caption: you really need somebody&#39;s phone number

00:26:06.339 --> 00:26:09.999
Caption: I don&#39;t think so

00:26:07.779 --> 00:26:10.960
Caption: do you need their title to need all of

00:26:09.999 --> 00:26:15.070
Caption: those details

00:26:10.959 --> 00:26:16.538
Caption: you probably don&#39;t article technology

00:26:15.069 --> 00:26:19.329
Caption: gives full control of data including

00:26:16.538 --> 00:26:22.118
Caption: permanent and swift deletion some

00:26:19.329 --> 00:26:24.579
Caption: servitors services will remove your data

00:26:22.119 --> 00:26:25.808
Caption: after a month when you request that and

00:26:24.579 --> 00:26:29.008
Caption: you do have to request that through

00:26:25.808 --> 00:26:32.168
Caption: email that shouldn&#39;t be the case

00:26:29.009 --> 00:26:36.519
Caption: ethical technology allows some level of

00:26:32.168 --> 00:26:39.128
Caption: anonymity which is very helpful to

00:26:36.519 --> 00:26:44.559
Caption: whisperers activists members of

00:26:39.129 --> 00:26:47.139
Caption: underrepresented groups the next ethical

00:26:44.558 --> 00:26:49.148
Caption: principle is transparency because

00:26:47.139 --> 00:26:51.489
Caption: ethical design is transparent as has

00:26:49.149 --> 00:26:53.950
Caption: nothing to hide and discuss an

00:26:51.489 --> 00:26:58.239
Caption: information how services operate how

00:26:53.949 --> 00:27:00.788
Caption: data is gathered stored and used really

00:26:58.239 --> 00:27:04.178
Caption: improves trustworthiness and mitigates

00:27:00.788 --> 00:27:05.918
Caption: misuse of that data often people who use

00:27:04.178 --> 00:27:07.628
Caption: our software and designs and products

00:27:05.918 --> 00:27:11.199
Caption: are not fully informed how they work

00:27:07.629 --> 00:27:13.599
Caption: they don&#39;t understand how the services

00:27:11.199 --> 00:27:16.118
Caption: affect them software is a black box and

00:27:13.599 --> 00:27:18.729
Caption: they don&#39;t understand its decisions

00:27:16.119 --> 00:27:21.099
Caption: those decisions cannot be explained so

00:27:18.729 --> 00:27:23.859
Caption: that lack of clarity increases the risk

00:27:21.099 --> 00:27:25.569
Caption: and magnitude of harm as well as lowers

00:27:23.859 --> 00:27:28.178
Caption: the chances of any responsibility in

00:27:25.569 --> 00:27:30.428
Caption: forensic accountability because there

00:27:28.178 --> 00:27:33.788
Caption: are no governing bodies over privately

00:27:30.428 --> 00:27:35.348
Caption: helped tech companies and startups so

00:27:33.788 --> 00:27:37.209
Caption: those algorithms and software makes

00:27:35.348 --> 00:27:41.489
Caption: choices that cannot be disputed or

00:27:37.209 --> 00:27:41.489
Caption: challenged because how would we do that

00:27:42.160 --> 00:27:48.440
Caption: so ethical technology responsibility

00:27:46.489 --> 00:27:50.510
Caption: discusses abuses of software we already

00:27:48.439 --> 00:27:52.519
Caption: talked about uber and about companies

00:27:50.510 --> 00:27:57.219
Caption: that didn&#39;t discuss data breaches until

00:27:52.520 --> 00:28:02.770
Caption: a year later this should be disclosed

00:27:57.219 --> 00:28:05.829
Caption: timely and transparently establishes

00:28:02.770 --> 00:28:10.280
Caption: rules for reporting and accountability

00:28:05.829 --> 00:28:13.279
Caption: how can we report a breach what is my

00:28:10.280 --> 00:28:15.320
Caption: job as a tech company or a tech service

00:28:13.280 --> 00:28:17.780
Caption: how would I react to us what is the

00:28:15.319 --> 00:28:22.309
Caption: process what can you expect expect from

00:28:17.780 --> 00:28:23.750
Caption: me and thinking about mission and value

00:28:22.310 --> 00:28:27.410
Caption: statements worried what is my mission

00:28:23.750 --> 00:28:31.699
Caption: what do I present as a company as an

00:28:27.410 --> 00:28:34.989
Caption: employee as an individual that

00:28:31.699 --> 00:28:34.989
Caption: transparency is crucial

00:28:35.589 --> 00:28:41.630
Caption: the next ethical principle is misuse and

00:28:38.300 --> 00:28:45.400
Caption: bias awareness you might have heard of

00:28:41.630 --> 00:28:45.400
Caption: Eric Meyer who is a very prominent

00:28:45.459 --> 00:28:50.119
Caption: designer in the front-end developer few

00:28:48.589 --> 00:28:53.029
Caption: years ago he wrote this article that was

00:28:50.119 --> 00:28:55.130
Caption: entitled inadvertent algorithmic cruelty

00:28:53.030 --> 00:28:56.900
Caption: and he described his absolutely horrific

00:28:55.130 --> 00:28:57.619
Caption: experience with facebook&#39;s

00:28:56.900 --> 00:28:59.810
Caption: year-in-review

00:28:57.619 --> 00:29:04.119
Caption: that treated the death of his daughter

00:28:59.810 --> 00:29:05.300
Caption: Rebecca as a highlight of his year in

00:29:04.119 --> 00:29:07.969
Caption: 2016

00:29:05.300 --> 00:29:09.680
Caption: Microsoft released a which was an

00:29:07.969 --> 00:29:11.750
Caption: artificial intelligence chat bot on

00:29:09.680 --> 00:29:13.690
Caption: Twitter which in less than 24 hours

00:29:11.750 --> 00:29:14.959
Caption: turned completely racist and

00:29:13.689 --> 00:29:20.479
Caption: misogynistic

00:29:14.959 --> 00:29:22.189
Caption: and had to be taken down software that&#39;s

00:29:20.479 --> 00:29:24.799
Caption: called compas that&#39;s used to predict

00:29:22.189 --> 00:29:27.049
Caption: future crimes and chance of recidivism

00:29:24.800 --> 00:29:30.290
Caption: has been proven by Pro Publica to

00:29:27.050 --> 00:29:32.660
Caption: falsely flag black defendants as future

00:29:30.290 --> 00:29:34.580
Caption: criminals at twice the rate compared to

00:29:32.660 --> 00:29:36.820
Caption: white defendants simultaneously

00:29:34.579 --> 00:29:41.619
Caption: mislabeling white suspects as low-risk

00:29:36.819 --> 00:29:43.759
Caption: and these are just free examples

00:29:41.619 --> 00:29:45.829
Caption: algorithms are thoughtless because

00:29:43.760 --> 00:29:48.140
Caption: software is written by people and does

00:29:45.829 --> 00:29:51.679
Caption: reflect their ideology biases thought

00:29:48.140 --> 00:29:53.630
Caption: processes beliefs and so often we test

00:29:51.680 --> 00:29:56.540
Caption: for those positive

00:29:53.630 --> 00:29:58.790
Caption: cases successful use cases we forget

00:29:56.540 --> 00:30:00.890
Caption: about malice and pathetic design and

00:29:58.790 --> 00:30:03.770
Caption: more importantly the social context that

00:30:00.890 --> 00:30:07.750
Caption: we&#39;re embedded in because software

00:30:03.770 --> 00:30:10.670
Caption: doesn&#39;t learn we teach the software

00:30:07.750 --> 00:30:13.280
Caption: technologies under design do not dictate

00:30:10.670 --> 00:30:19.160
Caption: racial technologies rather they reflect

00:30:13.280 --> 00:30:20.830
Caption: the current climate so ethical

00:30:19.160 --> 00:30:24.950
Caption: technology is aware in combats

00:30:20.829 --> 00:30:28.069
Caption: unconscious bias tests for misuse and

00:30:24.949 --> 00:30:30.139
Caption: malice how the canvas product fail how

00:30:28.069 --> 00:30:33.589
Caption: can somebody use this to exploit data

00:30:30.140 --> 00:30:36.170
Caption: how can somebody use this to harass the

00:30:33.589 --> 00:30:40.219
Caption: users of this application how can the

00:30:36.170 --> 00:30:45.790
Caption: software fail ethical technology fights

00:30:40.219 --> 00:30:49.369
Caption: against harmful societal inequalities in

00:30:45.790 --> 00:30:50.750
Caption: the last ethical principle is something

00:30:49.369 --> 00:30:55.160
Caption: that I&#39;ve talked about quite a bit and

00:30:50.750 --> 00:30:56.719
Caption: is diversity inclusion so the lack of

00:30:55.160 --> 00:30:59.239
Caption: diversity inclusion is one of the

00:30:56.719 --> 00:31:02.030
Caption: reasons why we find ourselves at such a

00:30:59.239 --> 00:31:04.849
Caption: crisis and increase increasing diversity

00:31:02.030 --> 00:31:11.270
Caption: inclusion is something that will help us

00:31:04.849 --> 00:31:13.969
Caption: to build a more welcoming ethical web if

00:31:11.270 --> 00:31:17.680
Caption: our team&#39;s organizations conferences

00:31:13.969 --> 00:31:20.239
Caption: friendship groups we&#39;re less homogeneous

00:31:17.680 --> 00:31:22.160
Caption: technological redlining in a form of

00:31:20.239 --> 00:31:24.800
Caption: algorithms reinforcing reinforcing

00:31:22.160 --> 00:31:26.780
Caption: oppressive social norms in enacting

00:31:24.800 --> 00:31:30.410
Caption: racial profiling might have been less

00:31:26.780 --> 00:31:33.739
Caption: much less significant because anyone no

00:31:30.410 --> 00:31:36.380
Caption: matter their background the race their

00:31:33.739 --> 00:31:42.559
Caption: gender level of ability deserves

00:31:36.380 --> 00:31:44.510
Caption: equality and equity when automated

00:31:42.560 --> 00:31:46.130
Caption: decision making tools are not built to

00:31:44.510 --> 00:31:48.459
Caption: explicitly dismantle structural

00:31:46.130 --> 00:31:52.130
Caption: inequalities their speed and scale

00:31:48.459 --> 00:31:55.909
Caption: intensifies them because if our systems

00:31:52.130 --> 00:31:59.780
Caption: are not effectively an actively fighting

00:31:55.910 --> 00:32:02.300
Caption: exclusion they&#39;re reinforced and if

00:31:59.780 --> 00:32:04.430
Caption: you&#39;re doing nothing you&#39;re reinforcing

00:32:02.300 --> 00:32:07.640
Caption: it

00:32:04.430 --> 00:32:11.119
Caption: so in that sense ethical technology it&#39;s

00:32:07.640 --> 00:32:14.109
Caption: inclusive of all people it prioritizes

00:32:11.119 --> 00:32:17.059
Caption: diverse teams and organizations it

00:32:14.109 --> 00:32:20.119
Caption: prevents that technological redlining

00:32:17.060 --> 00:32:30.110
Caption: that we just talked about and examples

00:32:20.119 --> 00:32:32.540
Caption: that we&#39;ve seen so how do we go about

00:32:30.109 --> 00:32:34.099
Caption: those principles there&#39;s a lot of take

00:32:32.540 --> 00:32:36.800
Caption: on there a lot of principles to be

00:32:34.099 --> 00:32:40.329
Caption: focusing on surely there are some tools

00:32:36.800 --> 00:32:42.530
Caption: of some resources that can help me in

00:32:40.329 --> 00:32:49.189
Caption: focusing on ethics and building a

00:32:42.530 --> 00:32:51.440
Caption: welcoming web last year&#39;s yielded quite

00:32:49.189 --> 00:32:52.939
Caption: a few books on algorithmic bias and the

00:32:51.439 --> 00:32:55.219
Caption: state of the tech industry

00:32:52.939 --> 00:32:57.739
Caption: some of them include weapons of mass

00:32:55.219 --> 00:33:00.380
Caption: destruction automating inequality

00:32:57.739 --> 00:33:04.130
Caption: algorithms of oppression technically

00:33:00.380 --> 00:33:07.010
Caption: wrong or reset by ellen power and these

00:33:04.130 --> 00:33:09.229
Caption: are fantastic resources that will let

00:33:07.010 --> 00:33:13.690
Caption: you understand more about algorithmic

00:33:09.229 --> 00:33:17.530
Caption: bias about the impact of technology and

00:33:13.689 --> 00:33:17.529
Caption: how to address that problem

00:33:18.339 --> 00:33:24.439
Caption: there are some fun projects like the

00:33:21.500 --> 00:33:27.260
Caption: tired cards of tech which is a deck of

00:33:24.439 --> 00:33:30.799
Caption: tarot cards of amazing questions and

00:33:27.260 --> 00:33:32.510
Caption: prompts about ethics and moral compass

00:33:30.800 --> 00:33:38.390
Caption: that will let you make more informed

00:33:32.510 --> 00:33:41.989
Caption: choices there are more structured

00:33:38.390 --> 00:33:44.390
Caption: resources such as the ethical thinking

00:33:41.989 --> 00:33:47.359
Caption: workshop that you can use at your

00:33:44.390 --> 00:33:48.969
Caption: organization in a very effective way

00:33:47.359 --> 00:33:52.280
Caption: that will guide you to making those

00:33:48.969 --> 00:33:53.780
Caption: ethical principles be at the core of

00:33:52.280 --> 00:33:57.820
Caption: everything you do whether it&#39;s software

00:33:53.780 --> 00:33:57.820
Caption: design business decision making

00:34:00.150 --> 00:34:04.229
Caption: and as much as manifestos and value

00:34:02.550 --> 00:34:06.090
Caption: statements mean nothing if you don&#39;t

00:34:04.229 --> 00:34:08.969
Caption: implement them and if you don&#39;t commit

00:34:06.089 --> 00:34:10.530
Caption: to actionable goals they can serve as a

00:34:08.969 --> 00:34:12.770
Caption: great source of inspiration and

00:34:10.530 --> 00:34:16.800
Caption: education and this is an example from

00:34:12.770 --> 00:34:18.209
Caption: Indy ethical design manifesto there&#39;s a

00:34:16.800 --> 00:34:20.250
Caption: company that&#39;s really committed to

00:34:18.209 --> 00:34:22.320
Caption: building an ethical web and there are

00:34:20.250 --> 00:34:25.080
Caption: many more out there designers oath is

00:34:22.320 --> 00:34:27.420
Caption: one of them and there&#39;s basically an

00:34:25.080 --> 00:34:30.150
Caption: endless list of resources that could be

00:34:27.419 --> 00:34:31.889
Caption: at your disposal to think what were my

00:34:30.149 --> 00:34:35.129
Caption: principles as a company or as an

00:34:31.889 --> 00:34:37.229
Caption: individual how do I start maybe setting

00:34:35.129 --> 00:34:39.659
Caption: those principles will be a great

00:34:37.229 --> 00:34:43.520
Caption: beginning to establishing this ethical

00:34:39.659 --> 00:34:43.520
Caption: welcoming web practice

00:34:50.270 --> 00:34:54.830
Caption: so I&#39;ve sure to quote at the very

00:34:52.638 --> 00:34:58.489
Caption: beginning of this talk and I didn&#39;t

00:34:54.830 --> 00:35:06.140
Caption: attribute it and I want to come back to

00:34:58.489 --> 00:35:10.310
Caption: it the anti-human web and if you don&#39;t

00:35:06.139 --> 00:35:11.929
Caption: remember sir term berners-lee is the man

00:35:10.310 --> 00:35:14.479
Caption: who literally invented the world wide

00:35:11.929 --> 00:35:18.050
Caption: web and his recent interview with Vanity

00:35:14.479 --> 00:35:21.010
Caption: Fair he expressed very negative feelings

00:35:18.050 --> 00:35:23.330
Caption: and the fact that he&#39;s devastated by the

00:35:21.010 --> 00:35:27.919
Caption: state of the web and what the web has

00:35:23.330 --> 00:35:31.280
Caption: become but at the same time though he is

00:35:27.919 --> 00:35:34.609
Caption: hopeful about the future he thinks we

00:35:31.280 --> 00:35:35.140
Caption: can turn this ship around and I think so

00:35:34.610 --> 00:35:37.160
Caption: too

00:35:35.139 --> 00:35:41.149
Caption: and that&#39;s why I&#39;m talking about this

00:35:37.159 --> 00:35:43.579
Caption: and that&#39;s why I&#39;m here today because I

00:35:41.149 --> 00:35:46.760
Caption: can see this shift towards more humane

00:35:43.580 --> 00:35:49.880
Caption: technology people first design empathy

00:35:46.760 --> 00:35:52.250
Caption: it is happening but it doesn&#39;t need your

00:35:49.879 --> 00:35:58.429
Caption: help because our common enemy is

00:35:52.250 --> 00:36:01.010
Caption: complacency and turning away it needs

00:35:58.429 --> 00:36:06.589
Caption: your help every single one of us in this

00:36:01.010 --> 00:36:08.868
Caption: room no matter what your job title is

00:36:06.590 --> 00:36:12.680
Caption: whether you&#39;re a freelancer employee

00:36:08.868 --> 00:36:15.679
Caption: business owner CEO you can be anyone but

00:36:12.679 --> 00:36:18.469
Caption: it&#39;s your responsibility and your

00:36:15.679 --> 00:36:21.770
Caption: accountability for the culture and

00:36:18.469 --> 00:36:24.760
Caption: technology you build we are all

00:36:21.770 --> 00:36:30.640
Caption: responsible for what the web has become

00:36:24.760 --> 00:36:30.639
Caption: today and it will become tomorrow

00:36:31.989 --> 00:36:35.858
Caption: for this reason it is crucial to

00:36:34.060 --> 00:36:38.890
Caption: understand those ethical challenges that

00:36:35.858 --> 00:36:42.250
Caption: we talked about just because we have a

00:36:38.889 --> 00:36:47.379
Caption: capacity to build something to design

00:36:42.250 --> 00:36:51.099
Caption: something with the use of technology we

00:36:47.379 --> 00:36:55.088
Caption: can fulfill our dreams of technology but

00:36:51.099 --> 00:36:57.220
Caption: it doesn&#39;t mean we should that focus on

00:36:55.089 --> 00:36:59.619
Caption: chasing disruption makes us forget that

00:36:57.219 --> 00:37:01.899
Caption: those digital tools are embedded in old

00:36:59.618 --> 00:37:06.969
Caption: systems of power and privilege and those

00:37:01.899 --> 00:37:08.889
Caption: systems have to be challenged the web

00:37:06.969 --> 00:37:12.368
Caption: shouldn&#39;t be negative the web shouldn&#39;t

00:37:08.889 --> 00:37:15.129
Caption: be oppressive the web shouldn&#39;t lead us

00:37:12.368 --> 00:37:17.348
Caption: to anxiety and depression and there&#39;s

00:37:15.129 --> 00:37:20.649
Caption: negative places it shouldn&#39;t compromise

00:37:17.349 --> 00:37:25.180
Caption: humanity the web should enhance our

00:37:20.649 --> 00:37:33.338
Caption: lives fulfill our dreams not magnify our

00:37:25.179 --> 00:37:37.088
Caption: fears deepen our divisions so today I&#39;m

00:37:33.339 --> 00:37:38.470
Caption: inviting you to carefully thing about

00:37:37.089 --> 00:37:40.300
Caption: our responsibilities and the

00:37:38.469 --> 00:37:46.679
Caption: ramifications of the work we do in the

00:37:40.300 --> 00:37:46.679
Caption: web today I urge you to take action

00:37:47.099 --> 00:37:54.140
Caption: let&#39;s build a better future together

00:37:51.929 --> 00:38:05.159
Caption: thank you

00:37:54.139 --> 00:38:08.379
Caption: [Applause]

00:38:05.159 --> 00:38:11.020
Caption: thank you so much I&#39;m gonna be here like

00:38:08.379 --> 00:38:13.750
Caption: playing with dongol sorry that was

00:38:11.020 --> 00:38:16.000
Caption: amazing I really really enjoyed the talk

00:38:13.750 --> 00:38:19.570
Caption: my main takeaway from it

00:38:16.000 --> 00:38:22.139
Caption: I love the vision of having like ethical

00:38:19.570 --> 00:38:24.820
Caption: technology that helps all of us equally

00:38:22.139 --> 00:38:26.439
Caption: and enhances our lives and fulfills our

00:38:24.820 --> 00:38:29.800
Caption: dreams because that is that&#39;s an amazing

00:38:26.439 --> 00:38:31.119
Caption: vision isn&#39;t it and I guess what I took

00:38:29.800 --> 00:38:33.070
Caption: from that is that all of us have a role

00:38:31.120 --> 00:38:35.860
Caption: to play and I guess the responsibility

00:38:33.070 --> 00:38:37.000
Caption: in making this happen and everyone in

00:38:35.860 --> 00:38:39.850
Caption: this room does right because we&#39;re all

00:38:37.000 --> 00:38:42.040
Caption: part of the tech industry yeah remember

00:38:39.850 --> 00:38:45.400
Caption: that it&#39;s not only the CEOs or you don&#39;t

00:38:42.040 --> 00:38:46.689
Caption: have to be a company owner or someone

00:38:45.399 --> 00:38:48.609
Caption: doesn&#39;t have to consider you an

00:38:46.689 --> 00:38:50.439
Caption: important person as every single one of

00:38:48.610 --> 00:38:53.800
Caption: you if each one of us leaves this room

00:38:50.439 --> 00:38:56.020
Caption: and does something ethical or changes

00:38:53.800 --> 00:38:57.760
Caption: their values or making the industry bad

00:38:56.020 --> 00:39:00.370
Caption: yes save my time the industry will be

00:38:57.760 --> 00:39:01.899
Caption: better day by day I was loved don&#39;t

00:39:00.370 --> 00:39:03.130
Caption: collect it don&#39;t store it don&#39;t keep it

00:39:01.899 --> 00:39:06.669
Caption: because very simple but it&#39;s very

00:39:03.129 --> 00:39:08.619
Caption: effective that&#39;s cool and I think you

00:39:06.669 --> 00:39:10.419
Caption: know if all of us can help turn the ship

00:39:08.620 --> 00:39:17.620
Caption: around and make Tim berners-lee proud

00:39:10.419 --> 00:39:25.658
Caption: then thank you thank you so much Kelly

00:39:17.620 --> 00:39:25.659
Caption: [Applause]

00:39:26.969 --> 00:39:29.030
Caption: you

