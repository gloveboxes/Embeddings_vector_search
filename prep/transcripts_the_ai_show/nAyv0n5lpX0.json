{"speaker": "", "title": "ONNX Runtime", "videoId": "nAyv0n5lpX0", "description": "ONNX Runtime inference engine is capable of executing ML models in different HW environments, taking advantage of the neural network acceleration capabilities. Microsoft and Xilinx worked together to integrate ONNX Runtime with the VitisAI SW libraries for executing ONNX models in the Xilinx U250 FPGAs. We are happy to introduce the preview release of this capability today.\n\nJump To: \n[06:15] Demo by PeakSpeed for satellite imaging Orthorectification \n\nLearn More:\nONNX Runtime https://aka.ms/AIShow/OnnxruntimeGithub\nONNX Runtime + VitisAI https://aka.ms/AIShow/VitisAi\nFollow: https://twitter.com/onnxruntime\n\nThe AI Show's Favorite links:\nDon't miss new episodes, subscribe to the AI Show: https://aka.ms/aishowsubscribe  \nCreate a Free account (Azure): https://aka.ms/aishow-seth-azurefree\nDeep Learning vs. Machine Learning: https://aka.ms/deeplearningmachinelea...\nGet Started with Machine Learning: https://aka.ms/GetStartedWithMachineL... \nFollow: https://twitter.com/sethjuarez\nFollow: https://twitter.com/ch9\nFollow: https://twitter.com/Azure\nFollow: https://twitter.com/msdev"}