[
    {
        "text": ">> Make sure to stay tuned to",
        "start": 0.0,
        "duration": 2.04
    },
    {
        "text": "this episode of the AI show where Manash Goswami,",
        "start": 2.04,
        "duration": 2.79
    },
    {
        "text": "Principal Program Manager for AI Frameworks,",
        "start": 4.83,
        "duration": 2.43
    },
    {
        "text": "tells us all about the ONNX Runtime,",
        "start": 7.26,
        "duration": 2.595
    },
    {
        "text": "what's new, and even some amazing hardware announcements.",
        "start": 9.855,
        "duration": 3.66
    },
    {
        "text": "Hint: there's partners involved,",
        "start": 13.515,
        "duration": 2.25
    },
    {
        "text": "FPGAs, special guests. Enjoy.",
        "start": 15.765,
        "duration": 2.985
    },
    {
        "text": "[MUSIC]",
        "start": 18.75,
        "duration": 8.13
    },
    {
        "text": ">> Hello, and welcome to this",
        "start": 26.88,
        "duration": 1.71
    },
    {
        "text": "on-demand session about ONNX Runtime.",
        "start": 28.59,
        "duration": 2.57
    },
    {
        "text": "Today, I'm going to talk about our work with",
        "start": 31.16,
        "duration": 2.85
    },
    {
        "text": "Xilinx to integrate ONNX Runtime with their FPGA solution.",
        "start": 34.01,
        "duration": 3.9
    },
    {
        "text": "I'm Manash Goswami, a Program",
        "start": 37.91,
        "duration": 2.07
    },
    {
        "text": "Manager in Microsoft's AI Frameworks team.",
        "start": 39.98,
        "duration": 3.37
    },
    {
        "text": "The Open Neural Network Exchange or ONNX is",
        "start": 44.45,
        "duration": 3.54
    },
    {
        "text": "an open source format for representing ML models,",
        "start": 47.99,
        "duration": 2.67
    },
    {
        "text": "both traditional ML and DNNs.",
        "start": 50.66,
        "duration": 2.49
    },
    {
        "text": "There are converters available to convert from",
        "start": 53.15,
        "duration": 2.58
    },
    {
        "text": "different training frameworks to ONNX.",
        "start": 55.73,
        "duration": 2.745
    },
    {
        "text": "By having a common model format,",
        "start": 58.475,
        "duration": 1.815
    },
    {
        "text": "we provide data science teams with the freedom to",
        "start": 60.29,
        "duration": 2.1
    },
    {
        "text": "choose their tools for development and training,",
        "start": 62.39,
        "duration": 3.72
    },
    {
        "text": "while ensuring that their model can be represented in",
        "start": 66.11,
        "duration": 2.49
    },
    {
        "text": "a common way that can be easily deployed and executed.",
        "start": 68.6,
        "duration": 3.875
    },
    {
        "text": "We achieve this by using",
        "start": 72.475,
        "duration": 2.615
    },
    {
        "text": "the ONNX Runtime inference engine to execute ONNX models.",
        "start": 75.09,
        "duration": 2.9
    },
    {
        "text": "On the other hand,",
        "start": 77.99,
        "duration": 1.35
    },
    {
        "text": "having a Standard Model format",
        "start": 79.34,
        "duration": 1.59
    },
    {
        "text": "allows hardware platform makers to focus on",
        "start": 80.93,
        "duration": 2.07
    },
    {
        "text": "innovation for optimizing",
        "start": 83.0,
        "duration": 1.23
    },
    {
        "text": "neural network computations on their platform.",
        "start": 84.23,
        "duration": 3.075
    },
    {
        "text": "This common format for the machine learning model",
        "start": 87.305,
        "duration": 3.45
    },
    {
        "text": "and the ONNX Runtime inference engine allows us to decouple",
        "start": 90.755,
        "duration": 3.675
    },
    {
        "text": "the data science world of model development from",
        "start": 94.43,
        "duration": 3.45
    },
    {
        "text": "the fragmented hardware execution environments",
        "start": 97.88,
        "duration": 2.565
    },
    {
        "text": "in the Cloud and the Edge.",
        "start": 100.445,
        "duration": 2.545
    },
    {
        "text": "ONNX Runtime is the open source inference engine",
        "start": 103.22,
        "duration": 3.945
    },
    {
        "text": "to execute ONNX models.",
        "start": 107.165,
        "duration": 2.3
    },
    {
        "text": "It was designed with a focus on performance, extensibility,",
        "start": 109.465,
        "duration": 3.534
    },
    {
        "text": "and scalability in order to support heavy workloads and",
        "start": 112.999,
        "duration": 3.211
    },
    {
        "text": "traffic as a production solution on the Cloud and Edge.",
        "start": 116.21,
        "duration": 4.04
    },
    {
        "text": "The runtime is optimized to implement",
        "start": 120.25,
        "duration": 3.205
    },
    {
        "text": "all the operators defined in the ONNX spec,",
        "start": 123.455,
        "duration": 2.955
    },
    {
        "text": "and therefore, it can run any valid ONNX model.",
        "start": 126.41,
        "duration": 4.085
    },
    {
        "text": "The ONNX Runtime supports the notion of",
        "start": 130.495,
        "duration": 3.655
    },
    {
        "text": "a custom operator that can run alongside official ONNX operators,",
        "start": 134.15,
        "duration": 4.535
    },
    {
        "text": "which means that as operators get introduced into the spec,",
        "start": 138.685,
        "duration": 4.825
    },
    {
        "text": "the ONNX Runtime is capable of running then even",
        "start": 143.51,
        "duration": 3.24
    },
    {
        "text": "before the spec is standardized.",
        "start": 146.75,
        "duration": 3.87
    },
    {
        "text": "The runtime is backwards and forwards compatible so",
        "start": 150.62,
        "duration": 3.63
    },
    {
        "text": "that as models are updated to newer versions of the offset,",
        "start": 154.25,
        "duration": 3.12
    },
    {
        "text": "the ONNX Runtime can execute them.",
        "start": 157.37,
        "duration": 2.6
    },
    {
        "text": "The ONNX random architecture supports",
        "start": 159.97,
        "duration": 3.22
    },
    {
        "text": "this extensible login interface",
        "start": 163.19,
        "duration": 2.46
    },
    {
        "text": "called the execution provider interface,",
        "start": 165.65,
        "duration": 2.19
    },
    {
        "text": "and this is used to run",
        "start": 167.84,
        "duration": 2.549
    },
    {
        "text": "models on different hardware optimized environments.",
        "start": 170.389,
        "duration": 3.631
    },
    {
        "text": "We will talk about the",
        "start": 174.02,
        "duration": 1.35
    },
    {
        "text": "ONNX Runtime Execution provider in a subsequent slide.",
        "start": 175.37,
        "duration": 2.97
    },
    {
        "text": "Finally, ONNX Runtime is integrated inside WinML,",
        "start": 178.34,
        "duration": 3.12
    },
    {
        "text": "which is the ML execution environment in Windows 10.",
        "start": 181.46,
        "duration": 4.45
    },
    {
        "text": "Today, we are releasing an update for",
        "start": 186.83,
        "duration": 2.97
    },
    {
        "text": "the ONNX Runtime Inference Engine version",
        "start": 189.8,
        "duration": 2.31
    },
    {
        "text": "1.3 This update includes support for ONNX spec version 1.7.",
        "start": 192.11,
        "duration": 6.53
    },
    {
        "text": "We are announcing the general availability for",
        "start": 198.64,
        "duration": 2.71
    },
    {
        "text": "the direct ML execution provider on Windows.",
        "start": 201.35,
        "duration": 3.165
    },
    {
        "text": "This provides acceleration for",
        "start": 204.515,
        "duration": 2.595
    },
    {
        "text": "ONNX model inference across",
        "start": 207.11,
        "duration": 2.01
    },
    {
        "text": "different platforms on Windows devices.",
        "start": 209.12,
        "duration": 2.405
    },
    {
        "text": "We are releasing a number of",
        "start": 211.525,
        "duration": 1.93
    },
    {
        "text": "other execution providers in preview to support",
        "start": 213.455,
        "duration": 2.355
    },
    {
        "text": "ONNX models across different platforms expanding our ecosystem.",
        "start": 215.81,
        "duration": 4.815
    },
    {
        "text": "The Python package for",
        "start": 220.625,
        "duration": 2.085
    },
    {
        "text": "ARM64 devices is going to be published starting this release.",
        "start": 222.71,
        "duration": 4.695
    },
    {
        "text": "Additional details about the",
        "start": 227.405,
        "duration": 3.255
    },
    {
        "text": "version 1.3 release can be found in the ONNX Runtime GitHub page.",
        "start": 230.66,
        "duration": 4.84
    },
    {
        "text": "The ONNX Runtime enables ML model execution on",
        "start": 236.42,
        "duration": 4.62
    },
    {
        "text": "different hardware platforms through",
        "start": 241.04,
        "duration": 1.935
    },
    {
        "text": "the execution provider interface.",
        "start": 242.975,
        "duration": 2.235
    },
    {
        "text": "This allows hardware platforms to",
        "start": 245.21,
        "duration": 2.16
    },
    {
        "text": "integrate with the ONNX Runtime by building",
        "start": 247.37,
        "duration": 1.98
    },
    {
        "text": "an execution provider for that specific compute endpoint.",
        "start": 249.35,
        "duration": 4.2
    },
    {
        "text": "We have a number of execution providers",
        "start": 253.55,
        "duration": 2.369
    },
    {
        "text": "from our hardware ecosystem partners across",
        "start": 255.919,
        "duration": 2.221
    },
    {
        "text": "the breadth of options from CPUs",
        "start": 258.14,
        "duration": 2.25
    },
    {
        "text": "to GPUs and custom neural accelerators.",
        "start": 260.39,
        "duration": 2.49
    },
    {
        "text": "This execution providers are used to execute models in the Cloud,",
        "start": 262.88,
        "duration": 4.81
    },
    {
        "text": "as well as on Edge devices.",
        "start": 267.69,
        "duration": 1.845
    },
    {
        "text": "Today, we are happy to announce a new partner",
        "start": 269.535,
        "duration": 3.425
    },
    {
        "text": "in our growing hardware ecosystem for ONNX Runtime.",
        "start": 272.96,
        "duration": 4.3
    },
    {
        "text": "We have partnered with Xilinx to build",
        "start": 278.24,
        "duration": 4.14
    },
    {
        "text": "the execution provider for the Vitis AI software library.",
        "start": 282.38,
        "duration": 4.29
    },
    {
        "text": "This execution provider is used to execute",
        "start": 286.67,
        "duration": 2.79
    },
    {
        "text": "ONNX models on the Xilinx U250 FPGA platform.",
        "start": 289.46,
        "duration": 4.62
    },
    {
        "text": "Developers can now take advantage of",
        "start": 294.08,
        "duration": 3.42
    },
    {
        "text": "this execution provider to build and",
        "start": 297.5,
        "duration": 2.94
    },
    {
        "text": "execute models on this Xilinx FPGA platform.",
        "start": 300.44,
        "duration": 4.09
    },
    {
        "text": "The Vitis AI software library is",
        "start": 304.63,
        "duration": 3.22
    },
    {
        "text": "the Xilinx development platform",
        "start": 307.85,
        "duration": 1.949
    },
    {
        "text": "for AI inference on Xilinx hardware platforms.",
        "start": 309.799,
        "duration": 2.866
    },
    {
        "text": "The U250 FPGA is available in private preview in Azure",
        "start": 312.665,
        "duration": 4.71
    },
    {
        "text": "as the NP VMs queue for",
        "start": 317.375,
        "duration": 1.995
    },
    {
        "text": "users to use with the Vitis AI software stack.",
        "start": 319.37,
        "duration": 3.255
    },
    {
        "text": "The Vitis AI toolkit consists of",
        "start": 322.625,
        "duration": 2.205
    },
    {
        "text": "optimized IP tools, libraries, models,",
        "start": 324.83,
        "duration": 2.955
    },
    {
        "text": "and example designs for developers to use with the FPGAs.",
        "start": 327.785,
        "duration": 5.715
    },
    {
        "text": "This allows FPGA developers to infuse",
        "start": 333.5,
        "duration": 3.36
    },
    {
        "text": "AI inferencing and acceleration on the FPGA.",
        "start": 336.86,
        "duration": 4.18
    },
    {
        "text": "Azure provides a wide variety of",
        "start": 341.53,
        "duration": 2.8
    },
    {
        "text": "infrastructure options specialized for your big compute workloads.",
        "start": 344.33,
        "duration": 3.875
    },
    {
        "text": "The Azure NP VM with Xilinx,",
        "start": 348.205,
        "duration": 2.225
    },
    {
        "text": "LVO U250 FPGA accelerator is in private preview now.",
        "start": 350.43,
        "duration": 3.92
    },
    {
        "text": "This queue offers from one to four Xilinx U250 FPGA devices as",
        "start": 354.35,
        "duration": 5.43
    },
    {
        "text": "an Azure VM backed by",
        "start": 359.78,
        "duration": 1.47
    },
    {
        "text": "powerful Xeon platinum CPU cores and fast NVMe-based storage.",
        "start": 361.25,
        "duration": 5.294
    },
    {
        "text": "The NP series will enable true lift and shift and",
        "start": 366.544,
        "duration": 3.886
    },
    {
        "text": "single dark-end development of",
        "start": 370.43,
        "duration": 1.635
    },
    {
        "text": "FPGA applications for general purpose Cloud applications.",
        "start": 372.065,
        "duration": 3.465
    },
    {
        "text": "This VM is based on",
        "start": 375.53,
        "duration": 1.53
    },
    {
        "text": "a board and software ecosystem for the Xilinx U250 card.",
        "start": 377.06,
        "duration": 5.065
    },
    {
        "text": "The Vitis software stack and",
        "start": 382.125,
        "duration": 2.085
    },
    {
        "text": "ONNX Runtime will run on this Azure VM,",
        "start": 384.21,
        "duration": 2.51
    },
    {
        "text": "as well as on other on-prem and Edge devices with the Xilinx FPGA.",
        "start": 386.72,
        "duration": 4.485
    },
    {
        "text": "This solution enables the bleeding edge",
        "start": 391.205,
        "duration": 2.175
    },
    {
        "text": "of accelerator development to",
        "start": 393.38,
        "duration": 2.01
    },
    {
        "text": "harness the power of Cloud without additional development costs.",
        "start": 395.39,
        "duration": 5.26
    },
    {
        "text": "Xilinx and Microsoft collaborated with Peakspeed,",
        "start": 401.9,
        "duration": 3.5
    },
    {
        "text": "an innovative startup that provides",
        "start": 405.4,
        "duration": 2.07
    },
    {
        "text": "geospatial analytic solutions to",
        "start": 407.47,
        "duration": 2.28
    },
    {
        "text": "integrate ONNX Runtime and the Vitis AI stack.",
        "start": 409.75,
        "duration": 3.765
    },
    {
        "text": "This solution seamlessly integrates",
        "start": 413.515,
        "duration": 2.295
    },
    {
        "text": "Peakspeed's deep-learning object classification models",
        "start": 415.81,
        "duration": 3.04
    },
    {
        "text": "and uses ONNX Runtime with",
        "start": 418.85,
        "duration": 1.34
    },
    {
        "text": "the Vitis AI execution provider to accelerate",
        "start": 420.19,
        "duration": 3.03
    },
    {
        "text": "the pipeline on the new Azure NP Series FPGA platform.",
        "start": 423.22,
        "duration": 3.87
    },
    {
        "text": "To discuss more about the solution,",
        "start": 427.09,
        "duration": 2.43
    },
    {
        "text": "please welcome Oscar Kramer, Peakspeed's chief scientists.",
        "start": 429.52,
        "duration": 4.21
    },
    {
        "text": ">> Thanks Manash. Here at Peakspeed,",
        "start": 433.85,
        "duration": 3.22
    },
    {
        "text": "we're very excited to be one of the first users of",
        "start": 437.07,
        "duration": 2.44
    },
    {
        "text": "Microsoft's new NP Series Azure VMs",
        "start": 439.51,
        "duration": 3.255
    },
    {
        "text": "for high-performance computing.",
        "start": 442.765,
        "duration": 1.805
    },
    {
        "text": "We've been focused on building",
        "start": 444.57,
        "duration": 1.79
    },
    {
        "text": "the world's fastest geospatial imaging solutions,",
        "start": 446.36,
        "duration": 3.285
    },
    {
        "text": "and Azure NP will enable us to meet some critical demands.",
        "start": 449.645,
        "duration": 4.16
    },
    {
        "text": "Today, I'll be showing Peakspeed's TrueView",
        "start": 453.805,
        "duration": 2.805
    },
    {
        "text": "and identify applications that",
        "start": 456.61,
        "duration": 1.93
    },
    {
        "text": "leverage the power of the FPGA for",
        "start": 458.54,
        "duration": 2.52
    },
    {
        "text": "performing compute intensive operations.",
        "start": 461.06,
        "duration": 2.97
    },
    {
        "text": "Here, we're using Esri's ArcGIS Pro application to illustrate.",
        "start": 464.03,
        "duration": 4.56
    },
    {
        "text": "This image over Estes Park in",
        "start": 468.59,
        "duration": 2.31
    },
    {
        "text": "the Colorado Rockies was provided by Maxar.",
        "start": 470.9,
        "duration": 2.885
    },
    {
        "text": "This area of about 100 square kilometers has high terrain relief,",
        "start": 473.785,
        "duration": 4.315
    },
    {
        "text": "resulting in significant distortions",
        "start": 478.1,
        "duration": 2.009
    },
    {
        "text": "due to the sensor's perspective.",
        "start": 480.109,
        "duration": 2.16
    },
    {
        "text": "Let's fly over to the Stanley Hotel",
        "start": 482.269,
        "duration": 2.071
    },
    {
        "text": "and look at this curved road here.",
        "start": 484.34,
        "duration": 1.875
    },
    {
        "text": "We can see a big offset between the map and the image.",
        "start": 486.215,
        "duration": 3.675
    },
    {
        "text": "To get the image to line up,",
        "start": 489.89,
        "duration": 1.875
    },
    {
        "text": "it needs to undergo a geospatial correction or",
        "start": 491.765,
        "duration": 3.135
    },
    {
        "text": "orthorectification that resamples every pixel",
        "start": 494.9,
        "duration": 3.39
    },
    {
        "text": "from the satellite perspective to a map projection.",
        "start": 498.29,
        "duration": 2.835
    },
    {
        "text": "Until now, this was being performed on CPUs, but Peakspeed,",
        "start": 501.125,
        "duration": 4.134
    },
    {
        "text": "using the FPGA on",
        "start": 505.259,
        "duration": 1.461
    },
    {
        "text": "an Azure NP instance has",
        "start": 506.72,
        "duration": 2.19
    },
    {
        "text": "managed to accelerate that process considerably.",
        "start": 508.91,
        "duration": 2.805
    },
    {
        "text": "Let's look at the performance numbers.",
        "start": 511.715,
        "duration": 2.37
    },
    {
        "text": "Here are Peakspeed's DevOps,",
        "start": 514.085,
        "duration": 1.755
    },
    {
        "text": "pipeline outputs that we use for bench-marking.",
        "start": 515.84,
        "duration": 2.945
    },
    {
        "text": "These numbers compare TrueView running on an Azure NP hosting",
        "start": 518.785,
        "duration": 4.585
    },
    {
        "text": "the Xilinx U250 FPGA against",
        "start": 523.37,
        "duration": 3.0
    },
    {
        "text": "the same algorithm running on a Xeon platinum CPU.",
        "start": 526.37,
        "duration": 3.51
    },
    {
        "text": "We're looking at 4 minutes 21 seconds on",
        "start": 529.88,
        "duration": 2.73
    },
    {
        "text": "the CPU versus only 1.7 seconds on the FPGA.",
        "start": 532.61,
        "duration": 4.86
    },
    {
        "text": "Let's go back to ArcGIS Pro",
        "start": 537.47,
        "duration": 2.52
    },
    {
        "text": "and look at the output produced by TrueView.",
        "start": 539.99,
        "duration": 2.535
    },
    {
        "text": "Here's a same image after orthorectification.",
        "start": 542.525,
        "duration": 3.455
    },
    {
        "text": "We'll zoom into the same spot,",
        "start": 545.98,
        "duration": 2.11
    },
    {
        "text": "and you can see that now,",
        "start": 548.09,
        "duration": 1.26
    },
    {
        "text": "it lines up perfectly with a base map.",
        "start": 549.35,
        "duration": 2.79
    },
    {
        "text": "Let's zoom back out and switch between the two images.",
        "start": 552.14,
        "duration": 3.345
    },
    {
        "text": "This gives you an idea of",
        "start": 555.485,
        "duration": 1.755
    },
    {
        "text": "the highly non-linear transformation involved.",
        "start": 557.24,
        "duration": 3.165
    },
    {
        "text": "Now that we have an ortho image,",
        "start": 560.405,
        "duration": 1.965
    },
    {
        "text": "we're ready for object detection.",
        "start": 562.37,
        "duration": 2.13
    },
    {
        "text": "This is where the ONNX Runtime running",
        "start": 564.5,
        "duration": 2.37
    },
    {
        "text": "Xilinx's Vitis AI library come in.",
        "start": 566.87,
        "duration": 3.09
    },
    {
        "text": "We'll be using Peakspeed's identify training models",
        "start": 569.96,
        "duration": 3.225
    },
    {
        "text": "to detect the cars in the satellite image.",
        "start": 573.185,
        "duration": 2.63
    },
    {
        "text": "The production pipeline is implemented in a Python script,",
        "start": 575.815,
        "duration": 3.475
    },
    {
        "text": "and we can easily flip between CPU, FPGA,",
        "start": 579.29,
        "duration": 3.285
    },
    {
        "text": "or GPU via a runtime parameter,",
        "start": 582.575,
        "duration": 2.655
    },
    {
        "text": "depending on our needs.",
        "start": 585.23,
        "duration": 1.68
    },
    {
        "text": "In this case, using an Azure NP instance,",
        "start": 586.91,
        "duration": 2.94
    },
    {
        "text": "customers will see a significant savings in",
        "start": 589.85,
        "duration": 2.25
    },
    {
        "text": "TCO and faster response times.",
        "start": 592.1,
        "duration": 2.88
    },
    {
        "text": "The ONNX Runtime enables Peakspeed to",
        "start": 594.98,
        "duration": 2.7
    },
    {
        "text": "target a variety of acceleration platforms in Azure,",
        "start": 597.68,
        "duration": 3.285
    },
    {
        "text": "and that helps us provide the best value to our customers.",
        "start": 600.965,
        "duration": 3.57
    },
    {
        "text": "Thanks, and back to you, Manash.",
        "start": 604.535,
        "duration": 2.845
    },
    {
        "text": ">> Thanks, Oscar. Peakspeed was able to",
        "start": 609.68,
        "duration": 3.08
    },
    {
        "text": "deploy their deep learning models on the Azure NP VM,",
        "start": 612.76,
        "duration": 2.9
    },
    {
        "text": "hence a great performance.",
        "start": 615.66,
        "duration": 1.62
    },
    {
        "text": "The YOLOv3 model showed improvements on",
        "start": 617.28,
        "duration": 2.29
    },
    {
        "text": "the VM relative to GPU instances,",
        "start": 619.57,
        "duration": 2.44
    },
    {
        "text": "and the end-to-end geospatial analytics pipeline",
        "start": 622.01,
        "duration": 2.54
    },
    {
        "text": "ran 4x faster than on a GPU.",
        "start": 624.55,
        "duration": 2.52
    },
    {
        "text": "ISPs and customers can now infuse",
        "start": 627.07,
        "duration": 2.46
    },
    {
        "text": "their applications with deep-learning using",
        "start": 629.53,
        "duration": 2.86
    },
    {
        "text": "ONNX Runtime and the Vitis AI stack and",
        "start": 632.39,
        "duration": 2.81
    },
    {
        "text": "accelerate on the FPGA endpoints in Azure,",
        "start": 635.2,
        "duration": 2.94
    },
    {
        "text": "as well as on-prem with the Xilinx U250 hardware.",
        "start": 638.14,
        "duration": 5.5
    },
    {
        "text": "Developers can take advantage of this integration by",
        "start": 645.08,
        "duration": 5.43
    },
    {
        "text": "building the Vitis AI execution provider",
        "start": 650.51,
        "duration": 2.87
    },
    {
        "text": "with ONNX Runtime from source.",
        "start": 653.38,
        "duration": 1.885
    },
    {
        "text": "You can do that from the ONNX Runtime, GitHub master repo.",
        "start": 655.265,
        "duration": 3.87
    },
    {
        "text": "Additionally, very soon,",
        "start": 659.135,
        "duration": 2.595
    },
    {
        "text": "Xilinx will release a VM image in Azure Marketplace with",
        "start": 661.73,
        "duration": 3.765
    },
    {
        "text": "all the pre-built software libraries integrated in",
        "start": 665.495,
        "duration": 2.805
    },
    {
        "text": "one place for easy deployment into the Azure NP VM.",
        "start": 668.3,
        "duration": 4.57
    },
    {
        "text": "Thank you for listening,",
        "start": 674.75,
        "duration": 2.135
    },
    {
        "text": "and I hope you have a great day.",
        "start": 676.885,
        "duration": 1.555
    },
    {
        "text": "[MUSIC]",
        "start": 678.44,
        "duration": 9.47
    }
]