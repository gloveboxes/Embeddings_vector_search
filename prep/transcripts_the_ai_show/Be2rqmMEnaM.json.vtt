[
    {
        "text": ">> You're not going to want to miss this episode of",
        "start": 0.0,
        "duration": 1.44
    },
    {
        "text": "the AI show where we show you how to upload",
        "start": 1.44,
        "duration": 2.19
    },
    {
        "text": "your own ML models and deploy",
        "start": 3.63,
        "duration": 2.61
    },
    {
        "text": "them with a single click. Make sure you tune it.",
        "start": 6.24,
        "duration": 2.73
    },
    {
        "text": "[MUSIC]",
        "start": 8.97,
        "duration": 4.44
    },
    {
        "text": ">> Hello and welcome to this episode of the AI Show,",
        "start": 13.41,
        "duration": 1.88
    },
    {
        "text": "we're going to talk a little bit about machine learning models.",
        "start": 15.29,
        "duration": 3.445
    },
    {
        "text": "Tell us who you are what you do, my friend.",
        "start": 18.735,
        "duration": 1.605
    },
    {
        "text": ">> Hi, I am Shivani Patel and I work on",
        "start": 20.34,
        "duration": 2.82
    },
    {
        "text": "Azure machine learning specifically machine-learning for DevOps.",
        "start": 23.16,
        "duration": 3.42
    },
    {
        "text": ">> Fantastic. So let's talk about",
        "start": 26.58,
        "duration": 1.17
    },
    {
        "text": "machine learning models. What are they?",
        "start": 27.75,
        "duration": 1.57
    },
    {
        "text": ">> So basically  Azure machine learning model is a set of files.",
        "start": 29.32,
        "duration": 4.505
    },
    {
        "text": "So you have your pick-up file,",
        "start": 33.825,
        "duration": 1.275
    },
    {
        "text": "dependency files to be able to run your model.",
        "start": 35.1,
        "duration": 2.04
    },
    {
        "text": "It's encompassing all of that.",
        "start": 37.14,
        "duration": 1.83
    },
    {
        "text": ">> So it's basically a file or set of files",
        "start": 38.97,
        "duration": 2.28
    },
    {
        "text": "that let you run the machine learning model.",
        "start": 41.25,
        "duration": 2.37
    },
    {
        "text": "Now, in Azure machine learning service,",
        "start": 43.62,
        "duration": 1.695
    },
    {
        "text": "we have the ability to save",
        "start": 45.315,
        "duration": 1.655
    },
    {
        "text": "these things into something called a model registry,",
        "start": 46.97,
        "duration": 2.23
    },
    {
        "text": "why don't you tell us what that is for those who don't know.",
        "start": 49.2,
        "duration": 1.61
    },
    {
        "text": ">> Sure, so basically a modern registry",
        "start": 50.81,
        "duration": 1.77
    },
    {
        "text": "in simple terms it's just a model store.",
        "start": 52.58,
        "duration": 1.89
    },
    {
        "text": "It's a collection of all of those files plus",
        "start": 54.47,
        "duration": 2.28
    },
    {
        "text": "a bunch of metadata that we collect on top of it.",
        "start": 56.75,
        "duration": 2.34
    },
    {
        "text": "So you make sure that you're able to track",
        "start": 59.09,
        "duration": 1.92
    },
    {
        "text": "all the information related to your model.",
        "start": 61.01,
        "duration": 2.325
    },
    {
        "text": ">> I see. So if you're a dev,",
        "start": 63.335,
        "duration": 1.23
    },
    {
        "text": "this is like a way of versioning",
        "start": 64.565,
        "duration": 1.485
    },
    {
        "text": "your assembly or DLL files",
        "start": 66.05,
        "duration": 2.34
    },
    {
        "text": "or whatever. It's basically the same thing?",
        "start": 68.39,
        "duration": 1.16
    },
    {
        "text": ">> Exactly, for data scientists to be able",
        "start": 69.55,
        "duration": 1.6
    },
    {
        "text": "to store and version all of",
        "start": 71.15,
        "duration": 1.68
    },
    {
        "text": "these models instead of just having",
        "start": 72.83,
        "duration": 1.56
    },
    {
        "text": "them be in a bunch of files on their local machine.",
        "start": 74.39,
        "duration": 2.06
    },
    {
        "text": ">> Or e-mailing them to each other which I know you do.",
        "start": 76.45,
        "duration": 2.77
    },
    {
        "text": "You need to stop doing that.",
        "start": 79.22,
        "duration": 1.23
    },
    {
        "text": "So what's new in the model registry coming up?",
        "start": 80.45,
        "duration": 3.08
    },
    {
        "text": ">> So right now in the model registry,",
        "start": 83.53,
        "duration": 2.51
    },
    {
        "text": "you basically have your model,",
        "start": 86.04,
        "duration": 1.41
    },
    {
        "text": "your model version, which run it",
        "start": 87.45,
        "duration": 2.25
    },
    {
        "text": "came from if you did your training on Azure machine learning.",
        "start": 89.7,
        "duration": 2.66
    },
    {
        "text": "How we're expanding is we're enabling you to actually",
        "start": 92.36,
        "duration": 2.76
    },
    {
        "text": "provide a framework that you chose to have your model in,",
        "start": 95.12,
        "duration": 3.135
    },
    {
        "text": "as well as a framework version,",
        "start": 98.255,
        "duration": 1.77
    },
    {
        "text": "sample input data, sample output data,",
        "start": 100.025,
        "duration": 2.775
    },
    {
        "text": "input schema, and output schema.",
        "start": 102.8,
        "duration": 2.0
    },
    {
        "text": ">> So if I'm understanding this right,",
        "start": 104.8,
        "duration": 2.42
    },
    {
        "text": "you're saying I can basically",
        "start": 107.22,
        "duration": 1.49
    },
    {
        "text": "describe what I want to go in a model?",
        "start": 108.71,
        "duration": 3.03
    },
    {
        "text": "Because to me, a model is",
        "start": 111.74,
        "duration": 1.38
    },
    {
        "text": "just a glorified function that we're too lazy to write it,",
        "start": 113.12,
        "duration": 2.12
    },
    {
        "text": "we just given data and it figures it out, right?",
        "start": 115.24,
        "duration": 1.6
    },
    {
        "text": ">> Yes.",
        "start": 116.84,
        "duration": 0.21
    },
    {
        "text": ">> So basically, all we have to do is tell",
        "start": 117.05,
        "duration": 1.95
    },
    {
        "text": "it what goes in and what goes out,",
        "start": 119.0,
        "duration": 2.22
    },
    {
        "text": "and I don't have to write a scoring file",
        "start": 121.22,
        "duration": 1.77
    },
    {
        "text": "anymore, is that what you're saying?",
        "start": 122.99,
        "duration": 1.83
    },
    {
        "text": ">> Currently. It's for certain frameworks.",
        "start": 124.82,
        "duration": 2.868
    },
    {
        "text": ">> I got it.",
        "start": 127.688,
        "duration": 0.312
    },
    {
        "text": ">> We're introducing this service called No code deployment.",
        "start": 128.0,
        "duration": 3.141
    },
    {
        "text": ">> Okay.",
        "start": 131.141,
        "duration": 0.269
    },
    {
        "text": ">> Now basically for ONNX Models,",
        "start": 131.41,
        "duration": 2.38
    },
    {
        "text": "Auto ML models, scikit-learn and TensorFlow Keras models,",
        "start": 133.79,
        "duration": 3.96
    },
    {
        "text": "we are able to actually ask you to tell us what framework it is,",
        "start": 137.75,
        "duration": 3.8
    },
    {
        "text": "what version it is and we'll go",
        "start": 141.55,
        "duration": 1.63
    },
    {
        "text": "ahead and actually create a driver file basically,",
        "start": 143.18,
        "duration": 2.31
    },
    {
        "text": "a scoring file to run your model,",
        "start": 145.49,
        "duration": 1.59
    },
    {
        "text": "create the image, and deploy it out for you.",
        "start": 147.08,
        "duration": 2.025
    },
    {
        "text": "So all you're doing is providing your models",
        "start": 149.105,
        "duration": 1.545
    },
    {
        "text": "and information and you just let it go.",
        "start": 150.65,
        "duration": 1.565
    },
    {
        "text": ">> I see. So it's basically for",
        "start": 152.215,
        "duration": 1.435
    },
    {
        "text": "certain styles of model which are very specific.",
        "start": 153.65,
        "duration": 2.49
    },
    {
        "text": "Like, for example, if you're making a gun that's",
        "start": 156.14,
        "duration": 2.505
    },
    {
        "text": "kind of a weird scenario where you",
        "start": 158.645,
        "duration": 1.785
    },
    {
        "text": "might need to write your own driver file,",
        "start": 160.43,
        "duration": 1.89
    },
    {
        "text": "but for scenarios that are standard like scikit-learn,",
        "start": 162.32,
        "duration": 2.07
    },
    {
        "text": "you said TensorFlow and ONNX.",
        "start": 164.39,
        "duration": 1.545
    },
    {
        "text": ">> ONNX and auto ML models.",
        "start": 165.935,
        "duration": 2.985
    },
    {
        "text": ">> Auto ML models, it'll basically all you have to do is",
        "start": 168.92,
        "duration": 1.95
    },
    {
        "text": "have a model and you can literally straight deploy out.",
        "start": 170.87,
        "duration": 2.075
    },
    {
        "text": ">> Pretty much, yeah. We have",
        "start": 172.945,
        "duration": 1.6
    },
    {
        "text": "a very simplified same way that you deploy models now,",
        "start": 174.545,
        "duration": 3.12
    },
    {
        "text": "you just put in less stuff.",
        "start": 177.665,
        "duration": 1.68
    },
    {
        "text": ">> That's nice. So can we take a look?",
        "start": 179.345,
        "duration": 1.275
    },
    {
        "text": ">> Yeah, let's take a look here.",
        "start": 180.62,
        "duration": 1.215
    },
    {
        "text": "So who's back here.",
        "start": 181.835,
        "duration": 1.815
    },
    {
        "text": "So basically here is the new Azure Machine Learning UI.",
        "start": 183.65,
        "duration": 2.88
    },
    {
        "text": "If you haven't checked it out, please check it out.",
        "start": 186.53,
        "duration": 1.71
    },
    {
        "text": ">> ml.Azure.com.",
        "start": 188.24,
        "duration": 1.575
    },
    {
        "text": ">> Exactly. So basically what I've done here is I've",
        "start": 189.815,
        "duration": 2.205
    },
    {
        "text": "registered a model before and right here,",
        "start": 192.02,
        "duration": 3.49
    },
    {
        "text": "I'm defining the model.",
        "start": 195.51,
        "duration": 1.1
    },
    {
        "text": "I can say my framework, the framework version.",
        "start": 196.61,
        "duration": 2.79
    },
    {
        "text": "If you're doing an experiment on Azure machine learning,",
        "start": 199.4,
        "duration": 2.37
    },
    {
        "text": "we can go ahead and store and extract",
        "start": 201.77,
        "duration": 1.92
    },
    {
        "text": "that information and put it in your model attributes.",
        "start": 203.69,
        "duration": 2.43
    },
    {
        "text": ">> That's awesome. So there's traceability",
        "start": 206.12,
        "duration": 1.35
    },
    {
        "text": "from where your model came from.",
        "start": 207.47,
        "duration": 1.38
    },
    {
        "text": ">> Exactly.",
        "start": 208.85,
        "duration": 0.81
    },
    {
        "text": ">> That's cool.",
        "start": 209.66,
        "duration": 0.445
    },
    {
        "text": ">> Yeah. So if we go back here,",
        "start": 210.105,
        "duration": 2.73
    },
    {
        "text": "all right, register a new model.",
        "start": 212.835,
        "duration": 1.73
    },
    {
        "text": "This is pretty much what it looks like.",
        "start": 214.565,
        "duration": 1.41
    },
    {
        "text": ">> So this is new.",
        "start": 215.975,
        "duration": 1.08
    },
    {
        "text": ">> Yes.",
        "start": 217.055,
        "duration": 0.33
    },
    {
        "text": ">> Because before I know I wasn't able to upload a model,",
        "start": 217.385,
        "duration": 3.895
    },
    {
        "text": "I basically had to do it in code, right?",
        "start": 221.28,
        "duration": 1.74
    },
    {
        "text": ">> Yeah. So now you can go ahead and upload",
        "start": 223.02,
        "duration": 1.535
    },
    {
        "text": "in browser model and register it.",
        "start": 224.555,
        "duration": 1.995
    },
    {
        "text": "So now if we want to actually",
        "start": 226.55,
        "duration": 1.77
    },
    {
        "text": "look at the code for a simplified deployment,",
        "start": 228.32,
        "duration": 3.345
    },
    {
        "text": "all you're pretty much changing in your natural flow",
        "start": 231.665,
        "duration": 2.265
    },
    {
        "text": "of let me pull a workspace which is basically your project.",
        "start": 233.93,
        "duration": 3.195
    },
    {
        "text": "Let me go ahead and train my model.",
        "start": 237.125,
        "duration": 2.655
    },
    {
        "text": "Now, when I register,",
        "start": 239.78,
        "duration": 1.439
    },
    {
        "text": "I'll go ahead and do what I just showed you in the UI.",
        "start": 241.219,
        "duration": 3.136
    },
    {
        "text": "Then when you deploy it, you just have",
        "start": 244.355,
        "duration": 1.725
    },
    {
        "text": "this simple line of code right here.",
        "start": 246.08,
        "duration": 3.03
    },
    {
        "text": "So I'm deploying an AKS here.",
        "start": 249.11,
        "duration": 1.68
    },
    {
        "text": "Define my deployment target,",
        "start": 250.79,
        "duration": 1.545
    },
    {
        "text": "here is my model,",
        "start": 252.335,
        "duration": 1.38
    },
    {
        "text": "I name it, we're good to go.",
        "start": 253.715,
        "duration": 1.725
    },
    {
        "text": ">> That's nice. The thing that's interesting",
        "start": 255.44,
        "duration": 4.78
    },
    {
        "text": "to me is how much more simplified",
        "start": 260.22,
        "duration": 2.3
    },
    {
        "text": "it is especially with things that we already know about.",
        "start": 262.52,
        "duration": 2.445
    },
    {
        "text": "Can you talk about this in the larger context of MLOps?",
        "start": 264.965,
        "duration": 3.135
    },
    {
        "text": "Because I've heard MLOps and I want to",
        "start": 268.1,
        "duration": 1.71
    },
    {
        "text": "hear what that means and how this fits into it.",
        "start": 269.81,
        "duration": 2.43
    },
    {
        "text": ">> So basically, we have",
        "start": 272.24,
        "duration": 1.95
    },
    {
        "text": "multiple different personas going out and deploying models, right?",
        "start": 274.19,
        "duration": 2.7
    },
    {
        "text": ">> Right.",
        "start": 276.89,
        "duration": 0.27
    },
    {
        "text": ">> So you have to understand",
        "start": 277.16,
        "duration": 2.82
    },
    {
        "text": "certain contexts depending on",
        "start": 279.98,
        "duration": 1.26
    },
    {
        "text": "what deployment pipeline or which stage you are at.",
        "start": 281.24,
        "duration": 2.226
    },
    {
        "text": ">> Sure.",
        "start": 283.466,
        "duration": 0.174
    },
    {
        "text": ">> So as data scientists, if I don't work in Python,",
        "start": 283.64,
        "duration": 2.22
    },
    {
        "text": "I want to go and learn a bunch of Python,",
        "start": 285.86,
        "duration": 1.62
    },
    {
        "text": "write these driver files.",
        "start": 287.48,
        "duration": 1.19
    },
    {
        "text": "So what this pretty much helps",
        "start": 288.67,
        "duration": 1.84
    },
    {
        "text": "you do is you start getting those models",
        "start": 290.51,
        "duration": 2.01
    },
    {
        "text": "out quickly without having",
        "start": 292.52,
        "duration": 1.53
    },
    {
        "text": "those intermediate stages of let me create the image,",
        "start": 294.05,
        "duration": 2.43
    },
    {
        "text": "let me learn Docker, let me understand the environment or",
        "start": 296.48,
        "duration": 3.54
    },
    {
        "text": "what is the optimal of resource to deploy",
        "start": 300.02,
        "duration": 1.89
    },
    {
        "text": "my environment, our deployment model.",
        "start": 301.91,
        "duration": 2.31
    },
    {
        "text": "So what we're doing with this is creating a baseline",
        "start": 304.22,
        "duration": 2.73
    },
    {
        "text": "to get the optimal resources,",
        "start": 306.95,
        "duration": 2.47
    },
    {
        "text": "optimal environment to be able to run your mouse.",
        "start": 309.42,
        "duration": 3.27
    },
    {
        "text": "Then you pass on those information to your DevOps engineer,",
        "start": 312.69,
        "duration": 3.15
    },
    {
        "text": "DevOps pipeline and it already has here's the optimal CPU,",
        "start": 315.84,
        "duration": 3.26
    },
    {
        "text": "here's what the MLOp should look like,",
        "start": 319.1,
        "duration": 1.67
    },
    {
        "text": "let me go ahead and do a deployment pipeline",
        "start": 320.77,
        "duration": 2.23
    },
    {
        "text": "validating all of these,",
        "start": 323.0,
        "duration": 1.14
    },
    {
        "text": "and I have that information from the model registry.",
        "start": 324.14,
        "duration": 2.685
    },
    {
        "text": ">> That's pretty cool. So MLOps is like DevOps but for ML, right?",
        "start": 326.825,
        "duration": 6.715
    },
    {
        "text": ">> Amazing.",
        "start": 333.54,
        "duration": 0.69
    },
    {
        "text": ">> This is pretty cool. Thank you so much.",
        "start": 334.23,
        "duration": 1.55
    },
    {
        "text": "Anything else you want to add to this?",
        "start": 335.78,
        "duration": 1.44
    },
    {
        "text": ">> No, I am just looking",
        "start": 337.22,
        "duration": 1.65
    },
    {
        "text": "forward to getting runs and put them feedback on this.",
        "start": 338.87,
        "duration": 2.22
    },
    {
        "text": "It's a really exciting new feature,",
        "start": 341.09,
        "duration": 1.68
    },
    {
        "text": "and I think it'll really help",
        "start": 342.77,
        "duration": 1.11
    },
    {
        "text": "a lot of the data scientists or engineers",
        "start": 343.88,
        "duration": 1.56
    },
    {
        "text": "out there that are trying to just get their models deployed.",
        "start": 345.44,
        "duration": 2.73
    },
    {
        "text": ">> This is awesome. So today we've learned about",
        "start": 348.17,
        "duration": 2.895
    },
    {
        "text": "no code deployment of models that you can literally upload today.",
        "start": 351.065,
        "duration": 3.855
    },
    {
        "text": "Thank you so much for watching",
        "start": 354.92,
        "duration": 1.305
    },
    {
        "text": "and we'll see you next time. Take care.",
        "start": 356.225,
        "duration": 2.635
    }
]