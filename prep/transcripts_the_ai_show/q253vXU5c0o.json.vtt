[
    {
        "text": ">> You're not going to want to miss. The the next",
        "start": 0.0,
        "duration": 1.5
    },
    {
        "text": "episode of the AI Show,",
        "start": 1.5,
        "duration": 1.14
    },
    {
        "text": "we talk all about",
        "start": 2.64,
        "duration": 1.305
    },
    {
        "text": "Responsible AI Support for Image and Text Models.",
        "start": 3.945,
        "duration": 2.565
    },
    {
        "text": "Part 2 of a two-part series",
        "start": 6.51,
        "duration": 2.82
    },
    {
        "text": "of the responsibility AI dashboard. Make sure you tune it.",
        "start": 9.33,
        "duration": 1.98
    },
    {
        "text": "[MUSIC]",
        "start": 11.31,
        "duration": 4.77
    },
    {
        "text": ">> Hello, and welcome to this episode of the AI Show.",
        "start": 17.27,
        "duration": 2.86
    },
    {
        "text": "We're talking all about Responsible AI Support",
        "start": 20.13,
        "duration": 2.1
    },
    {
        "text": "for Image and Text Models.",
        "start": 22.23,
        "duration": 1.74
    },
    {
        "text": "I have my friend Jacqueline here.",
        "start": 23.97,
        "duration": 1.74
    },
    {
        "text": "Why don't you tell us who you are and what you do my friend?",
        "start": 25.71,
        "duration": 2.1
    },
    {
        "text": ">> For sure. Thanks, Seth.",
        "start": 27.81,
        "duration": 1.62
    },
    {
        "text": "I'm really excited to be here.",
        "start": 29.43,
        "duration": 2.135
    },
    {
        "text": "I've been working on the responsible AI tooling",
        "start": 31.565,
        "duration": 2.565
    },
    {
        "text": "for detection models.",
        "start": 34.13,
        "duration": 1.55
    },
    {
        "text": "I'm so excited to be able to share that with you-all today.",
        "start": 35.68,
        "duration": 2.535
    },
    {
        "text": ">> Fantastic. People can watch Part 1,",
        "start": 38.215,
        "duration": 4.76
    },
    {
        "text": "but if you can give us a sense of what the RAI dashboard is,",
        "start": 42.975,
        "duration": 2.975
    },
    {
        "text": "and then maybe what it looks like",
        "start": 45.95,
        "duration": 1.47
    },
    {
        "text": "pointed to the image problem. Can you do that?",
        "start": 47.42,
        "duration": 2.45
    },
    {
        "text": ">> For sure. Essentially,",
        "start": 49.87,
        "duration": 2.24
    },
    {
        "text": "what we want to do here is to really accelerate",
        "start": 52.11,
        "duration": 2.375
    },
    {
        "text": "the responsible development of",
        "start": 54.485,
        "duration": 2.28
    },
    {
        "text": "both text and image models focusing on image today.",
        "start": 56.765,
        "duration": 3.465
    },
    {
        "text": "We'll be diving into how this dashboard will be able",
        "start": 60.23,
        "duration": 3.21
    },
    {
        "text": "to help ML professionals to holistically",
        "start": 63.44,
        "duration": 3.165
    },
    {
        "text": "assess and debug their models for any kind of",
        "start": 66.605,
        "duration": 2.505
    },
    {
        "text": "fairness issues using Azure Machine Learning's",
        "start": 69.11,
        "duration": 3.36
    },
    {
        "text": ", responsible AI dashboard.",
        "start": 72.47,
        "duration": 1.865
    },
    {
        "text": ">> That's cool. I've always found like,",
        "start": 74.335,
        "duration": 1.83
    },
    {
        "text": "how would you do that with images though?",
        "start": 76.165,
        "duration": 2.075
    },
    {
        "text": "I'm interested in what that looks like.",
        "start": 78.24,
        "duration": 1.82
    },
    {
        "text": "Can you show us a little bit about how one might go about this?",
        "start": 80.06,
        "duration": 3.56
    },
    {
        "text": ">> For sure. What you see here",
        "start": 83.62,
        "duration": 2.675
    },
    {
        "text": "is the Responsible AI dashboard for image,",
        "start": 86.295,
        "duration": 2.735
    },
    {
        "text": "specifically for the object detection scenario.",
        "start": 89.03,
        "duration": 3.015
    },
    {
        "text": "I'll be showing just a quick walk through of how you can",
        "start": 92.045,
        "duration": 3.195
    },
    {
        "text": "be identifying fairness issues in your models and data,",
        "start": 95.24,
        "duration": 3.195
    },
    {
        "text": "diagnose why those issues are happening,",
        "start": 98.435,
        "duration": 2.31
    },
    {
        "text": "and inform more targeted medications.",
        "start": 100.745,
        "duration": 2.445
    },
    {
        "text": "Here today we'll be focusing on object detection models",
        "start": 103.19,
        "duration": 2.85
    },
    {
        "text": "predictions on images from the MS cohort dataset,",
        "start": 106.04,
        "duration": 3.06
    },
    {
        "text": "and how we can use the dashboard to debug those predictions.",
        "start": 109.1,
        "duration": 3.945
    },
    {
        "text": "The Responsible AI dashboard for",
        "start": 113.045,
        "duration": 2.55
    },
    {
        "text": "object detection has three main features.",
        "start": 115.595,
        "duration": 2.265
    },
    {
        "text": "It has the model overview,",
        "start": 117.86,
        "duration": 1.35
    },
    {
        "text": "data explorer, and model interpretability.",
        "start": 119.21,
        "duration": 2.475
    },
    {
        "text": "To start your analysis,",
        "start": 121.685,
        "duration": 1.425
    },
    {
        "text": "the model overview here provides a comprehensive set of",
        "start": 123.11,
        "duration": 4.29
    },
    {
        "text": "performance metrics so that you can evaluate",
        "start": 127.4,
        "duration": 2.235
    },
    {
        "text": "your object detection model across dataset or a feature cohorts.",
        "start": 129.635,
        "duration": 4.17
    },
    {
        "text": "In the dataset cohorts pane,",
        "start": 133.805,
        "duration": 2.01
    },
    {
        "text": "you can view these performance metrics across all of your data,",
        "start": 135.815,
        "duration": 4.255
    },
    {
        "text": "and how that compares to any dataset cohort you create.",
        "start": 140.07,
        "duration": 3.2
    },
    {
        "text": "For example, here I have a group of",
        "start": 143.27,
        "duration": 2.34
    },
    {
        "text": "images of people with either skis or snowboards.",
        "start": 145.61,
        "duration": 2.85
    },
    {
        "text": "Since I've selected the person class from this drop down here,",
        "start": 148.46,
        "duration": 5.13
    },
    {
        "text": "the table now shows the average position and",
        "start": 153.59,
        "duration": 2.49
    },
    {
        "text": "the average recall scores for detecting the person class,",
        "start": 156.08,
        "duration": 3.585
    },
    {
        "text": "and to evaluate your object detection models.",
        "start": 159.665,
        "duration": 3.095
    },
    {
        "text": "Predict confidence.",
        "start": 162.76,
        "duration": 2.225
    },
    {
        "text": "You can also set the IOU threshold value.",
        "start": 164.985,
        "duration": 2.945
    },
    {
        "text": "IOU here refers to intersection of union between",
        "start": 167.93,
        "duration": 2.7
    },
    {
        "text": "be ground-truth and prediction bounding boxes.",
        "start": 170.63,
        "duration": 3.0
    },
    {
        "text": "This really defines error and affects",
        "start": 173.63,
        "duration": 1.95
    },
    {
        "text": "the calculation of these model performance metrics.",
        "start": 175.58,
        "duration": 3.12
    },
    {
        "text": "This can help you better evaluate your object detection model.",
        "start": 178.7,
        "duration": 3.905
    },
    {
        "text": "At the bottom, we do have metrics visualizations that provides",
        "start": 182.605,
        "duration": 3.79
    },
    {
        "text": "a visual comparison of",
        "start": 186.395,
        "duration": 1.785
    },
    {
        "text": "performance metrics across the dataset cohorts.",
        "start": 188.18,
        "duration": 2.895
    },
    {
        "text": "These insights are useful because a common issue in",
        "start": 191.075,
        "duration": 3.045
    },
    {
        "text": "responsible AI is that a model may have high accuracy overall,",
        "start": 194.12,
        "duration": 3.81
    },
    {
        "text": "but perform poorly for certain subgroups of data.",
        "start": 197.93,
        "duration": 2.925
    },
    {
        "text": "That can result in unintended harms,",
        "start": 200.855,
        "duration": 2.955
    },
    {
        "text": "for example, a face detection model can",
        "start": 203.81,
        "duration": 2.19
    },
    {
        "text": "perform worse for certain ethnic groups than others.",
        "start": 206.0,
        "duration": 2.775
    },
    {
        "text": "These failures are hidden if we only look",
        "start": 208.775,
        "duration": 2.265
    },
    {
        "text": "at aggregate performance metrics and",
        "start": 211.04,
        "duration": 2.31
    },
    {
        "text": "not segmented into groups of data to evaluate the model.",
        "start": 213.35,
        "duration": 4.6
    },
    {
        "text": "Another lens to evaluate performance",
        "start": 217.95,
        "duration": 2.3
    },
    {
        "text": "is based on feature-based cohorts.",
        "start": 220.25,
        "duration": 2.595
    },
    {
        "text": "For fairness [inaudible] evaluation,",
        "start": 222.845,
        "duration": 2.025
    },
    {
        "text": "this is useful if you want to examine",
        "start": 224.87,
        "duration": 1.65
    },
    {
        "text": "your data based on sensitive features.",
        "start": 226.52,
        "duration": 2.265
    },
    {
        "text": "And the future cohorts pane,",
        "start": 228.785,
        "duration": 1.89
    },
    {
        "text": "you can automatically create a default of",
        "start": 230.675,
        "duration": 2.625
    },
    {
        "text": "three cohorts split according",
        "start": 233.3,
        "duration": 2.49
    },
    {
        "text": "to values of a feature that you specify,",
        "start": 235.79,
        "duration": 2.715
    },
    {
        "text": "or you can also adjust which",
        "start": 238.505,
        "duration": 2.475
    },
    {
        "text": "features to use to evaluate your model's fairness,",
        "start": 240.98,
        "duration": 2.67
    },
    {
        "text": "and determine the number of features splits that can be used here.",
        "start": 243.65,
        "duration": 3.95
    },
    {
        "text": "With more granularity or feature splits,",
        "start": 247.6,
        "duration": 2.335
    },
    {
        "text": "you may be able to more precisely pinpoint those error cohorts.",
        "start": 249.935,
        "duration": 5.045
    },
    {
        "text": "Now, moving on,",
        "start": 254.98,
        "duration": 2.15
    },
    {
        "text": "we can use the data explorer component to further",
        "start": 257.13,
        "duration": 4.37
    },
    {
        "text": "debug and diagnose any discrepancies",
        "start": 261.5,
        "duration": 2.22
    },
    {
        "text": "that you notice in the model over the section.",
        "start": 263.72,
        "duration": 2.79
    },
    {
        "text": "This data explorer has various views,",
        "start": 266.51,
        "duration": 2.52
    },
    {
        "text": "provide different perspectives of your data.",
        "start": 269.03,
        "duration": 2.085
    },
    {
        "text": "First, the image explorer here allows us to",
        "start": 271.115,
        "duration": 3.09
    },
    {
        "text": "easily view all of these image instances and predict it,",
        "start": 274.205,
        "duration": 4.665
    },
    {
        "text": "and ground-truth bounding boxes segmented by",
        "start": 278.87,
        "duration": 3.66
    },
    {
        "text": "both error and success instances",
        "start": 282.53,
        "duration": 2.04
    },
    {
        "text": "for you to observe any error patterns.",
        "start": 284.57,
        "duration": 2.52
    },
    {
        "text": "Here I can filter",
        "start": 287.09,
        "duration": 2.925
    },
    {
        "text": "these images shown by an index and classification outcome.",
        "start": 290.015,
        "duration": 5.175
    },
    {
        "text": "For example, maybe based on",
        "start": 295.19,
        "duration": 1.8
    },
    {
        "text": "previous observations of model performance across feature cohorts,",
        "start": 296.99,
        "duration": 3.915
    },
    {
        "text": "I may want to specifically",
        "start": 300.905,
        "duration": 2.025
    },
    {
        "text": "investigate certain images that",
        "start": 302.93,
        "duration": 3.3
    },
    {
        "text": "are based on certain feature values.",
        "start": 306.23,
        "duration": 2.01
    },
    {
        "text": "I can also stack",
        "start": 308.24,
        "duration": 1.29
    },
    {
        "text": "multiple filters and kind of create a new cohort from there.",
        "start": 309.53,
        "duration": 5.92
    },
    {
        "text": "Then next we have the table view.",
        "start": 316.46,
        "duration": 3.13
    },
    {
        "text": "This pane shows different metadata features",
        "start": 319.59,
        "duration": 2.945
    },
    {
        "text": "along with the dataset index,",
        "start": 322.535,
        "duration": 2.505
    },
    {
        "text": "ground-truth predicted class labels",
        "start": 325.04,
        "duration": 2.25
    },
    {
        "text": "for each of the image instances.",
        "start": 327.29,
        "duration": 2.19
    },
    {
        "text": "This is also where you can create",
        "start": 329.48,
        "duration": 1.98
    },
    {
        "text": "a new dataset cohort to analyze in a more granular manner.",
        "start": 331.46,
        "duration": 3.835
    },
    {
        "text": "See I'm selecting these images here",
        "start": 335.295,
        "duration": 3.185
    },
    {
        "text": "and I could save it into a new cohort for more evaluation.",
        "start": 338.48,
        "duration": 3.51
    },
    {
        "text": "Then lastly, we also have the class view pane which",
        "start": 341.99,
        "duration": 3.99
    },
    {
        "text": "breaks down your model's predictions by class labels.",
        "start": 345.98,
        "duration": 4.74
    },
    {
        "text": "I just also want to highlight here our last cornerstone feature,",
        "start": 350.72,
        "duration": 4.7
    },
    {
        "text": "which is being able to access",
        "start": 355.42,
        "duration": 2.64
    },
    {
        "text": "visual explanations for modeled behavior",
        "start": 358.06,
        "duration": 2.76
    },
    {
        "text": "leading to object detection.",
        "start": 360.82,
        "duration": 1.5
    },
    {
        "text": "This is really helpful to debug what",
        "start": 362.32,
        "duration": 2.82
    },
    {
        "text": "contributes to miss detections and any biases that may occur.",
        "start": 365.14,
        "duration": 4.47
    },
    {
        "text": "This is really unique to the object detection scenario.",
        "start": 369.61,
        "duration": 2.98
    },
    {
        "text": "Going into this feature here,",
        "start": 372.59,
        "duration": 1.94
    },
    {
        "text": "if I click on this instance,",
        "start": 374.53,
        "duration": 2.43
    },
    {
        "text": "I can see that",
        "start": 376.96,
        "duration": 1.71
    },
    {
        "text": "the model's prediction is aligned with ground truths.",
        "start": 378.67,
        "duration": 2.85
    },
    {
        "text": "It's successful instance is the person,",
        "start": 381.52,
        "duration": 1.92
    },
    {
        "text": "and the ski was correctly detected.",
        "start": 383.44,
        "duration": 2.325
    },
    {
        "text": "This visual here is a saliency map that highlights",
        "start": 385.765,
        "duration": 3.525
    },
    {
        "text": "parts of an image that contribute most to the model's detection.",
        "start": 389.29,
        "duration": 3.635
    },
    {
        "text": "This is generated by visual explanation methods,",
        "start": 392.925,
        "duration": 3.415
    },
    {
        "text": "which is MSR's open source package that implements D-RISE.",
        "start": 396.34,
        "duration": 4.135
    },
    {
        "text": "D-RISE is a model agnostic method to visually explain",
        "start": 400.475,
        "duration": 3.075
    },
    {
        "text": "your object detection models that you can learn",
        "start": 403.55,
        "duration": 2.43
    },
    {
        "text": "more later in our blog and technical documents.",
        "start": 405.98,
        "duration": 2.925
    },
    {
        "text": "In this visual, I can see that really highly ceiling pixels,",
        "start": 408.905,
        "duration": 5.055
    },
    {
        "text": "as indicated by these really intense warm colors,",
        "start": 413.96,
        "duration": 3.0
    },
    {
        "text": "are focused on the correctly detected person.",
        "start": 416.96,
        "duration": 3.045
    },
    {
        "text": "As you can see, there's no saliency pixels",
        "start": 420.005,
        "duration": 3.195
    },
    {
        "text": "towards most of the skis,",
        "start": 423.2,
        "duration": 1.35
    },
    {
        "text": "and this shows that the objects around the person don't contribute",
        "start": 424.55,
        "duration": 3.33
    },
    {
        "text": "to the prediction as significantly as the person itself,",
        "start": 427.88,
        "duration": 3.27
    },
    {
        "text": "which is what we desire from our model.",
        "start": 431.15,
        "duration": 3.125
    },
    {
        "text": "This is confirmed behavior,",
        "start": 434.275,
        "duration": 2.105
    },
    {
        "text": "but when we look at the skis and how the model detects the skis,",
        "start": 436.38,
        "duration": 4.64
    },
    {
        "text": "notice that there are saliency pixels on the skis here,",
        "start": 441.02,
        "duration": 3.33
    },
    {
        "text": "and the person's legs.",
        "start": 444.35,
        "duration": 2.175
    },
    {
        "text": "This may imply that the co-occurrence of the person and",
        "start": 446.525,
        "duration": 3.345
    },
    {
        "text": "the skis is leading to bias in the prediction of the skis.",
        "start": 449.87,
        "duration": 3.965
    },
    {
        "text": "Imagine if there was a pair of skis on a wall,",
        "start": 453.835,
        "duration": 3.955
    },
    {
        "text": "would the skis be successfully detected without a person?",
        "start": 457.79,
        "duration": 3.72
    },
    {
        "text": "This would be maybe an area of",
        "start": 461.51,
        "duration": 2.19
    },
    {
        "text": "investigation I would want to go in where maybe I'd want",
        "start": 463.7,
        "duration": 2.52
    },
    {
        "text": "to consider adding more images of jet skis without the person to",
        "start": 466.22,
        "duration": 4.17
    },
    {
        "text": "avoid the model making",
        "start": 470.39,
        "duration": 1.38
    },
    {
        "text": "spurious correlations between the skis and the person object,",
        "start": 471.77,
        "duration": 3.63
    },
    {
        "text": "even if this leads to a correct detection.",
        "start": 475.4,
        "duration": 3.2
    },
    {
        "text": "You can repeat this process even with most detections.",
        "start": 478.6,
        "duration": 3.535
    },
    {
        "text": "That's basically what we have",
        "start": 482.135,
        "duration": 1.695
    },
    {
        "text": "with the other object detection dashboard.",
        "start": 483.83,
        "duration": 1.61
    },
    {
        "text": ">> Wow, this is really cool.",
        "start": 485.44,
        "duration": 1.805
    },
    {
        "text": "I'm going to ask a bunch of questions, if that's okay.",
        "start": 487.245,
        "duration": 2.84
    },
    {
        "text": "Does this work with",
        "start": 490.085,
        "duration": 3.105
    },
    {
        "text": "any computer vision models or",
        "start": 493.19,
        "duration": 2.22
    },
    {
        "text": "do you have to train the models in a specific way?",
        "start": 495.41,
        "duration": 2.54
    },
    {
        "text": "Does it come from automated machine learning models,",
        "start": 497.95,
        "duration": 2.77
    },
    {
        "text": "or how do you get those models?",
        "start": 500.72,
        "duration": 2.085
    },
    {
        "text": "In the last video,",
        "start": 502.805,
        "duration": 1.455
    },
    {
        "text": "we saw a process where you had to",
        "start": 504.26,
        "duration": 1.92
    },
    {
        "text": "actually explicitly tell it to make this dashboard.",
        "start": 506.18,
        "duration": 2.685
    },
    {
        "text": "Is that the same process? The first question",
        "start": 508.865,
        "duration": 1.785
    },
    {
        "text": "is, what kind of models?",
        "start": 510.65,
        "duration": 1.32
    },
    {
        "text": "The second is, how do we make this dashboard?",
        "start": 511.97,
        "duration": 2.235
    },
    {
        "text": ">> For sure. For models,",
        "start": 514.205,
        "duration": 2.04
    },
    {
        "text": "this dashboard covers both AutoML and non AutoML models.",
        "start": 516.245,
        "duration": 6.06
    },
    {
        "text": "For non AutoML, we cover PyTorch models,",
        "start": 522.305,
        "duration": 3.705
    },
    {
        "text": "so that includes [inaudible] as well as others.",
        "start": 526.01,
        "duration": 3.695
    },
    {
        "text": "In terms of setting it up,",
        "start": 529.705,
        "duration": 1.975
    },
    {
        "text": "I think that was your second question.",
        "start": 531.68,
        "duration": 1.925
    },
    {
        "text": "This goes to the same process as setting up this text dashboard.",
        "start": 533.605,
        "duration": 5.5
    },
    {
        "text": "You can look into maybe our documentation for",
        "start": 539.105,
        "duration": 2.055
    },
    {
        "text": "a certain other details that differentiate the two.",
        "start": 541.16,
        "duration": 3.605
    },
    {
        "text": ">> It's basically a call to a component run kind of thing.",
        "start": 544.765,
        "duration": 3.235
    },
    {
        "text": ">> Yes.",
        "start": 548.0,
        "duration": 1.27
    },
    {
        "text": ">> I'm already on number 3.",
        "start": 550.84,
        "duration": 2.245
    },
    {
        "text": "There was a lot of this notion of mean pixel value.",
        "start": 553.085,
        "duration": 4.455
    },
    {
        "text": "Remember, you were able to separate",
        "start": 557.54,
        "duration": 1.74
    },
    {
        "text": "the cohorts in the mean pixel value greater than.",
        "start": 559.28,
        "duration": 2.58
    },
    {
        "text": "Is the mean pixel value,",
        "start": 561.86,
        "duration": 1.62
    },
    {
        "text": "and if I sound really dumb,",
        "start": 563.48,
        "duration": 1.905
    },
    {
        "text": "that that's it right there,",
        "start": 565.385,
        "duration": 1.455
    },
    {
        "text": "let me know if it's the wrong thing.",
        "start": 566.84,
        "duration": 1.65
    },
    {
        "text": "Does that mean the average value of the pixel?",
        "start": 568.49,
        "duration": 5.44
    },
    {
        "text": ">> I believe so.",
        "start": 574.9,
        "duration": 2.83
    },
    {
        "text": ">> Okay.",
        "start": 577.73,
        "duration": 1.005
    },
    {
        "text": ">> Yeah.",
        "start": 578.735,
        "duration": 0.96
    },
    {
        "text": ">> I think that's cool. If that's the case,",
        "start": 579.695,
        "duration": 2.475
    },
    {
        "text": "that's actually really cool because",
        "start": 582.17,
        "duration": 2.235
    },
    {
        "text": "it lets you look at images based on",
        "start": 584.405,
        "duration": 2.865
    },
    {
        "text": "saturation to see if there's bias",
        "start": 587.27,
        "duration": 2.55
    },
    {
        "text": "with darker images versus lighter images.",
        "start": 589.82,
        "duration": 3.525
    },
    {
        "text": "I'm not trying to say anything,",
        "start": 593.345,
        "duration": 1.935
    },
    {
        "text": "but that is actually really cool.",
        "start": 595.28,
        "duration": 2.67
    },
    {
        "text": "The last thing I want to ask you,",
        "start": 597.95,
        "duration": 1.95
    },
    {
        "text": "and this is probably the fun bits,",
        "start": 599.9,
        "duration": 2.46
    },
    {
        "text": "is where can people go to find out more about this stuff?",
        "start": 602.36,
        "duration": 3.36
    },
    {
        "text": "I know you gave me some links.",
        "start": 605.72,
        "duration": 1.47
    },
    {
        "text": "Here's the first one.",
        "start": 607.19,
        "duration": 1.62
    },
    {
        "text": "What will people find here in the RAI vision insights?",
        "start": 608.81,
        "duration": 3.8
    },
    {
        "text": ">> For sure. This is a link to our technical documentation.",
        "start": 612.61,
        "duration": 3.67
    },
    {
        "text": "If you're looking into detailed formation of",
        "start": 616.28,
        "duration": 2.61
    },
    {
        "text": "how to set up this Responsible AI dashboard,",
        "start": 618.89,
        "duration": 2.55
    },
    {
        "text": "and what all these components mean,",
        "start": 621.44,
        "duration": 2.055
    },
    {
        "text": "any limitations or guidance on like your dataset and your models,",
        "start": 623.495,
        "duration": 4.165
    },
    {
        "text": "this is where you'll find all that information.",
        "start": 627.66,
        "duration": 2.23
    },
    {
        "text": ">> Fantastic.",
        "start": 629.89,
        "duration": 1.16
    },
    {
        "text": "It looks like this helps you",
        "start": 631.05,
        "duration": 2.03
    },
    {
        "text": "understand the dashboard as well, this one.",
        "start": 633.08,
        "duration": 2.91
    },
    {
        "text": ">> Yeah. This goes into the dashboard components",
        "start": 635.99,
        "duration": 2.565
    },
    {
        "text": "and what each of them mean,",
        "start": 638.555,
        "duration": 1.095
    },
    {
        "text": "while the first one was more on set up.",
        "start": 639.65,
        "duration": 1.97
    },
    {
        "text": ">> Amazing. Like I said,",
        "start": 641.62,
        "duration": 2.18
    },
    {
        "text": "the RAI dashboards,",
        "start": 643.8,
        "duration": 2.085
    },
    {
        "text": "I still need to dive in and understand because it's got",
        "start": 645.885,
        "duration": 2.58
    },
    {
        "text": "so much information there that it's even overwhelming for me,",
        "start": 648.465,
        "duration": 3.335
    },
    {
        "text": "so make sure you take a look at this.",
        "start": 651.8,
        "duration": 1.695
    },
    {
        "text": "Then finally, before we go,",
        "start": 653.495,
        "duration": 1.815
    },
    {
        "text": "there is a cool blog that we'll talk about",
        "start": 655.31,
        "duration": 1.65
    },
    {
        "text": "both the text and the image, RAI stuff.",
        "start": 656.96,
        "duration": 3.16
    },
    {
        "text": "Is that right? Is that what this blog is?",
        "start": 660.12,
        "duration": 1.725
    },
    {
        "text": ">> This blog primarily talks about the image,",
        "start": 661.845,
        "duration": 2.585
    },
    {
        "text": "particularly the object detection",
        "start": 664.43,
        "duration": 1.35
    },
    {
        "text": "scenario that I just walked into.",
        "start": 665.78,
        "duration": 2.07
    },
    {
        "text": ">> Fantastic. Awesome. Well, thank you",
        "start": 667.85,
        "duration": 1.89
    },
    {
        "text": "so much for spending some time with us my friend.",
        "start": 669.74,
        "duration": 1.7
    },
    {
        "text": ">> For sure, I'm happy to be here",
        "start": 671.44,
        "duration": 1.97
    },
    {
        "text": "and to share. Thank you for having me.",
        "start": 673.41,
        "duration": 2.07
    },
    {
        "text": ">> Amazing. Well, you've been learning all about",
        "start": 675.48,
        "duration": 1.98
    },
    {
        "text": "the Responsible AI Support for Images and Text Models Part 2,",
        "start": 677.46,
        "duration": 4.335
    },
    {
        "text": "where we talked about images.",
        "start": 681.795,
        "duration": 1.125
    },
    {
        "text": "Thank you so much for watching and",
        "start": 682.92,
        "duration": 1.01
    },
    {
        "text": "hopefully we'll see you next time. Take care.",
        "start": 683.93,
        "duration": 1.168
    },
    {
        "text": "[MUSIC]",
        "start": 685.098,
        "duration": 1.122
    }
]