[
    {
        "text": "hi everyone welcome to the thanks for",
        "start": 1.03,
        "duration": 6.34
    },
    {
        "text": "coming out welcome to the women and",
        "start": 5.81,
        "duration": 3.45
    },
    {
        "text": "machine learning in data science with",
        "start": 7.37,
        "duration": 3.87
    },
    {
        "text": "Microsoft reactor Meetup",
        "start": 9.26,
        "duration": 3.75
    },
    {
        "text": "so happy to have all you guys here",
        "start": 11.24,
        "duration": 4.1
    },
    {
        "text": "tonight and so happy to see so many",
        "start": 13.01,
        "duration": 4.529
    },
    {
        "text": "first-timers at the meetup tonight",
        "start": 15.34,
        "duration": 4.54
    },
    {
        "text": "that's really exciting I'm Alexandra I'm",
        "start": 17.539,
        "duration": 4.05
    },
    {
        "text": "one of the co-organizers of the meetup",
        "start": 19.88,
        "duration": 4.08
    },
    {
        "text": "with Aaron unfortunately she's sick",
        "start": 21.589,
        "duration": 5.401
    },
    {
        "text": "tonight and was able to make it but we",
        "start": 23.96,
        "duration": 5.43
    },
    {
        "text": "are really in store for some great talks",
        "start": 26.99,
        "duration": 5.82
    },
    {
        "text": "we've got Rita and Lauren Lauren is a",
        "start": 29.39,
        "duration": 5.51
    },
    {
        "text": "machine learning engineer on the",
        "start": 32.81,
        "duration": 8.63
    },
    {
        "text": "Microsoft commercial services team and",
        "start": 34.9,
        "duration": 6.54
    },
    {
        "text": "commercial the commercial software team",
        "start": 41.89,
        "duration": 5.65
    },
    {
        "text": "sorry I like swear I rehearsed that",
        "start": 45.47,
        "duration": 4.41
    },
    {
        "text": "beforehand and then Rita is an open",
        "start": 47.54,
        "duration": 3.6
    },
    {
        "text": "source engineer and they're gonna talk",
        "start": 49.88,
        "duration": 4.26
    },
    {
        "text": "about sequence classification with time",
        "start": 51.14,
        "duration": 4.65
    },
    {
        "text": "series data Finance time series data",
        "start": 54.14,
        "duration": 4.56
    },
    {
        "text": "which is something that a lot of data",
        "start": 55.79,
        "duration": 4.35
    },
    {
        "text": "scientists out there are working with",
        "start": 58.7,
        "duration": 4.77
    },
    {
        "text": "and also how to do tensor flow models at",
        "start": 60.14,
        "duration": 5.909
    },
    {
        "text": "scale with kubernetes on Azure for those",
        "start": 63.47,
        "duration": 3.63
    },
    {
        "text": "of you who are getting into the",
        "start": 66.049,
        "duration": 3.511
    },
    {
        "text": "container space so great talks tonight",
        "start": 67.1,
        "duration": 4.29
    },
    {
        "text": "we're gonna start with with Lauren and",
        "start": 69.56,
        "duration": 3.089
    },
    {
        "text": "then move on to Rita but thank you so",
        "start": 71.39,
        "duration": 6.57
    },
    {
        "text": "much for coming Thank You Alexandra so",
        "start": 72.649,
        "duration": 7.53
    },
    {
        "text": "my name is Lauren Tran I am a machine",
        "start": 77.96,
        "duration": 3.93
    },
    {
        "text": "learning engineer on Microsoft",
        "start": 80.179,
        "duration": 3.72
    },
    {
        "text": "commercial software engineering team and",
        "start": 81.89,
        "duration": 4.019
    },
    {
        "text": "I've been doing machine learning",
        "start": 83.899,
        "duration": 4.381
    },
    {
        "text": "projects at Microsoft for the last three",
        "start": 85.909,
        "duration": 5.28
    },
    {
        "text": "and a half years or so usually focus in",
        "start": 88.28,
        "duration": 4.859
    },
    {
        "text": "the areas of image recognition and image",
        "start": 91.189,
        "duration": 4.35
    },
    {
        "text": "classification but I do work in many",
        "start": 93.139,
        "duration": 3.631
    },
    {
        "text": "different domains and machine learning",
        "start": 95.539,
        "duration": 3.631
    },
    {
        "text": "so the project that I wanted to share",
        "start": 96.77,
        "duration": 4.049
    },
    {
        "text": "with you today it happens to be on",
        "start": 99.17,
        "duration": 4.5
    },
    {
        "text": "financial time series data and it's a",
        "start": 100.819,
        "duration": 4.89
    },
    {
        "text": "project that I just wrapped up with a",
        "start": 103.67,
        "duration": 4.019
    },
    {
        "text": "financial institution so we'll dive into",
        "start": 105.709,
        "duration": 4.5
    },
    {
        "text": "that a little bit more so what exactly",
        "start": 107.689,
        "duration": 5.161
    },
    {
        "text": "is commercial software engineering so we",
        "start": 110.209,
        "duration": 4.11
    },
    {
        "text": "are an organization of software",
        "start": 112.85,
        "duration": 3.719
    },
    {
        "text": "engineers at Microsoft that work on",
        "start": 114.319,
        "duration": 4.71
    },
    {
        "text": "various different projects with the",
        "start": 116.569,
        "duration": 4.5
    },
    {
        "text": "community so we work with students",
        "start": 119.029,
        "duration": 3.601
    },
    {
        "text": "startups professional developers",
        "start": 121.069,
        "duration": 4.68
    },
    {
        "text": "partners customers to help code projects",
        "start": 122.63,
        "duration": 4.29
    },
    {
        "text": "with them so when you have an",
        "start": 125.749,
        "duration": 2.79
    },
    {
        "text": "interesting technical challenge or a",
        "start": 126.92,
        "duration": 3.33
    },
    {
        "text": "project that you want to solve",
        "start": 128.539,
        "duration": 3.81
    },
    {
        "text": "we like to partner with people in the",
        "start": 130.25,
        "duration": 4.05
    },
    {
        "text": "community to help build those problems",
        "start": 132.349,
        "duration": 3.751
    },
    {
        "text": "and then we come back and share the",
        "start": 134.3,
        "duration": 3.39
    },
    {
        "text": "learnings with communities like yourself",
        "start": 136.1,
        "duration": 4.53
    },
    {
        "text": "so a little bit of coding some",
        "start": 137.69,
        "duration": 7.38
    },
    {
        "text": "evangelism and many hats in between all",
        "start": 140.63,
        "duration": 6.57
    },
    {
        "text": "right so the data the the problem that",
        "start": 145.07,
        "duration": 3.93
    },
    {
        "text": "we're solving that I wanted to share",
        "start": 147.2,
        "duration": 3.45
    },
    {
        "text": "with you today is on financial default",
        "start": 149.0,
        "duration": 4.89
    },
    {
        "text": "prediction so I worked recently with an",
        "start": 150.65,
        "duration": 5.61
    },
    {
        "text": "organization of credit unions where the",
        "start": 153.89,
        "duration": 5.88
    },
    {
        "text": "scenario they had was to be able to see",
        "start": 156.26,
        "duration": 5.49
    },
    {
        "text": "if they could predict which of their",
        "start": 159.77,
        "duration": 4.08
    },
    {
        "text": "members were likely to default or not",
        "start": 161.75,
        "duration": 4.77
    },
    {
        "text": "based on the transaction histories of",
        "start": 163.85,
        "duration": 5.1
    },
    {
        "text": "these members and the impetus for this",
        "start": 166.52,
        "duration": 5.07
    },
    {
        "text": "project is they have lots of situations",
        "start": 168.95,
        "duration": 5.22
    },
    {
        "text": "where when they can detect financial",
        "start": 171.59,
        "duration": 4.2
    },
    {
        "text": "distress when they do know that someone",
        "start": 174.17,
        "duration": 3.24
    },
    {
        "text": "is in distress they can proactively",
        "start": 175.79,
        "duration": 3.9
    },
    {
        "text": "reach out so they will you know do",
        "start": 177.41,
        "duration": 3.57
    },
    {
        "text": "something like get them on the phone and",
        "start": 179.69,
        "duration": 4.01
    },
    {
        "text": "say hey we know this bill is coming up",
        "start": 180.98,
        "duration": 4.86
    },
    {
        "text": "maybe we'll help you pay that down they",
        "start": 183.7,
        "duration": 3.76
    },
    {
        "text": "actually gave an example of paying down",
        "start": 185.84,
        "duration": 3.9
    },
    {
        "text": "a washer and dryer for a particular",
        "start": 187.46,
        "duration": 4.38
    },
    {
        "text": "member to help them a little bit get",
        "start": 189.74,
        "duration": 4.29
    },
    {
        "text": "above water and if they can do that then",
        "start": 191.84,
        "duration": 5.07
    },
    {
        "text": "it actually helps prevent larger default",
        "start": 194.03,
        "duration": 4.95
    },
    {
        "text": "in the long run and that makes it better",
        "start": 196.91,
        "duration": 5.61
    },
    {
        "text": "for everyone so in this scenario we have",
        "start": 198.98,
        "duration": 6.03
    },
    {
        "text": "a data set of financial transaction and",
        "start": 202.52,
        "duration": 4.5
    },
    {
        "text": "members who have defaulted or not and",
        "start": 205.01,
        "duration": 4.38
    },
    {
        "text": "that naturally lead both to a binary",
        "start": 207.02,
        "duration": 4.95
    },
    {
        "text": "classifier right so our approach here",
        "start": 209.39,
        "duration": 6.09
    },
    {
        "text": "was to build a binary classifier with a",
        "start": 211.97,
        "duration": 6.42
    },
    {
        "text": "couple of different methods so I started",
        "start": 215.48,
        "duration": 4.71
    },
    {
        "text": "with some traditional machine learning",
        "start": 218.39,
        "duration": 4.77
    },
    {
        "text": "methods using a support vector machine",
        "start": 220.19,
        "duration": 4.53
    },
    {
        "text": "and then went into some deep learning",
        "start": 223.16,
        "duration": 4.26
    },
    {
        "text": "methods using LST M's in CNN to solve",
        "start": 224.72,
        "duration": 4.71
    },
    {
        "text": "the problem so I'll talk through all of",
        "start": 227.42,
        "duration": 6.42
    },
    {
        "text": "those implementations so the data the",
        "start": 229.43,
        "duration": 7.68
    },
    {
        "text": "data that we had available to us were",
        "start": 233.84,
        "duration": 5.73
    },
    {
        "text": "the transaction history of members and",
        "start": 237.11,
        "duration": 4.62
    },
    {
        "text": "we knew whether they had defaulted or",
        "start": 239.57,
        "duration": 3.9
    },
    {
        "text": "not and we also knew what they were",
        "start": 241.73,
        "duration": 3.77
    },
    {
        "text": "spending what categories they were",
        "start": 243.47,
        "duration": 6.12
    },
    {
        "text": "spending in this is you can see here a",
        "start": 245.5,
        "duration": 7.12
    },
    {
        "text": "list of some of the categories that we",
        "start": 249.59,
        "duration": 4.679
    },
    {
        "text": "did use so think similar to mint.com",
        "start": 252.62,
        "duration": 4.23
    },
    {
        "text": "right your transactions roll up into a",
        "start": 254.269,
        "duration": 5.25
    },
    {
        "text": "certain category and by looking at these",
        "start": 256.85,
        "duration": 5.25
    },
    {
        "text": "categories could we build a data set and",
        "start": 259.519,
        "duration": 5.131
    },
    {
        "text": "do some feature engineering to build out",
        "start": 262.1,
        "duration": 3.87
    },
    {
        "text": "our feature vectors to solve this",
        "start": 264.65,
        "duration": 2.2
    },
    {
        "text": "problem",
        "start": 265.97,
        "duration": 3.16
    },
    {
        "text": "and here's how we did the feature",
        "start": 266.85,
        "duration": 5.04
    },
    {
        "text": "engineering so what we did to",
        "start": 269.13,
        "duration": 5.22
    },
    {
        "text": "incorporate this notion of time so we",
        "start": 271.89,
        "duration": 5.37
    },
    {
        "text": "have samples of people and we have",
        "start": 274.35,
        "duration": 4.59
    },
    {
        "text": "features about them what they vent how",
        "start": 277.26,
        "duration": 4.59
    },
    {
        "text": "do we actually capture time so we did to",
        "start": 278.94,
        "duration": 5.94
    },
    {
        "text": "solve that the first approach that we",
        "start": 281.85,
        "duration": 6.33
    },
    {
        "text": "tried is taking the transaction activity",
        "start": 284.88,
        "duration": 6.18
    },
    {
        "text": "aggregating it monthly weekly daily and",
        "start": 288.18,
        "duration": 5.64
    },
    {
        "text": "creating a feature vector from that from",
        "start": 291.06,
        "duration": 4.8
    },
    {
        "text": "those aggregates so you can see from",
        "start": 293.82,
        "duration": 4.32
    },
    {
        "text": "this sample data here we you have the",
        "start": 295.86,
        "duration": 3.78
    },
    {
        "text": "label you have the ground truth right",
        "start": 298.14,
        "duration": 4.16
    },
    {
        "text": "for each member and then you have a",
        "start": 299.64,
        "duration": 6.18
    },
    {
        "text": "vector of all of the categories the",
        "start": 302.3,
        "duration": 5.35
    },
    {
        "text": "transaction categories and the weekly",
        "start": 305.82,
        "duration": 4.17
    },
    {
        "text": "sum of what they did in that week and",
        "start": 307.65,
        "duration": 4.32
    },
    {
        "text": "then you concatenate that over and over",
        "start": 309.99,
        "duration": 4.08
    },
    {
        "text": "and I have that just this red line drawn",
        "start": 311.97,
        "duration": 5.04
    },
    {
        "text": "through so you can see that time period",
        "start": 314.07,
        "duration": 5.67
    },
    {
        "text": "one all the way through time period 120",
        "start": 317.01,
        "duration": 4.44
    },
    {
        "text": "and there there were several more time",
        "start": 319.74,
        "duration": 3.93
    },
    {
        "text": "periods that we took into account but",
        "start": 321.45,
        "duration": 4.83
    },
    {
        "text": "that kind of shows you that transaction",
        "start": 323.67,
        "duration": 4.89
    },
    {
        "text": "category is rolled up by week and over",
        "start": 326.28,
        "duration": 5.79
    },
    {
        "text": "time again to kind of do this in a",
        "start": 328.56,
        "duration": 5.64
    },
    {
        "text": "notation format to make it a little bit",
        "start": 332.07,
        "duration": 3.96
    },
    {
        "text": "clearer we have samples down the rows",
        "start": 334.2,
        "duration": 3.99
    },
    {
        "text": "features across the columns like you",
        "start": 336.03,
        "duration": 3.63
    },
    {
        "text": "would structure a typical machine",
        "start": 338.19,
        "duration": 5.76
    },
    {
        "text": "learning problem right and for T not all",
        "start": 339.66,
        "duration": 8.34
    },
    {
        "text": "the way through TX you have separate",
        "start": 343.95,
        "duration": 7.02
    },
    {
        "text": "weeks and then you have I have a tuple",
        "start": 348.0,
        "duration": 5.22
    },
    {
        "text": "here under the feature so f is for",
        "start": 350.97,
        "duration": 4.62
    },
    {
        "text": "features I have a tuple representing the",
        "start": 353.22,
        "duration": 5.67
    },
    {
        "text": "first tuple is the time slice and the",
        "start": 355.59,
        "duration": 5.61
    },
    {
        "text": "second is the corresponds to the",
        "start": 358.89,
        "duration": 7.38
    },
    {
        "text": "category so with that that's a very",
        "start": 361.2,
        "duration": 6.81
    },
    {
        "text": "typical you know it's a future",
        "start": 366.27,
        "duration": 3.6
    },
    {
        "text": "engineered process to get you your",
        "start": 368.01,
        "duration": 3.51
    },
    {
        "text": "feature vectors that you'll feed into a",
        "start": 369.87,
        "duration": 3.81
    },
    {
        "text": "machine learning algorithm and what we",
        "start": 371.52,
        "duration": 3.6
    },
    {
        "text": "started with was traditional machine",
        "start": 373.68,
        "duration": 4.11
    },
    {
        "text": "learning so um one of the things that",
        "start": 375.12,
        "duration": 4.26
    },
    {
        "text": "you know we kind of run into being in",
        "start": 377.79,
        "duration": 3.81
    },
    {
        "text": "commercial software engineering as we'll",
        "start": 379.38,
        "duration": 5.18
    },
    {
        "text": "see partners and and startups and",
        "start": 381.6,
        "duration": 5.46
    },
    {
        "text": "customers in very different stages of",
        "start": 384.56,
        "duration": 4.51
    },
    {
        "text": "the game so they might not always know",
        "start": 387.06,
        "duration": 3.66
    },
    {
        "text": "something about machine learning and",
        "start": 389.07,
        "duration": 3.21
    },
    {
        "text": "that's okay that's totally fine we'll",
        "start": 390.72,
        "duration": 3.48
    },
    {
        "text": "we'll work with them to educate them and",
        "start": 392.28,
        "duration": 4.77
    },
    {
        "text": "help them come up to speed and the way",
        "start": 394.2,
        "duration": 4.92
    },
    {
        "text": "that I found works really well is to",
        "start": 397.05,
        "duration": 3.359
    },
    {
        "text": "start with traditional machine",
        "start": 399.12,
        "duration": 2.669
    },
    {
        "text": "earning start with something like a",
        "start": 400.409,
        "duration": 2.94
    },
    {
        "text": "support vector machine where it's pretty",
        "start": 401.789,
        "duration": 3.69
    },
    {
        "text": "intuitive right if you draw out this",
        "start": 403.349,
        "duration": 6.12
    },
    {
        "text": "example this image of a support vector",
        "start": 405.479,
        "duration": 6.75
    },
    {
        "text": "machine with samples projected into the",
        "start": 409.469,
        "duration": 4.29
    },
    {
        "text": "future states and then a hyperplane",
        "start": 412.229,
        "duration": 4.14
    },
    {
        "text": "drawn through them it tend to be more",
        "start": 413.759,
        "duration": 4.59
    },
    {
        "text": "intuitive of how you might separate",
        "start": 416.369,
        "duration": 3.87
    },
    {
        "text": "those classes and the notion of",
        "start": 418.349,
        "duration": 3.54
    },
    {
        "text": "separable ax t",
        "start": 420.239,
        "duration": 4.621
    },
    {
        "text": "gets well illustrated here so this is",
        "start": 421.889,
        "duration": 4.74
    },
    {
        "text": "where we started it also happens to be a",
        "start": 424.86,
        "duration": 4.079
    },
    {
        "text": "really good algorithm for our specific",
        "start": 426.629,
        "duration": 4.71
    },
    {
        "text": "case because SVM's work very very well",
        "start": 428.939,
        "duration": 4.26
    },
    {
        "text": "when you have many many features right",
        "start": 431.339,
        "duration": 3.3
    },
    {
        "text": "when you're working in high dimension",
        "start": 433.199,
        "duration": 4.25
    },
    {
        "text": "also works really well when you have",
        "start": 434.639,
        "duration": 5.76
    },
    {
        "text": "fewer training examples so we only had a",
        "start": 437.449,
        "duration": 5.23
    },
    {
        "text": "couple of hundred training examples per",
        "start": 440.399,
        "duration": 4.92
    },
    {
        "text": "class so given that we started here and",
        "start": 442.679,
        "duration": 5.64
    },
    {
        "text": "actually did it worked pretty well",
        "start": 445.319,
        "duration": 5.91
    },
    {
        "text": "I used a couple of different tools I'll",
        "start": 448.319,
        "duration": 5.58
    },
    {
        "text": "show you one of them this one is a tool",
        "start": 451.229,
        "duration": 5.94
    },
    {
        "text": "that I like to use when I kind of first",
        "start": 453.899,
        "duration": 7.23
    },
    {
        "text": "get my hands on some data just for",
        "start": 457.169,
        "duration": 12.24
    },
    {
        "text": "prototyping so let's say so I'm reading",
        "start": 461.129,
        "duration": 11.04
    },
    {
        "text": "in my data and this takes a quick 20",
        "start": 469.409,
        "duration": 4.11
    },
    {
        "text": "minutes to build this is our machine",
        "start": 472.169,
        "duration": 4.05
    },
    {
        "text": "learning studio and when I get to the",
        "start": 473.519,
        "duration": 5.07
    },
    {
        "text": "end okay let's see let me go ahead and",
        "start": 476.219,
        "duration": 11.31
    },
    {
        "text": "refresh that should come up shortly so",
        "start": 478.589,
        "duration": 10.53
    },
    {
        "text": "yeah what what I'll do while this is",
        "start": 487.529,
        "duration": 3.54
    },
    {
        "text": "loading is I'll bring in the data see if",
        "start": 489.119,
        "duration": 3.81
    },
    {
        "text": "it is separable and if I get a pretty",
        "start": 491.069,
        "duration": 3.87
    },
    {
        "text": "good ROC curve on my binary classifier",
        "start": 492.929,
        "duration": 4.38
    },
    {
        "text": "then I know that I have data I've",
        "start": 494.939,
        "duration": 3.66
    },
    {
        "text": "engineered features that are going to",
        "start": 497.309,
        "duration": 5.85
    },
    {
        "text": "work pretty well so to show you I've got",
        "start": 498.599,
        "duration": 6.87
    },
    {
        "text": "this module that shows me it shows me a",
        "start": 503.159,
        "duration": 5.01
    },
    {
        "text": "visualization on the on the output here",
        "start": 505.469,
        "duration": 4.44
    },
    {
        "text": "looks pretty good right pretty good",
        "start": 508.169,
        "duration": 5.28
    },
    {
        "text": "Rucker so given these metrics and that",
        "start": 509.909,
        "duration": 6.9
    },
    {
        "text": "ROC curve I know that we're ready to to",
        "start": 513.449,
        "duration": 6.421
    },
    {
        "text": "go I can teach this to a start-up a",
        "start": 516.809,
        "duration": 4.65
    },
    {
        "text": "customer who is new to machine learning",
        "start": 519.87,
        "duration": 3.93
    },
    {
        "text": "and have them really understand the",
        "start": 521.459,
        "duration": 4.62
    },
    {
        "text": "machine learning process and I'll flip",
        "start": 523.8,
        "duration": 7.74
    },
    {
        "text": "back over here to kind of show you how",
        "start": 526.079,
        "duration": 7.361
    },
    {
        "text": "how this works",
        "start": 531.54,
        "duration": 6.4
    },
    {
        "text": "so all right I've got my machine",
        "start": 533.44,
        "duration": 6.93
    },
    {
        "text": "learning workflow on on the right hand",
        "start": 537.94,
        "duration": 3.96
    },
    {
        "text": "side and this tends to be really good",
        "start": 540.37,
        "duration": 3.21
    },
    {
        "text": "for education and then once you",
        "start": 541.9,
        "duration": 3.48
    },
    {
        "text": "understand the process of reading and",
        "start": 543.58,
        "duration": 4.23
    },
    {
        "text": "data normalizing it splitting training",
        "start": 545.38,
        "duration": 5.7
    },
    {
        "text": "testing evaluating then you can easily",
        "start": 547.81,
        "duration": 5.22
    },
    {
        "text": "transfer that to code right so get right",
        "start": 551.08,
        "duration": 3.78
    },
    {
        "text": "into scikit-learn and it's a pretty",
        "start": 553.03,
        "duration": 4.29
    },
    {
        "text": "close not exactly one to one but it's a",
        "start": 554.86,
        "duration": 3.93
    },
    {
        "text": "pretty close mapping of how these things",
        "start": 557.32,
        "duration": 3.39
    },
    {
        "text": "work so it's a really good stepping",
        "start": 558.79,
        "duration": 4.65
    },
    {
        "text": "stone so we go into scikit-learn and and",
        "start": 560.71,
        "duration": 5.37
    },
    {
        "text": "do an SVM implementation and get pretty",
        "start": 563.44,
        "duration": 5.0
    },
    {
        "text": "good results at ninety percent accuracy",
        "start": 566.08,
        "duration": 6.72
    },
    {
        "text": "all right so next now that we've gotten",
        "start": 568.44,
        "duration": 6.76
    },
    {
        "text": "in into Python and we're writing code",
        "start": 572.8,
        "duration": 4.35
    },
    {
        "text": "with python scikit-learn then it's a",
        "start": 575.2,
        "duration": 3.63
    },
    {
        "text": "natural time to introduce how we might",
        "start": 577.15,
        "duration": 3.21
    },
    {
        "text": "solve this in a deep learning approach",
        "start": 578.83,
        "duration": 5.04
    },
    {
        "text": "right so LS TM were the the stand for",
        "start": 580.36,
        "duration": 5.67
    },
    {
        "text": "long short-term memory networks they're",
        "start": 583.87,
        "duration": 4.08
    },
    {
        "text": "very very good at sequence",
        "start": 586.03,
        "duration": 3.39
    },
    {
        "text": "classification and sequence labeling",
        "start": 587.95,
        "duration": 3.57
    },
    {
        "text": "they also happen to be very good at",
        "start": 589.42,
        "duration": 4.68
    },
    {
        "text": "forecasting but in this case we use them",
        "start": 591.52,
        "duration": 4.98
    },
    {
        "text": "for classification so given a specific",
        "start": 594.1,
        "duration": 4.44
    },
    {
        "text": "sequence can you determine which class",
        "start": 596.5,
        "duration": 3.72
    },
    {
        "text": "that belongs to and if you think about",
        "start": 598.54,
        "duration": 3.66
    },
    {
        "text": "it we have time series data right we",
        "start": 600.22,
        "duration": 4.02
    },
    {
        "text": "have sequences of transactions for",
        "start": 602.2,
        "duration": 4.26
    },
    {
        "text": "people who have defaulted and sequences",
        "start": 604.24,
        "duration": 4.17
    },
    {
        "text": "of transactions for people who have not",
        "start": 606.46,
        "duration": 4.77
    },
    {
        "text": "defaulted given that can we feed it into",
        "start": 608.41,
        "duration": 6.3
    },
    {
        "text": "an LS TM and get us something all right",
        "start": 611.23,
        "duration": 6.18
    },
    {
        "text": "so the data input here before I showed",
        "start": 614.71,
        "duration": 5.16
    },
    {
        "text": "you something that was 2d we had it was",
        "start": 617.41,
        "duration": 5.19
    },
    {
        "text": "like a spreadsheet right rows of samples",
        "start": 619.87,
        "duration": 4.89
    },
    {
        "text": "and then column the futures now I'm",
        "start": 622.6,
        "duration": 4.26
    },
    {
        "text": "going to turn that into a 3d matrix",
        "start": 624.76,
        "duration": 4.89
    },
    {
        "text": "where now I don't just have samples and",
        "start": 626.86,
        "duration": 5.04
    },
    {
        "text": "features I have samples time steps and",
        "start": 629.65,
        "duration": 6.57
    },
    {
        "text": "features you saw this before I'm going",
        "start": 631.9,
        "duration": 6.3
    },
    {
        "text": "to simplify this notation just a little",
        "start": 636.22,
        "duration": 4.98
    },
    {
        "text": "bit to take out that tuple it is a bit",
        "start": 638.2,
        "duration": 4.65
    },
    {
        "text": "redundant if you understand that it does",
        "start": 641.2,
        "duration": 4.86
    },
    {
        "text": "correlate to the time slice and what I'm",
        "start": 642.85,
        "duration": 5.85
    },
    {
        "text": "gonna do is just transform this into not",
        "start": 646.06,
        "duration": 5.19
    },
    {
        "text": "just samples and features that samples",
        "start": 648.7,
        "duration": 4.59
    },
    {
        "text": "time steps and then features so you're",
        "start": 651.25,
        "duration": 4.2
    },
    {
        "text": "really just projecting the data into a",
        "start": 653.29,
        "duration": 5.16
    },
    {
        "text": "3d representation of it where at each",
        "start": 655.45,
        "duration": 5.7
    },
    {
        "text": "time slice you get the features for that",
        "start": 658.45,
        "duration": 4.2
    },
    {
        "text": "happened at that time size so we're",
        "start": 661.15,
        "duration": 3.42
    },
    {
        "text": "still looking at weekly aggregate to",
        "start": 662.65,
        "duration": 4.29
    },
    {
        "text": "make this just a little bit more clear",
        "start": 664.57,
        "duration": 5.07
    },
    {
        "text": "I've labeled sort of the columns and and",
        "start": 666.94,
        "duration": 4.95
    },
    {
        "text": "the rows that we have here again your",
        "start": 669.64,
        "duration": 5.1
    },
    {
        "text": "training examples down the rows and then",
        "start": 671.89,
        "duration": 6.48
    },
    {
        "text": "for us we use the transaction categories",
        "start": 674.74,
        "duration": 5.01
    },
    {
        "text": "and the dollar amount that they spent",
        "start": 678.37,
        "duration": 5.46
    },
    {
        "text": "per each week and then you can feed that",
        "start": 679.75,
        "duration": 5.94
    },
    {
        "text": "into an LST I'm I'll actually go and",
        "start": 683.83,
        "duration": 6.33
    },
    {
        "text": "kind of show you the code for that a",
        "start": 685.69,
        "duration": 15.21
    },
    {
        "text": "little bit all right so so here is the",
        "start": 690.16,
        "duration": 13.62
    },
    {
        "text": "Azure machine learning workbench and we",
        "start": 700.9,
        "duration": 4.71
    },
    {
        "text": "have a couple of different tools but I",
        "start": 703.78,
        "duration": 4.53
    },
    {
        "text": "wanted to show you my code that I have",
        "start": 705.61,
        "duration": 8.13
    },
    {
        "text": "here and this is really the the meaty",
        "start": 708.31,
        "duration": 9.12
    },
    {
        "text": "part right in here so this isn't Harris",
        "start": 713.74,
        "duration": 6.03
    },
    {
        "text": "after you define your model then you",
        "start": 717.43,
        "duration": 4.46
    },
    {
        "text": "just build your LS GM layer right and",
        "start": 719.77,
        "duration": 4.65
    },
    {
        "text": "you're defining the number of time sets",
        "start": 721.89,
        "duration": 4.51
    },
    {
        "text": "the number of features and the number of",
        "start": 724.42,
        "duration": 5.25
    },
    {
        "text": "output notes and then following the LS",
        "start": 726.4,
        "duration": 5.22
    },
    {
        "text": "TM input layer then you add a gents",
        "start": 729.67,
        "duration": 3.84
    },
    {
        "text": "layer to handle the classification step",
        "start": 731.62,
        "duration": 3.6
    },
    {
        "text": "so the dense layer is just the fully",
        "start": 733.51,
        "duration": 3.36
    },
    {
        "text": "connected part of your neural network",
        "start": 735.22,
        "duration": 4.89
    },
    {
        "text": "and that will with a sigmoid there that",
        "start": 736.87,
        "duration": 4.77
    },
    {
        "text": "will give you the classification does it",
        "start": 740.11,
        "duration": 4.5
    },
    {
        "text": "belong to zero or one not default or",
        "start": 741.64,
        "duration": 8.16
    },
    {
        "text": "default and if I go over to my run",
        "start": 744.61,
        "duration": 9.71
    },
    {
        "text": "history I can take a look at the latest",
        "start": 749.8,
        "duration": 7.65
    },
    {
        "text": "run that I did and how that performs so",
        "start": 754.32,
        "duration": 6.07
    },
    {
        "text": "I'll scroll all the way down and in this",
        "start": 757.45,
        "duration": 5.73
    },
    {
        "text": "particular run we got to 93 percent",
        "start": 760.39,
        "duration": 3.72
    },
    {
        "text": "accuracy",
        "start": 763.18,
        "duration": 4.95
    },
    {
        "text": "yeah so next we have a CNN and the",
        "start": 764.11,
        "duration": 6.45
    },
    {
        "text": "reason I wanted to use the CNN is while",
        "start": 768.13,
        "duration": 4.56
    },
    {
        "text": "they are typically used in image based",
        "start": 770.56,
        "duration": 4.65
    },
    {
        "text": "tasks they also happen to be very good",
        "start": 772.69,
        "duration": 4.23
    },
    {
        "text": "with sequence labeling and the reason",
        "start": 775.21,
        "duration": 3.87
    },
    {
        "text": "why is because CNN's if you think about",
        "start": 776.92,
        "duration": 3.78
    },
    {
        "text": "the fact that they're good at images",
        "start": 779.08,
        "duration": 3.6
    },
    {
        "text": "that means that they're good at spatial",
        "start": 780.7,
        "duration": 4.14
    },
    {
        "text": "relationships right so understanding",
        "start": 782.68,
        "duration": 4.35
    },
    {
        "text": "where pixels are in images they're very",
        "start": 784.84,
        "duration": 4.65
    },
    {
        "text": "very good at understanding so given that",
        "start": 787.03,
        "duration": 5.48
    },
    {
        "text": "nature of them that quality of being",
        "start": 789.49,
        "duration": 6.0
    },
    {
        "text": "excellent at spatial reasoning and they",
        "start": 792.51,
        "duration": 4.87
    },
    {
        "text": "they're a really good fit for a problem",
        "start": 795.49,
        "duration": 4.5
    },
    {
        "text": "like this and we did find that we got to",
        "start": 797.38,
        "duration": 3.09
    },
    {
        "text": "about nine",
        "start": 799.99,
        "duration": 2.52
    },
    {
        "text": "five percent accuracy here the only",
        "start": 800.47,
        "duration": 3.78
    },
    {
        "text": "difference that you have is that instead",
        "start": 802.51,
        "duration": 4.8
    },
    {
        "text": "of using a 2d filter kernel you'll use a",
        "start": 804.25,
        "duration": 6.72
    },
    {
        "text": "1d filter so you some of you might be",
        "start": 807.31,
        "duration": 6.24
    },
    {
        "text": "many of you I'm sure familiar with a CNN",
        "start": 810.97,
        "duration": 5.03
    },
    {
        "text": "model where we do image classification",
        "start": 813.55,
        "duration": 5.55
    },
    {
        "text": "you'll see the kernel on the right-hand",
        "start": 816.0,
        "duration": 4.18
    },
    {
        "text": "side on that car",
        "start": 819.1,
        "duration": 3.03
    },
    {
        "text": "that's a 2d kernel that will slide",
        "start": 820.18,
        "duration": 4.02
    },
    {
        "text": "across that image to do feature",
        "start": 822.13,
        "duration": 6.81
    },
    {
        "text": "extraction for non image data you would",
        "start": 824.2,
        "duration": 7.02
    },
    {
        "text": "just use the 1d kernel pretty simple so",
        "start": 828.94,
        "duration": 4.71
    },
    {
        "text": "what you do is you have a kernel this is",
        "start": 831.22,
        "duration": 4.67
    },
    {
        "text": "a this is just illustrative of a",
        "start": 833.65,
        "duration": 4.74
    },
    {
        "text": "convolution step so I have a kernel of",
        "start": 835.89,
        "duration": 5.35
    },
    {
        "text": "size three I have my stride set to two",
        "start": 838.39,
        "duration": 5.31
    },
    {
        "text": "so what that means is this kernel is",
        "start": 841.24,
        "duration": 6.69
    },
    {
        "text": "going to slide across with two steps and",
        "start": 843.7,
        "duration": 5.76
    },
    {
        "text": "we're going to take the dot product and",
        "start": 847.93,
        "duration": 4.08
    },
    {
        "text": "to get the output to our feature map and",
        "start": 849.46,
        "duration": 4.74
    },
    {
        "text": "that's going to go so on and so forth",
        "start": 852.01,
        "duration": 3.18
    },
    {
        "text": "all the way through the end of that",
        "start": 854.2,
        "duration": 9.0
    },
    {
        "text": "input vector and I will show you quickly",
        "start": 855.19,
        "duration": 15.87
    },
    {
        "text": "how that looks in code I think the",
        "start": 863.2,
        "duration": 9.96
    },
    {
        "text": "aspect ratio causes a little bit of a",
        "start": 871.06,
        "duration": 5.37
    },
    {
        "text": "lag all right so if I go over to my",
        "start": 873.16,
        "duration": 5.91
    },
    {
        "text": "transient and model then it's very",
        "start": 876.43,
        "duration": 5.49
    },
    {
        "text": "similar this this is the crux of it here",
        "start": 879.07,
        "duration": 5.01
    },
    {
        "text": "we define our model do you our",
        "start": 881.92,
        "duration": 5.78
    },
    {
        "text": "convolution step then the typical",
        "start": 884.08,
        "duration": 6.48
    },
    {
        "text": "dropout and Max fooling step so dropout",
        "start": 887.7,
        "duration": 5.86
    },
    {
        "text": "to prevent generalization or prevent",
        "start": 890.56,
        "duration": 5.64
    },
    {
        "text": "overfitting sorry to your training set",
        "start": 893.56,
        "duration": 5.52
    },
    {
        "text": "and allow better generalization and a",
        "start": 896.2,
        "duration": 5.13
    },
    {
        "text": "similar flattening so that we can take",
        "start": 899.08,
        "duration": 4.44
    },
    {
        "text": "all of those output feature map flatten",
        "start": 901.33,
        "duration": 5.82
    },
    {
        "text": "them into one single high dimensional",
        "start": 903.52,
        "duration": 5.73
    },
    {
        "text": "feature vector that can then be fed into",
        "start": 907.15,
        "duration": 5.1
    },
    {
        "text": "the algorithm into the dense layer all",
        "start": 909.25,
        "duration": 4.71
    },
    {
        "text": "right and finally the last thing that we",
        "start": 912.25,
        "duration": 3.51
    },
    {
        "text": "did is I talk to you about weekly roll",
        "start": 913.96,
        "duration": 4.89
    },
    {
        "text": "ups weekly aggregation the financial",
        "start": 915.76,
        "duration": 4.53
    },
    {
        "text": "institution that we worked with was",
        "start": 918.85,
        "duration": 3.33
    },
    {
        "text": "highly highly interested in seeing if we",
        "start": 920.29,
        "duration": 3.3
    },
    {
        "text": "could just take raw individual",
        "start": 922.18,
        "duration": 3.12
    },
    {
        "text": "transactions what if we didn't aggregate",
        "start": 923.59,
        "duration": 3.63
    },
    {
        "text": "anything could we find a pattern you",
        "start": 925.3,
        "duration": 3.96
    },
    {
        "text": "know there thought was hey what if",
        "start": 927.22,
        "duration": 5.55
    },
    {
        "text": "someone takes a huge sum of money out of",
        "start": 929.26,
        "duration": 4.42
    },
    {
        "text": "an ATM at a",
        "start": 932.77,
        "duration": 2.35
    },
    {
        "text": "you know and then they go get a payday",
        "start": 933.68,
        "duration": 3.3
    },
    {
        "text": "loan can it pick up patterns like that",
        "start": 935.12,
        "duration": 4.32
    },
    {
        "text": "to do that to determine whether they're",
        "start": 936.98,
        "duration": 4.799
    },
    {
        "text": "likely to default or not we found that",
        "start": 939.44,
        "duration": 4.89
    },
    {
        "text": "by feeding in individual transactions we",
        "start": 941.779,
        "duration": 5.011
    },
    {
        "text": "did get to about seventy at five percent",
        "start": 944.33,
        "duration": 4.8
    },
    {
        "text": "validation accuracy we didn't we only",
        "start": 946.79,
        "duration": 4.35
    },
    {
        "text": "again had a couple hundred samples per",
        "start": 949.13,
        "duration": 5.37
    },
    {
        "text": "class but traditionally you know the way",
        "start": 951.14,
        "duration": 5.4
    },
    {
        "text": "neural networks work is if you feed in a",
        "start": 954.5,
        "duration": 3.33
    },
    {
        "text": "lot more data than you'll get better",
        "start": 956.54,
        "duration": 2.82
    },
    {
        "text": "results and if you were to use a",
        "start": 957.83,
        "duration": 4.05
    },
    {
        "text": "traditional ml model so our our thought",
        "start": 959.36,
        "duration": 4.289
    },
    {
        "text": "is that with more data it will be able",
        "start": 961.88,
        "duration": 3.81
    },
    {
        "text": "to the LS jams will be able to learn",
        "start": 963.649,
        "duration": 4.711
    },
    {
        "text": "those patterns and get you a better",
        "start": 965.69,
        "duration": 7.41
    },
    {
        "text": "better than seventy-five percent all",
        "start": 968.36,
        "duration": 6.66
    },
    {
        "text": "right and this is what I showed you",
        "start": 973.1,
        "duration": 4.28
    },
    {
        "text": "before the way that we structure",
        "start": 975.02,
        "duration": 4.53
    },
    {
        "text": "individual transactions is just slightly",
        "start": 977.38,
        "duration": 7.149
    },
    {
        "text": "different instead of doing it with weeks",
        "start": 979.55,
        "duration": 7.289
    },
    {
        "text": "on the time steps we actually are",
        "start": 984.529,
        "duration": 4.711
    },
    {
        "text": "showing a transaction on the time step",
        "start": 986.839,
        "duration": 4.201
    },
    {
        "text": "so those are observations they can be",
        "start": 989.24,
        "duration": 4.08
    },
    {
        "text": "structured the same way where instead of",
        "start": 991.04,
        "duration": 4.919
    },
    {
        "text": "an actual time like week month or day",
        "start": 993.32,
        "duration": 5.58
    },
    {
        "text": "you can use observations in your LS TM",
        "start": 995.959,
        "duration": 5.911
    },
    {
        "text": "matrix so we have transaction not",
        "start": 998.9,
        "duration": 5.82
    },
    {
        "text": "through 2,000 transactions we found that",
        "start": 1001.87,
        "duration": 4.38
    },
    {
        "text": "that was actually pretty effective going",
        "start": 1004.72,
        "duration": 3.39
    },
    {
        "text": "back to thousand transactions in time",
        "start": 1006.25,
        "duration": 6.63
    },
    {
        "text": "and then instead of a weekly aggregate",
        "start": 1008.11,
        "duration": 8.55
    },
    {
        "text": "of dollar amounts for each category we",
        "start": 1012.88,
        "duration": 6.0
    },
    {
        "text": "have the category shown under on the",
        "start": 1016.66,
        "duration": 5.6
    },
    {
        "text": "futures as well as the amount the year",
        "start": 1018.88,
        "duration": 6.42
    },
    {
        "text": "month of I think it was yeah your month",
        "start": 1022.26,
        "duration": 5.949
    },
    {
        "text": "day an hour to inject back in time so by",
        "start": 1025.3,
        "duration": 5.55
    },
    {
        "text": "using transactions you lose that notion",
        "start": 1028.209,
        "duration": 4.801
    },
    {
        "text": "of time right instead of time SEP so we",
        "start": 1030.85,
        "duration": 4.38
    },
    {
        "text": "actually tried to inject it back in in",
        "start": 1033.01,
        "duration": 6.6
    },
    {
        "text": "the future space alright so that was",
        "start": 1035.23,
        "duration": 7.199
    },
    {
        "text": "that project I have my code up on github",
        "start": 1039.61,
        "duration": 5.55
    },
    {
        "text": "we also are always looking for great",
        "start": 1042.429,
        "duration": 4.201
    },
    {
        "text": "projects so if you have an interesting",
        "start": 1045.16,
        "duration": 3.45
    },
    {
        "text": "machine learning project or something",
        "start": 1046.63,
        "duration": 4.35
    },
    {
        "text": "else that should I showed on the first",
        "start": 1048.61,
        "duration": 3.66
    },
    {
        "text": "slide about commercial software",
        "start": 1050.98,
        "duration": 2.52
    },
    {
        "text": "engineering you can come talk to me",
        "start": 1052.27,
        "duration": 5.52
    },
    {
        "text": "after the talk and are we about at this",
        "start": 1053.5,
        "duration": 6.09
    },
    {
        "text": "should be transitioned directly yeah so",
        "start": 1057.79,
        "duration": 4.11
    },
    {
        "text": "we don't have time for Q&A right now but",
        "start": 1059.59,
        "duration": 3.93
    },
    {
        "text": "do you feel free to come up to me and",
        "start": 1061.9,
        "duration": 5.07
    },
    {
        "text": "then yeah awesome okay any questions",
        "start": 1063.52,
        "duration": 4.02
    },
    {
        "text": "while Reed",
        "start": 1066.97,
        "duration": 3.14
    },
    {
        "text": "setting up yeah",
        "start": 1067.54,
        "duration": 2.57
    },
    {
        "text": "[Music]",
        "start": 1071.11,
        "duration": 7.46
    }
]