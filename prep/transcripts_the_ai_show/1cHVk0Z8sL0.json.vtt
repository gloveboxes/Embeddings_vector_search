[
    {
        "text": "you're not going to want to miss this",
        "start": 0.179,
        "duration": 3.121
    },
    {
        "text": "episode of the AI show where we talk all",
        "start": 1.62,
        "duration": 3.36
    },
    {
        "text": "about next generation computer vision",
        "start": 3.3,
        "duration": 3.78
    },
    {
        "text": "capabilities with project Florence my",
        "start": 4.98,
        "duration": 3.96
    },
    {
        "text": "friend Adina trufanescu make sure you",
        "start": 7.08,
        "duration": 2.42
    },
    {
        "text": "tune in",
        "start": 8.94,
        "duration": 6.5
    },
    {
        "text": "[Music]",
        "start": 9.5,
        "duration": 8.08
    },
    {
        "text": "hello and welcome to this episode of the",
        "start": 15.44,
        "duration": 4.12
    },
    {
        "text": "AI show we're talking all about next",
        "start": 17.58,
        "duration": 3.72
    },
    {
        "text": "generation computer vision capabilities",
        "start": 19.56,
        "duration": 3.299
    },
    {
        "text": "with project Florence with my friend",
        "start": 21.3,
        "duration": 3.54
    },
    {
        "text": "Adina trufanescu hello Adina how you",
        "start": 22.859,
        "duration": 3.901
    },
    {
        "text": "doing my friend I said Thank you for",
        "start": 24.84,
        "duration": 3.0
    },
    {
        "text": "having me",
        "start": 26.76,
        "duration": 3.12
    },
    {
        "text": "also so tell us who you are and what you",
        "start": 27.84,
        "duration": 3.54
    },
    {
        "text": "do we've had you on before it's been a",
        "start": 29.88,
        "duration": 4.14
    },
    {
        "text": "while though yeah I know it's been a",
        "start": 31.38,
        "duration": 3.839
    },
    {
        "text": "while so",
        "start": 34.02,
        "duration": 2.36
    },
    {
        "text": "um",
        "start": 35.219,
        "duration": 5.041
    },
    {
        "text": "product manager Azure cognitive services",
        "start": 36.38,
        "duration": 6.699
    },
    {
        "text": "for vision I've been in this space for",
        "start": 40.26,
        "duration": 5.04
    },
    {
        "text": "four years now uh we've had a recent",
        "start": 43.079,
        "duration": 5.16
    },
    {
        "text": "release so I'm super excited to be here",
        "start": 45.3,
        "duration": 5.4
    },
    {
        "text": "now it's not just a recent release it's",
        "start": 48.239,
        "duration": 4.14
    },
    {
        "text": "pretty exciting I've been hearing all",
        "start": 50.7,
        "duration": 3.24
    },
    {
        "text": "about this thing called project Florence",
        "start": 52.379,
        "duration": 4.2
    },
    {
        "text": "is this the release can you tell us a",
        "start": 53.94,
        "duration": 4.68
    },
    {
        "text": "little bit more about it yeah for sure",
        "start": 56.579,
        "duration": 3.3
    },
    {
        "text": "so",
        "start": 58.62,
        "duration": 1.86
    },
    {
        "text": "um",
        "start": 59.879,
        "duration": 3.061
    },
    {
        "text": "you know I guess the audience knows uh",
        "start": 60.48,
        "duration": 4.319
    },
    {
        "text": "you know for the past two years the AI",
        "start": 62.94,
        "duration": 3.66
    },
    {
        "text": "industry has moved towards large",
        "start": 64.799,
        "duration": 3.901
    },
    {
        "text": "Foundation model so by now everybody",
        "start": 66.6,
        "duration": 4.5
    },
    {
        "text": "knows about child GPT you've heard about",
        "start": 68.7,
        "duration": 5.16
    },
    {
        "text": "Bert and Dali uh so this is the first",
        "start": 71.1,
        "duration": 5.28
    },
    {
        "text": "time when cognitive services for vision",
        "start": 73.86,
        "duration": 5.04
    },
    {
        "text": "is launching a large foundational model",
        "start": 76.38,
        "duration": 4.68
    },
    {
        "text": "the code name is Florence",
        "start": 78.9,
        "duration": 3.78
    },
    {
        "text": "um there are research papers published",
        "start": 81.06,
        "duration": 4.379
    },
    {
        "text": "on this uh you can look it up this is a",
        "start": 82.68,
        "duration": 5.16
    },
    {
        "text": "large foundational model trained with a",
        "start": 85.439,
        "duration": 5.761
    },
    {
        "text": "massive uh amount of data billions of",
        "start": 87.84,
        "duration": 6.84
    },
    {
        "text": "text to image pairs it is a multi-modal",
        "start": 91.2,
        "duration": 6.0
    },
    {
        "text": "so it is a multi-modality model meaning",
        "start": 94.68,
        "duration": 5.34
    },
    {
        "text": "that it's both language and vision and",
        "start": 97.2,
        "duration": 5.64
    },
    {
        "text": "because of that it enables new tasks in",
        "start": 100.02,
        "duration": 5.16
    },
    {
        "text": "the vision AI space so basically",
        "start": 102.84,
        "duration": 3.48
    },
    {
        "text": "um these are the things I'm going to",
        "start": 105.18,
        "duration": 3.479
    },
    {
        "text": "share later but something like uh text",
        "start": 106.32,
        "duration": 5.339
    },
    {
        "text": "to image retrieval or dense captions so",
        "start": 108.659,
        "duration": 4.74
    },
    {
        "text": "basically the language capabilities and",
        "start": 111.659,
        "duration": 4.021
    },
    {
        "text": "the visual Vision capabilities are",
        "start": 113.399,
        "duration": 4.68
    },
    {
        "text": "merged together in this new foundational",
        "start": 115.68,
        "duration": 3.36
    },
    {
        "text": "model",
        "start": 118.079,
        "duration": 2.82
    },
    {
        "text": "so how do these things actually work and",
        "start": 119.04,
        "duration": 3.359
    },
    {
        "text": "now you brought a slide and I want to",
        "start": 120.899,
        "duration": 3.241
    },
    {
        "text": "make sure you explain this because I",
        "start": 122.399,
        "duration": 3.421
    },
    {
        "text": "think for those that are hearing about",
        "start": 124.14,
        "duration": 4.64
    },
    {
        "text": "large language models when we talk about",
        "start": 125.82,
        "duration": 5.46
    },
    {
        "text": "multi-modal models they're a little bit",
        "start": 128.78,
        "duration": 3.76
    },
    {
        "text": "different can you explain them to us",
        "start": 131.28,
        "duration": 4.319
    },
    {
        "text": "yeah so uh think of it this way before",
        "start": 132.54,
        "duration": 6.059
    },
    {
        "text": "to train a modeling Vision you would",
        "start": 135.599,
        "duration": 5.401
    },
    {
        "text": "find like a data set for that particular",
        "start": 138.599,
        "duration": 4.621
    },
    {
        "text": "task so if you want to train a model for",
        "start": 141.0,
        "duration": 4.62
    },
    {
        "text": "object detection you label the data for",
        "start": 143.22,
        "duration": 4.379
    },
    {
        "text": "object detection and you train a model",
        "start": 145.62,
        "duration": 3.78
    },
    {
        "text": "which is pretty much like specific to",
        "start": 147.599,
        "duration": 4.381
    },
    {
        "text": "that task and then for each task you'll",
        "start": 149.4,
        "duration": 4.14
    },
    {
        "text": "have to train an independent model",
        "start": 151.98,
        "duration": 3.899
    },
    {
        "text": "whereas now with the Live Foundation and",
        "start": 153.54,
        "duration": 4.199
    },
    {
        "text": "model you train the large foundational",
        "start": 155.879,
        "duration": 4.141
    },
    {
        "text": "model once so this is where you bring",
        "start": 157.739,
        "duration": 5.28
    },
    {
        "text": "like the the large data set the large",
        "start": 160.02,
        "duration": 6.299
    },
    {
        "text": "training data set and then you train the",
        "start": 163.019,
        "duration": 6.061
    },
    {
        "text": "foundation model in this case the",
        "start": 166.319,
        "duration": 4.081
    },
    {
        "text": "foundation model for vision project",
        "start": 169.08,
        "duration": 4.14
    },
    {
        "text": "Florence it has both a language encoded",
        "start": 170.4,
        "duration": 4.8
    },
    {
        "text": "and image encoder so this is where the",
        "start": 173.22,
        "duration": 4.08
    },
    {
        "text": "language and vision capabilities are",
        "start": 175.2,
        "duration": 4.74
    },
    {
        "text": "being you know come together and then",
        "start": 177.3,
        "duration": 4.92
    },
    {
        "text": "you train adaptation models so these are",
        "start": 179.94,
        "duration": 4.5
    },
    {
        "text": "adaptation models which are fine-tuned",
        "start": 182.22,
        "duration": 5.159
    },
    {
        "text": "with additional data for the individual",
        "start": 184.44,
        "duration": 5.1
    },
    {
        "text": "tasks so the individual tasks that we",
        "start": 187.379,
        "duration": 4.321
    },
    {
        "text": "show here are classification image",
        "start": 189.54,
        "duration": 4.559
    },
    {
        "text": "retrieval object detection segmentation",
        "start": 191.7,
        "duration": 4.679
    },
    {
        "text": "captions and then coming in the future",
        "start": 194.099,
        "duration": 4.5
    },
    {
        "text": "we have like a bunch more that I you",
        "start": 196.379,
        "duration": 3.78
    },
    {
        "text": "know that are gonna come in the future",
        "start": 198.599,
        "duration": 4.14
    },
    {
        "text": "so the idea here is that you train the",
        "start": 200.159,
        "duration": 5.281
    },
    {
        "text": "the large Foundation once and then you",
        "start": 202.739,
        "duration": 5.22
    },
    {
        "text": "adapt It For The Individual tasks",
        "start": 205.44,
        "duration": 5.04
    },
    {
        "text": "I see so help me understand this and",
        "start": 207.959,
        "duration": 4.081
    },
    {
        "text": "because this is the part that sometimes",
        "start": 210.48,
        "duration": 4.8
    },
    {
        "text": "is confusing at least to me when you're",
        "start": 212.04,
        "duration": 6.6
    },
    {
        "text": "training a multi-modal model and I have",
        "start": 215.28,
        "duration": 4.98
    },
    {
        "text": "some training in machine learning my",
        "start": 218.64,
        "duration": 3.12
    },
    {
        "text": "understanding is that when you're doing",
        "start": 220.26,
        "duration": 3.539
    },
    {
        "text": "machine learning there is like the right",
        "start": 221.76,
        "duration": 4.32
    },
    {
        "text": "answer and then there's like the input",
        "start": 223.799,
        "duration": 4.681
    },
    {
        "text": "and you're trying to minimize a loss",
        "start": 226.08,
        "duration": 4.26
    },
    {
        "text": "function over those things what does",
        "start": 228.48,
        "duration": 4.44
    },
    {
        "text": "this look like for multi-modal things",
        "start": 230.34,
        "duration": 6.5
    },
    {
        "text": "because it has language and uh uh Vision",
        "start": 232.92,
        "duration": 7.08
    },
    {
        "text": "what is it calculating a loss over in",
        "start": 236.84,
        "duration": 5.08
    },
    {
        "text": "order to optimize its parameters the",
        "start": 240.0,
        "duration": 3.379
    },
    {
        "text": "data set is",
        "start": 241.92,
        "duration": 6.48
    },
    {
        "text": "image and text pairs meaning that you go",
        "start": 243.379,
        "duration": 7.841
    },
    {
        "text": "and find a data set where the images the",
        "start": 248.4,
        "duration": 5.28
    },
    {
        "text": "images also have not labels but they",
        "start": 251.22,
        "duration": 4.919
    },
    {
        "text": "have the description of the image and",
        "start": 253.68,
        "duration": 5.7
    },
    {
        "text": "then basically the image to text pair is",
        "start": 256.139,
        "duration": 6.421
    },
    {
        "text": "pushed through the training process and",
        "start": 259.38,
        "duration": 5.759
    },
    {
        "text": "this is where we have contrastive",
        "start": 262.56,
        "duration": 4.5
    },
    {
        "text": "learning which is like another form of",
        "start": 265.139,
        "duration": 5.041
    },
    {
        "text": "like self supervision learning and",
        "start": 267.06,
        "duration": 5.94
    },
    {
        "text": "basically in this process to train like",
        "start": 270.18,
        "duration": 5.22
    },
    {
        "text": "a vision and language model this is",
        "start": 273.0,
        "duration": 4.38
    },
    {
        "text": "where a language encoder and an image",
        "start": 275.4,
        "duration": 4.32
    },
    {
        "text": "encoder are being trained such that you",
        "start": 277.38,
        "duration": 4.68
    },
    {
        "text": "can push like the images and text",
        "start": 279.72,
        "duration": 4.8
    },
    {
        "text": "through the same model and then this is",
        "start": 282.06,
        "duration": 4.139
    },
    {
        "text": "where you bring together like both",
        "start": 284.52,
        "duration": 4.38
    },
    {
        "text": "capabilities of language and vision I",
        "start": 286.199,
        "duration": 5.341
    },
    {
        "text": "see so so basic because I don't know in",
        "start": 288.9,
        "duration": 4.38
    },
    {
        "text": "my narrow thinking it's like oh we have",
        "start": 291.54,
        "duration": 3.599
    },
    {
        "text": "a single problem and we're trying but",
        "start": 293.28,
        "duration": 4.02
    },
    {
        "text": "now these foundational models are",
        "start": 295.139,
        "duration": 4.441
    },
    {
        "text": "learning separate encodings but then",
        "start": 297.3,
        "duration": 3.959
    },
    {
        "text": "they're putting them together as they go",
        "start": 299.58,
        "duration": 3.54
    },
    {
        "text": "through the optimization phase",
        "start": 301.259,
        "duration": 3.961
    },
    {
        "text": "right and then that's why the adaptation",
        "start": 303.12,
        "duration": 4.56
    },
    {
        "text": "models come in because the light",
        "start": 305.22,
        "duration": 5.22
    },
    {
        "text": "Foundation model is good at like zero",
        "start": 307.68,
        "duration": 5.16
    },
    {
        "text": "shot like this broad range kind of",
        "start": 310.44,
        "duration": 4.199
    },
    {
        "text": "things but then if you want to have like",
        "start": 312.84,
        "duration": 4.98
    },
    {
        "text": "high accuracy for very narrow tasks that",
        "start": 314.639,
        "duration": 5.161
    },
    {
        "text": "this is what you do additional fine",
        "start": 317.82,
        "duration": 3.84
    },
    {
        "text": "tuning of the large foundational model",
        "start": 319.8,
        "duration": 4.2
    },
    {
        "text": "so the if you look at this image you",
        "start": 321.66,
        "duration": 4.56
    },
    {
        "text": "have the training data which is like the",
        "start": 324.0,
        "duration": 3.96
    },
    {
        "text": "massive training data that you train the",
        "start": 326.22,
        "duration": 3.18
    },
    {
        "text": "foundational model you have the",
        "start": 327.96,
        "duration": 3.299
    },
    {
        "text": "foundation model itself and then you",
        "start": 329.4,
        "duration": 4.139
    },
    {
        "text": "have this adaptation models and then",
        "start": 331.259,
        "duration": 4.621
    },
    {
        "text": "these adaptation models actually produce",
        "start": 333.539,
        "duration": 4.621
    },
    {
        "text": "the individual models for the tasks",
        "start": 335.88,
        "duration": 3.42
    },
    {
        "text": "above",
        "start": 338.16,
        "duration": 4.5
    },
    {
        "text": "I see so it's basically the the large",
        "start": 339.3,
        "duration": 6.72
    },
    {
        "text": "foundational model has a joint latent",
        "start": 342.66,
        "duration": 5.22
    },
    {
        "text": "space with these two different",
        "start": 346.02,
        "duration": 3.239
    },
    {
        "text": "modalities",
        "start": 347.88,
        "duration": 3.659
    },
    {
        "text": "that is correct and then you know in the",
        "start": 349.259,
        "duration": 4.38
    },
    {
        "text": "fullness of time what we're striving for",
        "start": 351.539,
        "duration": 4.921
    },
    {
        "text": "is like four modality so when you're",
        "start": 353.639,
        "duration": 4.5
    },
    {
        "text": "talking about like you're talking about",
        "start": 356.46,
        "duration": 3.84
    },
    {
        "text": "images right so images are pretty much",
        "start": 358.139,
        "duration": 4.261
    },
    {
        "text": "like language and image and then you",
        "start": 360.3,
        "duration": 3.839
    },
    {
        "text": "have that dual modality but then when",
        "start": 362.4,
        "duration": 3.9
    },
    {
        "text": "you bring in video into the picture this",
        "start": 364.139,
        "duration": 4.201
    },
    {
        "text": "is where you have like multi this is",
        "start": 366.3,
        "duration": 4.44
    },
    {
        "text": "where your true modality you know comes",
        "start": 368.34,
        "duration": 4.74
    },
    {
        "text": "in I see and that that makes a lot more",
        "start": 370.74,
        "duration": 4.5
    },
    {
        "text": "sense because like I think for some",
        "start": 373.08,
        "duration": 4.32
    },
    {
        "text": "reason I heard the adaptation models and",
        "start": 375.24,
        "duration": 4.26
    },
    {
        "text": "I just heard that way too fast you're",
        "start": 377.4,
        "duration": 4.859
    },
    {
        "text": "basically training the base model to",
        "start": 379.5,
        "duration": 5.4
    },
    {
        "text": "understand images and text at the same",
        "start": 382.259,
        "duration": 4.38
    },
    {
        "text": "time and then you're using the",
        "start": 384.9,
        "duration": 4.44
    },
    {
        "text": "adaptation models to specifically go to",
        "start": 386.639,
        "duration": 6.601
    },
    {
        "text": "a task right and then some tasks were",
        "start": 389.34,
        "duration": 6.24
    },
    {
        "text": "not possible before for instance when",
        "start": 393.24,
        "duration": 5.16
    },
    {
        "text": "you we have a new task for uh we call it",
        "start": 395.58,
        "duration": 5.16
    },
    {
        "text": "text to image retrieval and DC enables",
        "start": 398.4,
        "duration": 4.739
    },
    {
        "text": "like finding similar images with the",
        "start": 400.74,
        "duration": 4.5
    },
    {
        "text": "text query that pass that was not",
        "start": 403.139,
        "duration": 3.961
    },
    {
        "text": "possible before because you didn't have",
        "start": 405.24,
        "duration": 3.6
    },
    {
        "text": "like a language model and you didn't",
        "start": 407.1,
        "duration": 4.379
    },
    {
        "text": "have the multimodality I see because",
        "start": 408.84,
        "duration": 4.799
    },
    {
        "text": "they they're now in the joint space if",
        "start": 411.479,
        "duration": 4.081
    },
    {
        "text": "you're typing some text give me all the",
        "start": 413.639,
        "duration": 5.161
    },
    {
        "text": "elephants it's able to know about what a",
        "start": 415.56,
        "duration": 4.919
    },
    {
        "text": "picture of an elephant looks like with",
        "start": 418.8,
        "duration": 3.54
    },
    {
        "text": "those words kind of thing that's right",
        "start": 420.479,
        "duration": 4.141
    },
    {
        "text": "so think about like you push the words",
        "start": 422.34,
        "duration": 4.799
    },
    {
        "text": "to the language part of the model the",
        "start": 424.62,
        "duration": 4.799
    },
    {
        "text": "language encoder and the you extract",
        "start": 427.139,
        "duration": 5.041
    },
    {
        "text": "vectors in a vector space you push the",
        "start": 429.419,
        "duration": 5.941
    },
    {
        "text": "images through the an image encoder you",
        "start": 432.18,
        "duration": 5.1
    },
    {
        "text": "extract the vectors in the same Vector",
        "start": 435.36,
        "duration": 3.959
    },
    {
        "text": "space and then you find the cosine",
        "start": 437.28,
        "duration": 3.84
    },
    {
        "text": "distance between vectors and this way",
        "start": 439.319,
        "duration": 4.081
    },
    {
        "text": "you find similar images absolutely",
        "start": 441.12,
        "duration": 5.28
    },
    {
        "text": "brilliant I'd love to see some of this",
        "start": 443.4,
        "duration": 6.359
    },
    {
        "text": "stuff in practice do you have a demo of",
        "start": 446.4,
        "duration": 4.5
    },
    {
        "text": "what this looks like for people that",
        "start": 449.759,
        "duration": 3.361
    },
    {
        "text": "don't want to I'm a fan of the latent",
        "start": 450.9,
        "duration": 4.799
    },
    {
        "text": "space and Vector math I'm a huge fan but",
        "start": 453.12,
        "duration": 4.079
    },
    {
        "text": "for people I just want to use this stuff",
        "start": 455.699,
        "duration": 3.661
    },
    {
        "text": "how could they do this for their",
        "start": 457.199,
        "duration": 3.12
    },
    {
        "text": "business",
        "start": 459.36,
        "duration": 2.1
    },
    {
        "text": "so",
        "start": 460.319,
        "duration": 2.88
    },
    {
        "text": "um this is what a visual studio comes",
        "start": 461.46,
        "duration": 4.019
    },
    {
        "text": "into play so I'm gonna show you that",
        "start": 463.199,
        "duration": 4.081
    },
    {
        "text": "so if you're familiar with the studios",
        "start": 465.479,
        "duration": 4.44
    },
    {
        "text": "for cognitive Services we have a speech",
        "start": 467.28,
        "duration": 4.5
    },
    {
        "text": "Studio we have a language Studio this is",
        "start": 469.919,
        "duration": 4.801
    },
    {
        "text": "Vision Studio we are very happy that you",
        "start": 471.78,
        "duration": 4.44
    },
    {
        "text": "can actually use it I'm gonna show you",
        "start": 474.72,
        "duration": 3.9
    },
    {
        "text": "in a myth and you can use it as a signed",
        "start": 476.22,
        "duration": 5.34
    },
    {
        "text": "in Azure user for more complex tasks but",
        "start": 478.62,
        "duration": 4.38
    },
    {
        "text": "you can actually use it without even",
        "start": 481.56,
        "duration": 3.96
    },
    {
        "text": "being signed in so let me show you let",
        "start": 483.0,
        "duration": 4.02
    },
    {
        "text": "me show you what it is",
        "start": 485.52,
        "duration": 3.66
    },
    {
        "text": "all right so what you see here is Vision",
        "start": 487.02,
        "duration": 5.22
    },
    {
        "text": "studio and you can see the features are",
        "start": 489.18,
        "duration": 6.66
    },
    {
        "text": "organized by type so you can see optical",
        "start": 492.24,
        "duration": 5.579
    },
    {
        "text": "character recognition special analysis",
        "start": 495.84,
        "duration": 4.799
    },
    {
        "text": "image analysis and under the feature",
        "start": 497.819,
        "duration": 5.041
    },
    {
        "text": "task the feature tab you can find here",
        "start": 500.639,
        "duration": 6.12
    },
    {
        "text": "like the latest things and let me start",
        "start": 502.86,
        "duration": 7.2
    },
    {
        "text": "by showing you uh",
        "start": 506.759,
        "duration": 6.181
    },
    {
        "text": "image captions so image captions we had",
        "start": 510.06,
        "duration": 5.94
    },
    {
        "text": "this for a while image captions we",
        "start": 512.94,
        "duration": 6.12
    },
    {
        "text": "reached human parity in 2020 but with",
        "start": 516.0,
        "duration": 5.52
    },
    {
        "text": "the large Foundation model now enabling",
        "start": 519.06,
        "duration": 5.82
    },
    {
        "text": "image captions like the the uh quality",
        "start": 521.52,
        "duration": 5.34
    },
    {
        "text": "of image captions continues to improve",
        "start": 524.88,
        "duration": 3.959
    },
    {
        "text": "so I'm gonna show you a few samples with",
        "start": 526.86,
        "duration": 3.659
    },
    {
        "text": "the images that Visual Studio already",
        "start": 528.839,
        "duration": 5.041
    },
    {
        "text": "has here so we have a group of cows",
        "start": 530.519,
        "duration": 5.161
    },
    {
        "text": "gazing a field",
        "start": 533.88,
        "duration": 3.899
    },
    {
        "text": "um you know let me click on this one a",
        "start": 535.68,
        "duration": 4.44
    },
    {
        "text": "man holding a surfboard surfboard on a",
        "start": 537.779,
        "duration": 4.861
    },
    {
        "text": "rock so this is the image description of",
        "start": 540.12,
        "duration": 4.8
    },
    {
        "text": "the picture the reason why I'm showing",
        "start": 542.64,
        "duration": 4.92
    },
    {
        "text": "you this is because we have now dense",
        "start": 544.92,
        "duration": 4.8
    },
    {
        "text": "captions so let me take you to the tab",
        "start": 547.56,
        "duration": 6.12
    },
    {
        "text": "and there's captions is basically",
        "start": 549.72,
        "duration": 6.96
    },
    {
        "text": "providing you not only the caption of",
        "start": 553.68,
        "duration": 5.159
    },
    {
        "text": "the full image which is you know what",
        "start": 556.68,
        "duration": 4.38
    },
    {
        "text": "this one is doing here but it's giving",
        "start": 558.839,
        "duration": 6.421
    },
    {
        "text": "you 10 regions inside the image with",
        "start": 561.06,
        "duration": 6.18
    },
    {
        "text": "that particular description so think of",
        "start": 565.26,
        "duration": 3.96
    },
    {
        "text": "it as you know if you think about like",
        "start": 567.24,
        "duration": 4.44
    },
    {
        "text": "the object section you know this would",
        "start": 569.22,
        "duration": 4.679
    },
    {
        "text": "be like object detection with labels of",
        "start": 571.68,
        "duration": 4.44
    },
    {
        "text": "objects but in this case it's giving you",
        "start": 573.899,
        "duration": 4.741
    },
    {
        "text": "regions inside the image and is giving",
        "start": 576.12,
        "duration": 6.86
    },
    {
        "text": "you the description of the image",
        "start": 578.64,
        "duration": 7.319
    },
    {
        "text": "is the adaptation model doing that all",
        "start": 582.98,
        "duration": 6.039
    },
    {
        "text": "on top of the the large model without it",
        "start": 585.959,
        "duration": 5.281
    },
    {
        "text": "yeah yeah that is correct so this is an",
        "start": 589.019,
        "duration": 4.741
    },
    {
        "text": "example of an adaptation model for",
        "start": 591.24,
        "duration": 5.76
    },
    {
        "text": "captions and dense captions that's cool",
        "start": 593.76,
        "duration": 6.66
    },
    {
        "text": "cool all right let me show you one",
        "start": 597.0,
        "duration": 7.68
    },
    {
        "text": "that I love I I I'm really I'm really",
        "start": 600.42,
        "duration": 6.419
    },
    {
        "text": "excited about this one this is the text",
        "start": 604.68,
        "duration": 4.5
    },
    {
        "text": "to image retrieval uh we call it image",
        "start": 606.839,
        "duration": 5.101
    },
    {
        "text": "retrieval the actual scenario actually",
        "start": 609.18,
        "duration": 4.62
    },
    {
        "text": "is like searching for photos with",
        "start": 611.94,
        "duration": 3.899
    },
    {
        "text": "natural language this is where the open",
        "start": 613.8,
        "duration": 4.14
    },
    {
        "text": "vocabulary capabilities of the model",
        "start": 615.839,
        "duration": 4.921
    },
    {
        "text": "come in because of the the amount of you",
        "start": 617.94,
        "duration": 4.38
    },
    {
        "text": "know training data that goes in the",
        "start": 620.76,
        "duration": 3.66
    },
    {
        "text": "model you pretty much like can search",
        "start": 622.32,
        "duration": 4.5
    },
    {
        "text": "with anything uh so this is where the",
        "start": 624.42,
        "duration": 4.08
    },
    {
        "text": "language uh the natural language",
        "start": 626.82,
        "duration": 5.4
    },
    {
        "text": "capabilities come in and we have here a",
        "start": 628.5,
        "duration": 5.76
    },
    {
        "text": "set of sample images that we provided",
        "start": 632.22,
        "duration": 5.16
    },
    {
        "text": "for you to try with if you are not",
        "start": 634.26,
        "duration": 5.16
    },
    {
        "text": "signed in you'll be able to try and try",
        "start": 637.38,
        "duration": 3.84
    },
    {
        "text": "with these images and then you can bring",
        "start": 639.42,
        "duration": 3.599
    },
    {
        "text": "your own so I'm gonna show you like how",
        "start": 641.22,
        "duration": 4.799
    },
    {
        "text": "to bring your own but first let me pick",
        "start": 643.019,
        "duration": 5.641
    },
    {
        "text": "one of these and",
        "start": 646.019,
        "duration": 5.481
    },
    {
        "text": "what we did here",
        "start": 648.66,
        "duration": 6.299
    },
    {
        "text": "we are let me position it",
        "start": 651.5,
        "duration": 6.72
    },
    {
        "text": "um we are giving you a search query",
        "start": 654.959,
        "duration": 6.421
    },
    {
        "text": "example uh just for you to see like the",
        "start": 658.22,
        "duration": 4.6
    },
    {
        "text": "kind of things that you can search for",
        "start": 661.38,
        "duration": 3.66
    },
    {
        "text": "and then you can also bring your own",
        "start": 662.82,
        "duration": 4.8
    },
    {
        "text": "custom search query so let me pick like",
        "start": 665.04,
        "duration": 6.0
    },
    {
        "text": "few of these we have uh employee wearing",
        "start": 667.62,
        "duration": 5.399
    },
    {
        "text": "a white safety hat so you can see here",
        "start": 671.04,
        "duration": 4.38
    },
    {
        "text": "like the top 10 results and then see",
        "start": 673.019,
        "duration": 5.041
    },
    {
        "text": "this slider here you know the model when",
        "start": 675.42,
        "duration": 4.74
    },
    {
        "text": "is looking for similarity in the vector",
        "start": 678.06,
        "duration": 3.24
    },
    {
        "text": "space",
        "start": 680.16,
        "duration": 2.94
    },
    {
        "text": "um you know the vector space is Broad",
        "start": 681.3,
        "duration": 3.779
    },
    {
        "text": "and is calculated based off all the",
        "start": 683.1,
        "duration": 4.38
    },
    {
        "text": "images in the set right so if you want",
        "start": 685.079,
        "duration": 4.081
    },
    {
        "text": "to see like the most relevant results",
        "start": 687.48,
        "duration": 3.84
    },
    {
        "text": "this is what you do but if you want to",
        "start": 689.16,
        "duration": 4.679
    },
    {
        "text": "see like you know all all the search",
        "start": 691.32,
        "duration": 4.68
    },
    {
        "text": "results and then you can pick your like",
        "start": 693.839,
        "duration": 6.12
    },
    {
        "text": "whatever you know relevance uh you know",
        "start": 696.0,
        "duration": 6.42
    },
    {
        "text": "you are comfortable with so this is how",
        "start": 699.959,
        "duration": 4.921
    },
    {
        "text": "you play with it and notice here like as",
        "start": 702.42,
        "duration": 4.8
    },
    {
        "text": "the images are ranked here based on",
        "start": 704.88,
        "duration": 4.74
    },
    {
        "text": "similarity you'll start seeing like the",
        "start": 707.22,
        "duration": 4.02
    },
    {
        "text": "results which are not exactly what you",
        "start": 709.62,
        "duration": 4.14
    },
    {
        "text": "are searching for but still within like",
        "start": 711.24,
        "duration": 5.159
    },
    {
        "text": "that similarity space",
        "start": 713.76,
        "duration": 5.639
    },
    {
        "text": "that that's cool uh and it's it's all",
        "start": 716.399,
        "duration": 5.821
    },
    {
        "text": "based upon that same model which is",
        "start": 719.399,
        "duration": 5.341
    },
    {
        "text": "absolutely amazing we used to have to",
        "start": 722.22,
        "duration": 4.98
    },
    {
        "text": "train like a ton of specialized models",
        "start": 724.74,
        "duration": 4.38
    },
    {
        "text": "for all these things and each one needed",
        "start": 727.2,
        "duration": 4.319
    },
    {
        "text": "to be babysat so to speak it's cool that",
        "start": 729.12,
        "duration": 4.26
    },
    {
        "text": "you're able to do this all with one",
        "start": 731.519,
        "duration": 3.601
    },
    {
        "text": "large model and some tiny adaptation",
        "start": 733.38,
        "duration": 4.44
    },
    {
        "text": "models yeah that's right so the other",
        "start": 735.12,
        "duration": 5.159
    },
    {
        "text": "thing that I want to show you here is",
        "start": 737.82,
        "duration": 4.079
    },
    {
        "text": "um you can bring your own search query",
        "start": 740.279,
        "duration": 3.901
    },
    {
        "text": "so the one that I want to show you here",
        "start": 741.899,
        "duration": 3.721
    },
    {
        "text": "is",
        "start": 744.18,
        "duration": 3.779
    },
    {
        "text": "um person",
        "start": 745.62,
        "duration": 6.659
    },
    {
        "text": "driving a forklift which is within the",
        "start": 747.959,
        "duration": 5.761
    },
    {
        "text": "realm of",
        "start": 752.279,
        "duration": 2.221
    },
    {
        "text": "um",
        "start": 753.72,
        "duration": 3.119
    },
    {
        "text": "of manufacturing so you can find here",
        "start": 754.5,
        "duration": 6.06
    },
    {
        "text": "like top search results like you can see",
        "start": 756.839,
        "duration": 6.361
    },
    {
        "text": "people driving forklifts so",
        "start": 760.56,
        "duration": 6.36
    },
    {
        "text": "um whether you uh try something that we",
        "start": 763.2,
        "duration": 5.699
    },
    {
        "text": "suggest or you bring your own thing then",
        "start": 766.92,
        "duration": 4.68
    },
    {
        "text": "you can do that here and like I said",
        "start": 768.899,
        "duration": 5.581
    },
    {
        "text": "um you can bring your own images so you",
        "start": 771.6,
        "duration": 4.62
    },
    {
        "text": "can try it with your own images so you",
        "start": 774.48,
        "duration": 3.9
    },
    {
        "text": "can bring your collection I already have",
        "start": 776.22,
        "duration": 3.96
    },
    {
        "text": "my collection here these are my personal",
        "start": 778.38,
        "duration": 4.56
    },
    {
        "text": "photos so you can see like my dog and",
        "start": 780.18,
        "duration": 6.06
    },
    {
        "text": "things which are going on here so let me",
        "start": 782.94,
        "duration": 5.82
    },
    {
        "text": "show you like a couple of things",
        "start": 786.24,
        "duration": 4.14
    },
    {
        "text": "so let me",
        "start": 788.76,
        "duration": 4.319
    },
    {
        "text": "come here and then I'm gonna say white",
        "start": 790.38,
        "duration": 5.639
    },
    {
        "text": "dog which is kind of an obvious one so",
        "start": 793.079,
        "duration": 4.621
    },
    {
        "text": "you can see like my puppy being",
        "start": 796.019,
        "duration": 3.361
    },
    {
        "text": "displayed here",
        "start": 797.7,
        "duration": 4.199
    },
    {
        "text": "um but then the model I told you that",
        "start": 799.38,
        "duration": 4.38
    },
    {
        "text": "the model has been trained with this",
        "start": 801.899,
        "duration": 4.021
    },
    {
        "text": "like huge amount of data it can reason",
        "start": 803.76,
        "duration": 4.98
    },
    {
        "text": "with external knowledge and it knows of",
        "start": 805.92,
        "duration": 6.479
    },
    {
        "text": "things so it can find things which even",
        "start": 808.74,
        "duration": 5.88
    },
    {
        "text": "if they are not labeled you know you can",
        "start": 812.399,
        "duration": 3.361
    },
    {
        "text": "find them so I'm going to give you an",
        "start": 814.62,
        "duration": 2.88
    },
    {
        "text": "example and it's going to be obvious so",
        "start": 815.76,
        "duration": 4.139
    },
    {
        "text": "I have a picture here in my data set",
        "start": 817.5,
        "duration": 4.459
    },
    {
        "text": "let's see if I can find it",
        "start": 819.899,
        "duration": 7.141
    },
    {
        "text": "uh let me Lisa reset the search and I",
        "start": 821.959,
        "duration": 7.601
    },
    {
        "text": "have look at this picture here this is",
        "start": 827.04,
        "duration": 4.38
    },
    {
        "text": "the picture of Horseshoe Bend on",
        "start": 829.56,
        "duration": 5.7
    },
    {
        "text": "Colorado River okay so you can search",
        "start": 831.42,
        "duration": 8.12
    },
    {
        "text": "here with Colorado River",
        "start": 835.26,
        "duration": 4.28
    },
    {
        "text": "and you'll notice like the first you",
        "start": 840.66,
        "duration": 5.34
    },
    {
        "text": "know similar image the top result is",
        "start": 843.6,
        "duration": 4.859
    },
    {
        "text": "actually the Horseshoe band on Colorado",
        "start": 846.0,
        "duration": 5.459
    },
    {
        "text": "River and this requires like no metadata",
        "start": 848.459,
        "duration": 6.241
    },
    {
        "text": "no GPS information and you will find",
        "start": 851.459,
        "duration": 6.421
    },
    {
        "text": "this kind of thing so you can say rice",
        "start": 854.7,
        "duration": 5.699
    },
    {
        "text": "Canyon",
        "start": 857.88,
        "duration": 5.34
    },
    {
        "text": "and then the model will know to find the",
        "start": 860.399,
        "duration": 5.101
    },
    {
        "text": "images coming from Bryce Canyon and then",
        "start": 863.22,
        "duration": 4.32
    },
    {
        "text": "the last one that I love is Mount",
        "start": 865.5,
        "duration": 4.56
    },
    {
        "text": "Rainier",
        "start": 867.54,
        "duration": 4.739
    },
    {
        "text": "and then he's gonna find the pictures",
        "start": 870.06,
        "duration": 5.1
    },
    {
        "text": "that I have you know with my hikes on",
        "start": 872.279,
        "duration": 5.041
    },
    {
        "text": "Mount Rainier and this is I don't think",
        "start": 875.16,
        "duration": 3.9
    },
    {
        "text": "people",
        "start": 877.32,
        "duration": 3.6
    },
    {
        "text": "if I'm understanding this right these",
        "start": 879.06,
        "duration": 3.839
    },
    {
        "text": "are just random pictures that you",
        "start": 880.92,
        "duration": 3.18
    },
    {
        "text": "uploaded",
        "start": 882.899,
        "duration": 3.3
    },
    {
        "text": "into the system",
        "start": 884.1,
        "duration": 5.7
    },
    {
        "text": "you ran the the search query without",
        "start": 886.199,
        "duration": 6.601
    },
    {
        "text": "having to do anything the model was able",
        "start": 889.8,
        "duration": 4.86
    },
    {
        "text": "to do all of this searching without even",
        "start": 892.8,
        "duration": 4.92
    },
    {
        "text": "having to specialize that is correct so",
        "start": 894.66,
        "duration": 5.22
    },
    {
        "text": "basically what you come here and then",
        "start": 897.72,
        "duration": 4.26
    },
    {
        "text": "you can try it yourself you bring your",
        "start": 899.88,
        "duration": 4.8
    },
    {
        "text": "images from either your blob storage or",
        "start": 901.98,
        "duration": 5.7
    },
    {
        "text": "your local disk the images are actually",
        "start": 904.68,
        "duration": 4.98
    },
    {
        "text": "you it will take a little bit of time",
        "start": 907.68,
        "duration": 4.08
    },
    {
        "text": "because each image is gonna be pushed to",
        "start": 909.66,
        "duration": 5.76
    },
    {
        "text": "the model to extract the vectors for",
        "start": 911.76,
        "duration": 6.36
    },
    {
        "text": "each image and then think of it as you",
        "start": 915.42,
        "duration": 5.46
    },
    {
        "text": "have like an index built for your set of",
        "start": 918.12,
        "duration": 4.62
    },
    {
        "text": "images and then you come with the text",
        "start": 920.88,
        "duration": 3.959
    },
    {
        "text": "query and then the text query is going",
        "start": 922.74,
        "duration": 3.899
    },
    {
        "text": "to be pushed through the model we",
        "start": 924.839,
        "duration": 4.141
    },
    {
        "text": "extract the vectors from the text query",
        "start": 926.639,
        "duration": 4.14
    },
    {
        "text": "and then every time when you click",
        "start": 928.98,
        "duration": 4.32
    },
    {
        "text": "search here we calculate you know the",
        "start": 930.779,
        "duration": 4.261
    },
    {
        "text": "similarity by doing cosine distance",
        "start": 933.3,
        "duration": 4.32
    },
    {
        "text": "between the vectors of the text and",
        "start": 935.04,
        "duration": 4.56
    },
    {
        "text": "vectors of the images in the data set",
        "start": 937.62,
        "duration": 4.5
    },
    {
        "text": "and the relevance just basically widens",
        "start": 939.6,
        "duration": 4.979
    },
    {
        "text": "the angle for for the return result I'm",
        "start": 942.12,
        "duration": 4.56
    },
    {
        "text": "guessing exactly Yeah so basically look",
        "start": 944.579,
        "duration": 4.861
    },
    {
        "text": "at this one like if I say most relevant",
        "start": 946.68,
        "duration": 5.339
    },
    {
        "text": "you'll notice here that each image has",
        "start": 949.44,
        "duration": 5.699
    },
    {
        "text": "Mount Rainier in it if I move the slider",
        "start": 952.019,
        "duration": 6.12
    },
    {
        "text": "you'll see things coming in which may or",
        "start": 955.139,
        "duration": 4.741
    },
    {
        "text": "may not be so some you see what I mean",
        "start": 958.139,
        "duration": 4.861
    },
    {
        "text": "like you know the the similarity this is",
        "start": 959.88,
        "duration": 4.92
    },
    {
        "text": "where you come in and then as you build",
        "start": 963.0,
        "duration": 4.86
    },
    {
        "text": "your own uh you know as you start using",
        "start": 964.8,
        "duration": 5.88
    },
    {
        "text": "the apis and that's as you get this kind",
        "start": 967.86,
        "duration": 5.339
    },
    {
        "text": "of results you'll have tolerance you",
        "start": 970.68,
        "duration": 4.44
    },
    {
        "text": "know different users and different use",
        "start": 973.199,
        "duration": 4.44
    },
    {
        "text": "cases have like different tolerance for",
        "start": 975.12,
        "duration": 5.339
    },
    {
        "text": "the similarity and we don't cut off to",
        "start": 977.639,
        "duration": 4.621
    },
    {
        "text": "like some number of results we give you",
        "start": 980.459,
        "duration": 3.781
    },
    {
        "text": "like the full similarity and then you",
        "start": 982.26,
        "duration": 4.199
    },
    {
        "text": "decide what's relevant for you that's",
        "start": 984.24,
        "duration": 5.339
    },
    {
        "text": "cool so is there any other aspects of",
        "start": 986.459,
        "duration": 4.861
    },
    {
        "text": "Florence that the project Florence you",
        "start": 989.579,
        "duration": 4.141
    },
    {
        "text": "wanted to show us before I asked ask",
        "start": 991.32,
        "duration": 4.139
    },
    {
        "text": "another obvious question that's coming",
        "start": 993.72,
        "duration": 3.84
    },
    {
        "text": "to me",
        "start": 995.459,
        "duration": 4.981
    },
    {
        "text": "search capabilities on medium all right",
        "start": 997.56,
        "duration": 6.779
    },
    {
        "text": "oh wait video yes so",
        "start": 1000.44,
        "duration": 5.94
    },
    {
        "text": "let's talk about video for a minute",
        "start": 1004.339,
        "duration": 4.74
    },
    {
        "text": "because this one is interesting um this",
        "start": 1006.38,
        "duration": 5.519
    },
    {
        "text": "is a demo for how to do frame locator",
        "start": 1009.079,
        "duration": 6.0
    },
    {
        "text": "and summary on videos and I have here",
        "start": 1011.899,
        "duration": 5.221
    },
    {
        "text": "sample videos that we provide for you",
        "start": 1015.079,
        "duration": 4.26
    },
    {
        "text": "and then same as before you can try it",
        "start": 1017.12,
        "duration": 3.779
    },
    {
        "text": "with your own video",
        "start": 1019.339,
        "duration": 4.021
    },
    {
        "text": "the problem with video today is that",
        "start": 1020.899,
        "duration": 4.68
    },
    {
        "text": "there are many cameras deployed on",
        "start": 1023.36,
        "duration": 5.04
    },
    {
        "text": "physical spaces uh there are petabytes",
        "start": 1025.579,
        "duration": 5.401
    },
    {
        "text": "of video that are being stored every day",
        "start": 1028.4,
        "duration": 5.519
    },
    {
        "text": "but imagine having to search you know",
        "start": 1030.98,
        "duration": 5.219
    },
    {
        "text": "video footage for events of Interest",
        "start": 1033.919,
        "duration": 5.101
    },
    {
        "text": "like it can takes like you know if if",
        "start": 1036.199,
        "duration": 4.441
    },
    {
        "text": "there is a security incident for",
        "start": 1039.02,
        "duration": 4.08
    },
    {
        "text": "instance it can take like hours or days",
        "start": 1040.64,
        "duration": 4.62
    },
    {
        "text": "to actually find the incident in like",
        "start": 1043.1,
        "duration": 4.8
    },
    {
        "text": "video footage so this is where video",
        "start": 1045.26,
        "duration": 5.46
    },
    {
        "text": "summarization and video search or rather",
        "start": 1047.9,
        "duration": 5.1
    },
    {
        "text": "frame locator comes in where you can",
        "start": 1050.72,
        "duration": 3.959
    },
    {
        "text": "actually you know",
        "start": 1053.0,
        "duration": 4.02
    },
    {
        "text": "take a video snippet run it through",
        "start": 1054.679,
        "duration": 4.5
    },
    {
        "text": "summarization see a summary of what's",
        "start": 1057.02,
        "duration": 4.32
    },
    {
        "text": "going on and then get some ideas like",
        "start": 1059.179,
        "duration": 4.081
    },
    {
        "text": "the things that are happening and then",
        "start": 1061.34,
        "duration": 3.78
    },
    {
        "text": "the things that you can search for",
        "start": 1063.26,
        "duration": 4.32
    },
    {
        "text": "that's immediately useful just for",
        "start": 1065.12,
        "duration": 4.799
    },
    {
        "text": "editing a video for example if I can say",
        "start": 1067.58,
        "duration": 4.56
    },
    {
        "text": "oh I remember when Adina said this thing",
        "start": 1069.919,
        "duration": 3.781
    },
    {
        "text": "and I just type it in and it can go to",
        "start": 1072.14,
        "duration": 5.22
    },
    {
        "text": "the frame that would be really cool too",
        "start": 1073.7,
        "duration": 6.54
    },
    {
        "text": "that is true but with the correction",
        "start": 1077.36,
        "duration": 4.98
    },
    {
        "text": "that is not when Adina said something",
        "start": 1080.24,
        "duration": 4.439
    },
    {
        "text": "because we are not processing audio yet",
        "start": 1082.34,
        "duration": 5.1
    },
    {
        "text": "it's when you know I did that I did I",
        "start": 1084.679,
        "duration": 4.261
    },
    {
        "text": "did something",
        "start": 1087.44,
        "duration": 3.66
    },
    {
        "text": "which is even harder because I mean",
        "start": 1088.94,
        "duration": 4.08
    },
    {
        "text": "processing audio you can get the text",
        "start": 1091.1,
        "duration": 4.14
    },
    {
        "text": "out of but it's like oh she showed me",
        "start": 1093.02,
        "duration": 5.399
    },
    {
        "text": "this window that had this you know she",
        "start": 1095.24,
        "duration": 5.04
    },
    {
        "text": "showed me apples and then it would go to",
        "start": 1098.419,
        "duration": 3.601
    },
    {
        "text": "the frame where you're showing me apples",
        "start": 1100.28,
        "duration": 3.24
    },
    {
        "text": "it's basically what you're saying am I",
        "start": 1102.02,
        "duration": 3.12
    },
    {
        "text": "getting this right yeah yeah the same",
        "start": 1103.52,
        "duration": 4.38
    },
    {
        "text": "idea of like finding stimuli images so",
        "start": 1105.14,
        "duration": 5.1
    },
    {
        "text": "think about videos as collection of",
        "start": 1107.9,
        "duration": 4.44
    },
    {
        "text": "images it's just that they are organized",
        "start": 1110.24,
        "duration": 4.38
    },
    {
        "text": "in a in a video stream right so same as",
        "start": 1112.34,
        "duration": 5.4
    },
    {
        "text": "before when you come in with a set of",
        "start": 1114.62,
        "duration": 4.98
    },
    {
        "text": "images now you come in with a set of",
        "start": 1117.74,
        "duration": 4.62
    },
    {
        "text": "frames we sample a set of frames from",
        "start": 1119.6,
        "duration": 4.86
    },
    {
        "text": "the video and then we build the summary",
        "start": 1122.36,
        "duration": 5.28
    },
    {
        "text": "and then we build the search index by by",
        "start": 1124.46,
        "duration": 5.219
    },
    {
        "text": "collecting you know extracting the",
        "start": 1127.64,
        "duration": 4.26
    },
    {
        "text": "vectors from the frames and then same as",
        "start": 1129.679,
        "duration": 5.221
    },
    {
        "text": "before we can do like text similarity",
        "start": 1131.9,
        "duration": 4.98
    },
    {
        "text": "between the input text and the video",
        "start": 1134.9,
        "duration": 3.42
    },
    {
        "text": "frames in the video",
        "start": 1136.88,
        "duration": 3.659
    },
    {
        "text": "let's take a look at it all right so I",
        "start": 1138.32,
        "duration": 3.42
    },
    {
        "text": "have like a bunch of things but I'm",
        "start": 1140.539,
        "duration": 4.02
    },
    {
        "text": "gonna choose the data center one",
        "start": 1141.74,
        "duration": 5.76
    },
    {
        "text": "so",
        "start": 1144.559,
        "duration": 5.461
    },
    {
        "text": "this is a video of people doing things",
        "start": 1147.5,
        "duration": 5.46
    },
    {
        "text": "in a data center where you know some",
        "start": 1150.02,
        "duration": 4.98
    },
    {
        "text": "things uh should be done something",
        "start": 1152.96,
        "duration": 4.14
    },
    {
        "text": "shouldn't so I'm not gonna comment on",
        "start": 1155.0,
        "duration": 4.86
    },
    {
        "text": "that so basically you can summarize this",
        "start": 1157.1,
        "duration": 4.26
    },
    {
        "text": "video",
        "start": 1159.86,
        "duration": 4.8
    },
    {
        "text": "and then you can see here the things",
        "start": 1161.36,
        "duration": 6.24
    },
    {
        "text": "that are happening and what we are doing",
        "start": 1164.66,
        "duration": 6.06
    },
    {
        "text": "here we are trying to highlight the",
        "start": 1167.6,
        "duration": 4.56
    },
    {
        "text": "interesting things that are happening",
        "start": 1170.72,
        "duration": 3.6
    },
    {
        "text": "such that that can give you an idea of",
        "start": 1172.16,
        "duration": 5.22
    },
    {
        "text": "like uh what to search for so we have",
        "start": 1174.32,
        "duration": 4.739
    },
    {
        "text": "here the summary and then the",
        "start": 1177.38,
        "duration": 4.08
    },
    {
        "text": "interesting events with a person seeing",
        "start": 1179.059,
        "duration": 6.541
    },
    {
        "text": "running a person seeing falling uh uh",
        "start": 1181.46,
        "duration": 6.48
    },
    {
        "text": "unattended backpack workers showing",
        "start": 1185.6,
        "duration": 5.579
    },
    {
        "text": "climbing ladders so now you can come",
        "start": 1187.94,
        "duration": 5.34
    },
    {
        "text": "here and then you say locate specific",
        "start": 1191.179,
        "duration": 4.441
    },
    {
        "text": "frames and same as before we give you",
        "start": 1193.28,
        "duration": 4.62
    },
    {
        "text": "ideas what to search for but let's",
        "start": 1195.62,
        "duration": 3.84
    },
    {
        "text": "search for the things that the summary",
        "start": 1197.9,
        "duration": 3.779
    },
    {
        "text": "told us that are they're happening so we",
        "start": 1199.46,
        "duration": 5.12
    },
    {
        "text": "have person",
        "start": 1201.679,
        "duration": 2.901
    },
    {
        "text": "uh climbing a ladder",
        "start": 1204.64,
        "duration": 8.44
    },
    {
        "text": "let's see what it finds so basically it",
        "start": 1209.36,
        "duration": 5.819
    },
    {
        "text": "sound like multiple frames here so you",
        "start": 1213.08,
        "duration": 3.719
    },
    {
        "text": "can click on the frame and then you can",
        "start": 1215.179,
        "duration": 3.421
    },
    {
        "text": "see the frames where the person was",
        "start": 1216.799,
        "duration": 3.601
    },
    {
        "text": "found you know you can click show more",
        "start": 1218.6,
        "duration": 3.12
    },
    {
        "text": "and that's going to give you like",
        "start": 1220.4,
        "duration": 5.58
    },
    {
        "text": "additional frames we had one with person",
        "start": 1221.72,
        "duration": 5.04
    },
    {
        "text": "um",
        "start": 1225.98,
        "duration": 4.5
    },
    {
        "text": "person falling so let's try that one",
        "start": 1226.76,
        "duration": 6.96
    },
    {
        "text": "so there we go it found it and then same",
        "start": 1230.48,
        "duration": 5.1
    },
    {
        "text": "as before you can see the multiple",
        "start": 1233.72,
        "duration": 2.88
    },
    {
        "text": "results",
        "start": 1235.58,
        "duration": 3.959
    },
    {
        "text": "and let's pick one of these once here",
        "start": 1236.6,
        "duration": 6.5
    },
    {
        "text": "person with a laptop",
        "start": 1239.539,
        "duration": 3.561
    },
    {
        "text": "uh there we go person with a laptop",
        "start": 1243.14,
        "duration": 4.919
    },
    {
        "text": "walking through so you get the idea so",
        "start": 1245.48,
        "duration": 5.819
    },
    {
        "text": "basically you you have the same you know",
        "start": 1248.059,
        "duration": 5.821
    },
    {
        "text": "idea as before except now that you are",
        "start": 1251.299,
        "duration": 5.041
    },
    {
        "text": "finding frames inside videos",
        "start": 1253.88,
        "duration": 4.56
    },
    {
        "text": "that is that's awesome",
        "start": 1256.34,
        "duration": 4.199
    },
    {
        "text": "um and it's cool that once you",
        "start": 1258.44,
        "duration": 4.859
    },
    {
        "text": "understand the basic knowledge of the",
        "start": 1260.539,
        "duration": 5.461
    },
    {
        "text": "foundational model how many adaptations",
        "start": 1263.299,
        "duration": 4.861
    },
    {
        "text": "there are is pretty impressive now the",
        "start": 1266.0,
        "duration": 4.2
    },
    {
        "text": "question I have for you to follow up",
        "start": 1268.16,
        "duration": 4.92
    },
    {
        "text": "there's there's got to be some ways to",
        "start": 1270.2,
        "duration": 5.64
    },
    {
        "text": "customize these models or the",
        "start": 1273.08,
        "duration": 5.52
    },
    {
        "text": "adaptations is that possible at all yeah",
        "start": 1275.84,
        "duration": 6.66
    },
    {
        "text": "so the foundation model uh can also be",
        "start": 1278.6,
        "duration": 7.14
    },
    {
        "text": "customized uh we offer two customization",
        "start": 1282.5,
        "duration": 6.659
    },
    {
        "text": "tasks uh the most popular ones for",
        "start": 1285.74,
        "duration": 6.48
    },
    {
        "text": "um uh image classification and object uh",
        "start": 1289.159,
        "duration": 6.181
    },
    {
        "text": "detection so actually let me show you uh",
        "start": 1292.22,
        "duration": 6.959
    },
    {
        "text": "these capabilities here I am going to uh",
        "start": 1295.34,
        "duration": 5.3
    },
    {
        "text": "show you",
        "start": 1299.179,
        "duration": 5.341
    },
    {
        "text": "the model that I trained and to train a",
        "start": 1300.64,
        "duration": 6.7
    },
    {
        "text": "model uh you need a training data set so",
        "start": 1304.52,
        "duration": 5.22
    },
    {
        "text": "I have here a training data set",
        "start": 1307.34,
        "duration": 5.219
    },
    {
        "text": "um I already have the blob storage with",
        "start": 1309.74,
        "duration": 5.939
    },
    {
        "text": "the the training data this is a a custom",
        "start": 1312.559,
        "duration": 6.12
    },
    {
        "text": "model that detects commercial drones and",
        "start": 1315.679,
        "duration": 5.101
    },
    {
        "text": "you'll notice here that in Visual Studio",
        "start": 1318.679,
        "duration": 4.201
    },
    {
        "text": "there is no such thing as a labeling",
        "start": 1320.78,
        "duration": 4.2
    },
    {
        "text": "experience that is because we are now",
        "start": 1322.88,
        "duration": 4.919
    },
    {
        "text": "using the same label experience as Azure",
        "start": 1324.98,
        "duration": 4.86
    },
    {
        "text": "machine learning is using for training",
        "start": 1327.799,
        "duration": 4.981
    },
    {
        "text": "Azure machine learning models so yeah so",
        "start": 1329.84,
        "duration": 4.8
    },
    {
        "text": "one of the problem customers complained",
        "start": 1332.78,
        "duration": 3.96
    },
    {
        "text": "before was that you know with the custom",
        "start": 1334.64,
        "duration": 4.38
    },
    {
        "text": "vision service today you have to label",
        "start": 1336.74,
        "duration": 4.2
    },
    {
        "text": "the data in some format in Azure machine",
        "start": 1339.02,
        "duration": 3.18
    },
    {
        "text": "learning you have to label it in",
        "start": 1340.94,
        "duration": 3.18
    },
    {
        "text": "different format if you want to train",
        "start": 1342.2,
        "duration": 4.14
    },
    {
        "text": "models in both services and compare you",
        "start": 1344.12,
        "duration": 5.039
    },
    {
        "text": "have to label the data once so this is",
        "start": 1346.34,
        "duration": 5.76
    },
    {
        "text": "where now you can go into Azure machine",
        "start": 1349.159,
        "duration": 5.821
    },
    {
        "text": "learning and label your data and I'm",
        "start": 1352.1,
        "duration": 4.559
    },
    {
        "text": "going I'm gonna show you like the kind",
        "start": 1354.98,
        "duration": 4.02
    },
    {
        "text": "of data that we have here",
        "start": 1356.659,
        "duration": 5.161
    },
    {
        "text": "so let's go to the data",
        "start": 1359.0,
        "duration": 4.74
    },
    {
        "text": "and then you'll see here images of",
        "start": 1361.82,
        "duration": 3.479
    },
    {
        "text": "drones",
        "start": 1363.74,
        "duration": 5.28
    },
    {
        "text": "um so let's see review labels so this is",
        "start": 1365.299,
        "duration": 6.961
    },
    {
        "text": "an object detection so for for uh",
        "start": 1369.02,
        "duration": 5.279
    },
    {
        "text": "Simplicity we have a single label it's",
        "start": 1372.26,
        "duration": 4.38
    },
    {
        "text": "called drone so you get the idea here",
        "start": 1374.299,
        "duration": 4.321
    },
    {
        "text": "this is the object detection labeling",
        "start": 1376.64,
        "duration": 4.26
    },
    {
        "text": "for Azure machine learning once you're",
        "start": 1378.62,
        "duration": 4.26
    },
    {
        "text": "done labeling you come back to visual",
        "start": 1380.9,
        "duration": 5.159
    },
    {
        "text": "studio you import your training data and",
        "start": 1382.88,
        "duration": 4.679
    },
    {
        "text": "then you train a model",
        "start": 1386.059,
        "duration": 3.36
    },
    {
        "text": "I have here tomorrow which is already",
        "start": 1387.559,
        "duration": 3.181
    },
    {
        "text": "trained",
        "start": 1389.419,
        "duration": 4.861
    },
    {
        "text": "um so let me click on that you can see",
        "start": 1390.74,
        "duration": 6.84
    },
    {
        "text": "here let's see if I can bring it up uh",
        "start": 1394.28,
        "duration": 5.58
    },
    {
        "text": "you can see here the model accuracy",
        "start": 1397.58,
        "duration": 5.04
    },
    {
        "text": "which is determined on the training data",
        "start": 1399.86,
        "duration": 6.0
    },
    {
        "text": "set we we have added an evaluation run",
        "start": 1402.62,
        "duration": 5.16
    },
    {
        "text": "so you can evaluate the model with",
        "start": 1405.86,
        "duration": 4.679
    },
    {
        "text": "additional data sets and then the last",
        "start": 1407.78,
        "duration": 4.8
    },
    {
        "text": "thing that I want to show you is if you",
        "start": 1410.539,
        "duration": 5.101
    },
    {
        "text": "want to test your model then this is an",
        "start": 1412.58,
        "duration": 5.339
    },
    {
        "text": "object detection model so on detect",
        "start": 1415.64,
        "duration": 5.519
    },
    {
        "text": "common objects on images here you can",
        "start": 1417.919,
        "duration": 5.701
    },
    {
        "text": "select from the pre-trained vision model",
        "start": 1421.159,
        "duration": 5.581
    },
    {
        "text": "versus the custom model that you trained",
        "start": 1423.62,
        "duration": 5.1
    },
    {
        "text": "with and then you bring an image and",
        "start": 1426.74,
        "duration": 3.54
    },
    {
        "text": "then you run inference",
        "start": 1428.72,
        "duration": 3.66
    },
    {
        "text": "that's amazing now is this is this doing",
        "start": 1430.28,
        "duration": 5.04
    },
    {
        "text": "like uh is this changing the base model",
        "start": 1432.38,
        "duration": 6.539
    },
    {
        "text": "or is it building an adaptation model on",
        "start": 1435.32,
        "duration": 5.16
    },
    {
        "text": "top of the base model or what is it",
        "start": 1438.919,
        "duration": 5.181
    },
    {
        "text": "doing exactly so the the phone model",
        "start": 1440.48,
        "duration": 6.72
    },
    {
        "text": "serves as",
        "start": 1444.1,
        "duration": 7.18
    },
    {
        "text": "basis for applying transfer learning and",
        "start": 1447.2,
        "duration": 5.7
    },
    {
        "text": "I should I shouldn't say the foundation",
        "start": 1451.28,
        "duration": 4.259
    },
    {
        "text": "but the adaptation models above so",
        "start": 1452.9,
        "duration": 4.68
    },
    {
        "text": "basically an adaptation model for object",
        "start": 1455.539,
        "duration": 6.901
    },
    {
        "text": "detection saves as uh as as uh the basis",
        "start": 1457.58,
        "duration": 7.74
    },
    {
        "text": "for applying transfer learning and we",
        "start": 1462.44,
        "duration": 4.56
    },
    {
        "text": "have object detection and image",
        "start": 1465.32,
        "duration": 3.839
    },
    {
        "text": "classification so the adaptation model",
        "start": 1467.0,
        "duration": 4.14
    },
    {
        "text": "for image classification is going to",
        "start": 1469.159,
        "duration": 4.14
    },
    {
        "text": "save is going to be the basis for",
        "start": 1471.14,
        "duration": 3.72
    },
    {
        "text": "transfer learning for image",
        "start": 1473.299,
        "duration": 3.901
    },
    {
        "text": "classification this has all been really",
        "start": 1474.86,
        "duration": 3.96
    },
    {
        "text": "amazing where can people go to find out",
        "start": 1477.2,
        "duration": 3.42
    },
    {
        "text": "more",
        "start": 1478.82,
        "duration": 4.2
    },
    {
        "text": "um well we have the link on the screen",
        "start": 1480.62,
        "duration": 5.46
    },
    {
        "text": "so visual studio AK dot Ms Vision studio",
        "start": 1483.02,
        "duration": 5.1
    },
    {
        "text": "so this is the portal that I showed you",
        "start": 1486.08,
        "duration": 3.0
    },
    {
        "text": "here",
        "start": 1488.12,
        "duration": 3.179
    },
    {
        "text": "um go ahead try things out this is a no",
        "start": 1489.08,
        "duration": 4.5
    },
    {
        "text": "code experience and when you are ready",
        "start": 1491.299,
        "duration": 5.221
    },
    {
        "text": "to test the apis then we have",
        "start": 1493.58,
        "duration": 4.979
    },
    {
        "text": "documentation and then you can test the",
        "start": 1496.52,
        "duration": 5.46
    },
    {
        "text": "apis as well Adina this has been amazing",
        "start": 1498.559,
        "duration": 5.761
    },
    {
        "text": "I can understand why we're so excited",
        "start": 1501.98,
        "duration": 4.26
    },
    {
        "text": "about project Florence at least why I'm",
        "start": 1504.32,
        "duration": 3.18
    },
    {
        "text": "excited about it thank you so much for",
        "start": 1506.24,
        "duration": 2.939
    },
    {
        "text": "spending some time with us",
        "start": 1507.5,
        "duration": 4.5
    },
    {
        "text": "glad to be here and very excited about",
        "start": 1509.179,
        "duration": 4.74
    },
    {
        "text": "the vision AI capabilities for cognitive",
        "start": 1512.0,
        "duration": 3.84
    },
    {
        "text": "services please try it out",
        "start": 1513.919,
        "duration": 4.081
    },
    {
        "text": "awesome we've been learning all about",
        "start": 1515.84,
        "duration": 3.54
    },
    {
        "text": "next generation computer vision",
        "start": 1518.0,
        "duration": 4.08
    },
    {
        "text": "capabilities with project Florence on",
        "start": 1519.38,
        "duration": 3.9
    },
    {
        "text": "the AI show thank you so much for",
        "start": 1522.08,
        "duration": 2.459
    },
    {
        "text": "watching and hopefully we'll see you",
        "start": 1523.28,
        "duration": 2.12
    },
    {
        "text": "next time take care",
        "start": 1524.539,
        "duration": 8.67
    },
    {
        "text": "[Music]",
        "start": 1525.4,
        "duration": 7.809
    }
]