[
    {
        "text": ">> You're not going to want to miss this episode of the AI Show,",
        "start": 0.0,
        "duration": 2.37
    },
    {
        "text": "where we look at ONNX and specifically the ONNX Runtime.",
        "start": 2.37,
        "duration": 2.94
    },
    {
        "text": "How you can make smaller models that are",
        "start": 5.31,
        "duration": 2.61
    },
    {
        "text": "sometimes faster and maybe will",
        "start": 7.92,
        "duration": 1.53
    },
    {
        "text": "even fit on a phone. Make sure you tune in.",
        "start": 9.45,
        "duration": 1.98
    },
    {
        "text": "[MUSIC]",
        "start": 11.43,
        "duration": 8.039
    },
    {
        "text": ">> Hello and welcome to this episode of the AI Show.",
        "start": 19.469,
        "duration": 1.891
    },
    {
        "text": "We're going to talk about something really",
        "start": 21.36,
        "duration": 1.89
    },
    {
        "text": "cool called the ONNX Runtime.",
        "start": 23.25,
        "duration": 1.92
    },
    {
        "text": "I have someone special with us.",
        "start": 25.17,
        "duration": 1.62
    },
    {
        "text": "Emma, why don't you introduce yourself,",
        "start": 26.79,
        "duration": 1.46
    },
    {
        "text": "tell us who you are and what you do.",
        "start": 28.25,
        "duration": 1.535
    },
    {
        "text": ">> Hi, everyone. This is Emma.",
        "start": 29.785,
        "duration": 2.675
    },
    {
        "text": "I'm a Senior Program Manager in the AI Framework",
        "start": 32.46,
        "duration": 3.36
    },
    {
        "text": "Team under Microsoft Cloud and AI group.",
        "start": 35.82,
        "duration": 4.52
    },
    {
        "text": "Our team aims to make Machine Learning Engineer more",
        "start": 40.34,
        "duration": 3.825
    },
    {
        "text": "efficiently as through optimize the library tools and communities.",
        "start": 44.165,
        "duration": 5.73
    },
    {
        "text": "I'm focusing on AI Model Inference",
        "start": 49.895,
        "duration": 2.925
    },
    {
        "text": "and [inaudible] Acceleration with",
        "start": 52.82,
        "duration": 1.77
    },
    {
        "text": "ONNX and the ONNX Runtime for open and inter-operable AI.",
        "start": 54.59,
        "duration": 6.035
    },
    {
        "text": ">> This is fantastic. For those who don't know,",
        "start": 60.625,
        "duration": 3.51
    },
    {
        "text": "what is ONNX and then what is the ONNX Runtime?",
        "start": 64.135,
        "duration": 3.495
    },
    {
        "text": ">> Yeah. So for people who are not",
        "start": 67.63,
        "duration": 3.07
    },
    {
        "text": "familiar with ONNX and ONNX Runtime,",
        "start": 70.7,
        "duration": 3.24
    },
    {
        "text": "here is the graph which explains",
        "start": 73.94,
        "duration": 2.88
    },
    {
        "text": "what a role ONNX and the ONNX Runtime play.",
        "start": 76.82,
        "duration": 3.12
    },
    {
        "text": ">> You have to pause right here, there's a window on your screen.",
        "start": 79.94,
        "duration": 3.125
    },
    {
        "text": ">> Oh, I see.",
        "start": 83.065,
        "duration": 1.88
    },
    {
        "text": ">> Yeah, we have to move that over.",
        "start": 84.945,
        "duration": 1.815
    },
    {
        "text": "Did you see that [inaudible] ?",
        "start": 86.76,
        "duration": 2.34
    },
    {
        "text": ">> I did now, let me take a look.",
        "start": 89.1,
        "duration": 3.3
    },
    {
        "text": "Sorry, I didn't catch it,",
        "start": 92.4,
        "duration": 1.05
    },
    {
        "text": "but I'll go full screen here and Emma,",
        "start": 93.45,
        "duration": 2.145
    },
    {
        "text": "just reintroduce this slide and we'll be okay.",
        "start": 95.595,
        "duration": 3.225
    },
    {
        "text": ">> Hit it.",
        "start": 98.82,
        "duration": 1.62
    },
    {
        "text": ">> Should we start off from the beginning?",
        "start": 100.44,
        "duration": 4.125
    },
    {
        "text": ">> No, I'll ask you the question.",
        "start": 104.565,
        "duration": 1.425
    },
    {
        "text": "For those who don't know what ONNX is or what the ONNX Runtime,",
        "start": 105.99,
        "duration": 3.96
    },
    {
        "text": "how would you explain it to them?",
        "start": 109.95,
        "duration": 1.74
    },
    {
        "text": ">> Yeah, so for people who are",
        "start": 111.69,
        "duration": 2.96
    },
    {
        "text": "not familiar with ONNX and ONNX Runtime,",
        "start": 114.65,
        "duration": 3.089
    },
    {
        "text": "here is a graph which explains what a role ONNX and",
        "start": 117.739,
        "duration": 4.471
    },
    {
        "text": "ONNX Runtime play in the AI Software Stack.",
        "start": 122.21,
        "duration": 7.145
    },
    {
        "text": "There are a lot of front end framework",
        "start": 129.355,
        "duration": 3.61
    },
    {
        "text": "outside like PyTorch and TensorFlow.",
        "start": 132.965,
        "duration": 3.085
    },
    {
        "text": "Each framework has its own format of the model graph.",
        "start": 136.05,
        "duration": 4.55
    },
    {
        "text": "The graph serves as IR or",
        "start": 140.6,
        "duration": 2.94
    },
    {
        "text": "shorter for Intermediate Representations.",
        "start": 143.54,
        "duration": 4.44
    },
    {
        "text": "ONNX stands for Open Neural Network Exchange is",
        "start": 147.98,
        "duration": 4.89
    },
    {
        "text": "a standard format for representing",
        "start": 152.87,
        "duration": 3.24
    },
    {
        "text": "on both [inaudible] and the traditional machine learning models.",
        "start": 156.11,
        "duration": 3.735
    },
    {
        "text": "As our standard IR,",
        "start": 159.845,
        "duration": 2.145
    },
    {
        "text": "it allows model on-trend on different frameworks,",
        "start": 161.99,
        "duration": 3.93
    },
    {
        "text": "it can be converted to your ONNX format.",
        "start": 165.92,
        "duration": 3.185
    },
    {
        "text": "Then ONNX model can be inferenced with",
        "start": 169.105,
        "duration": 3.47
    },
    {
        "text": "ONNX runtime on which leverage a variety",
        "start": 172.575,
        "duration": 3.245
    },
    {
        "text": "of hardware accelerators to get at",
        "start": 175.82,
        "duration": 3.18
    },
    {
        "text": "their optimal performance for different hardware.",
        "start": 179.0,
        "duration": 3.65
    },
    {
        "text": "As you can see,",
        "start": 182.65,
        "duration": 1.61
    },
    {
        "text": "we build our comma inference stag on its",
        "start": 184.26,
        "duration": 3.105
    },
    {
        "text": "ONNX runtime for Microsoft,",
        "start": 187.365,
        "duration": 2.36
    },
    {
        "text": "which is a highly optimized across the machine learning of",
        "start": 189.725,
        "duration": 5.25
    },
    {
        "text": "inference lifecycle of a wider range of Cloud and edge device.",
        "start": 194.975,
        "duration": 6.875
    },
    {
        "text": ">> This is really cool and I really liked this.",
        "start": 201.85,
        "duration": 3.965
    },
    {
        "text": "ONNX feels like it's a PDF format",
        "start": 205.815,
        "duration": 3.645
    },
    {
        "text": "for models and ONNX runtime is what runs the actual thing.",
        "start": 209.46,
        "duration": 3.38
    },
    {
        "text": "My question to you is,",
        "start": 212.84,
        "duration": 1.14
    },
    {
        "text": "how do people run these ONNX models?",
        "start": 213.98,
        "duration": 3.06
    },
    {
        "text": "How do they operationalize them?",
        "start": 217.04,
        "duration": 1.83
    },
    {
        "text": ">> Yeah, that's a good question.",
        "start": 218.87,
        "duration": 3.31
    },
    {
        "text": "Since the beauty of ONNX is our framework into our probability,",
        "start": 222.77,
        "duration": 5.415
    },
    {
        "text": "you can turn on model from any frameworks and services,",
        "start": 228.185,
        "duration": 4.95
    },
    {
        "text": "which is of course, ONNX format.",
        "start": 233.135,
        "duration": 2.49
    },
    {
        "text": "This model can be combined either to",
        "start": 235.625,
        "duration": 2.34
    },
    {
        "text": "archivists are using ONNX converter tools,",
        "start": 237.965,
        "duration": 3.24
    },
    {
        "text": "they're not inference with ONNX runtime for",
        "start": 241.205,
        "duration": 3.375
    },
    {
        "text": "high performance on different hardware.",
        "start": 244.58,
        "duration": 3.58
    },
    {
        "text": "As you can see, ONNX Converters and",
        "start": 248.16,
        "duration": 3.065
    },
    {
        "text": "ONNX Runtime are the major pieces in this workflow.",
        "start": 251.225,
        "duration": 5.035
    },
    {
        "text": ">> This is really cool, who's using this?",
        "start": 257.45,
        "duration": 4.17
    },
    {
        "text": "Are there people using this already?",
        "start": 261.62,
        "duration": 2.56
    },
    {
        "text": ">> Actually, I have some more content.",
        "start": 266.78,
        "duration": 3.18
    },
    {
        "text": ">> Okay. Let's go full screen,",
        "start": 269.96,
        "duration": 1.825
    },
    {
        "text": "Cam. Okay keep going, sorry.",
        "start": 271.785,
        "duration": 3.15
    },
    {
        "text": ">> Yeah, in terms of model commercial,",
        "start": 274.935,
        "duration": 8.685
    },
    {
        "text": "there are a lot of",
        "start": 283.62,
        "duration": 1.41
    },
    {
        "text": "popular frameworks that are so-called commercial for ONNX.",
        "start": 285.03,
        "duration": 4.415
    },
    {
        "text": "For some of these, like PyTorch,",
        "start": 289.445,
        "duration": 3.0
    },
    {
        "text": "ONNX format, export a spewed in natively,",
        "start": 292.445,
        "duration": 3.604
    },
    {
        "text": "and for others like TensorFlow or Carers are there are",
        "start": 296.049,
        "duration": 4.121
    },
    {
        "text": "separate installable package that a can't handle the conversion.",
        "start": 300.17,
        "duration": 5.295
    },
    {
        "text": "ONNX runtime is our high-performance inference engine",
        "start": 305.465,
        "duration": 4.065
    },
    {
        "text": "until around ONNX models.",
        "start": 309.53,
        "duration": 3.17
    },
    {
        "text": "It is suppose CPU, GPU,",
        "start": 312.7,
        "duration": 3.32
    },
    {
        "text": "and VPU offers APIs covering a variety of languages,",
        "start": 316.02,
        "duration": 5.06
    },
    {
        "text": "including Python, C++, and Java Script and others.",
        "start": 321.08,
        "duration": 5.375
    },
    {
        "text": "It's also a cross platform and",
        "start": 326.455,
        "duration": 2.845
    },
    {
        "text": "compatible on the new Windows and Mac.",
        "start": 329.3,
        "duration": 3.21
    },
    {
        "text": "You get easily plug it into your existing, server now pipeline.",
        "start": 332.51,
        "duration": 5.12
    },
    {
        "text": "The highlight of ONNX runtime is open and extensible",
        "start": 337.63,
        "duration": 4.855
    },
    {
        "text": "architecture for easy optimizing",
        "start": 342.485,
        "duration": 3.015
    },
    {
        "text": "and accelerating machine learning inference.",
        "start": 345.5,
        "duration": 3.045
    },
    {
        "text": "It a marriage viewed in graph optimizations,",
        "start": 348.545,
        "duration": 3.285
    },
    {
        "text": "and on a lot of",
        "start": 351.83,
        "duration": 3.015
    },
    {
        "text": "hardware cellular ration capabilities for different hardware.",
        "start": 354.845,
        "duration": 4.715
    },
    {
        "text": ">> This is really cool because it feels like it's an end to",
        "start": 359.56,
        "duration": 3.31
    },
    {
        "text": "end thing for machine-learning models.",
        "start": 362.87,
        "duration": 3.06
    },
    {
        "text": "Are there many people using this already?",
        "start": 365.93,
        "duration": 3.24
    },
    {
        "text": ">> Yeah. ONNX Runtime has been",
        "start": 369.17,
        "duration": 2.985
    },
    {
        "text": "integrated into a variety of platforms.",
        "start": 372.155,
        "duration": 3.345
    },
    {
        "text": "Intimal platforms like our web ML, Azure ML,",
        "start": 375.5,
        "duration": 4.035
    },
    {
        "text": "and the some external platforms are like Oracle, navy towel.",
        "start": 379.535,
        "duration": 4.905
    },
    {
        "text": "Allow ONNX runtime runs on millions of devices and it's",
        "start": 384.44,
        "duration": 3.93
    },
    {
        "text": "processing billions of requests in our production everyday,",
        "start": 388.37,
        "duration": 5.215
    },
    {
        "text": "and then from production point of view,",
        "start": 393.585,
        "duration": 2.535
    },
    {
        "text": "ONNX Runtime has been now powering many flagship products like",
        "start": 396.12,
        "duration": 4.475
    },
    {
        "text": "Office collagen services across all kinds of machinery models,",
        "start": 400.595,
        "duration": 6.28
    },
    {
        "text": "including CM, RM and the transformers.",
        "start": 406.875,
        "duration": 3.405
    },
    {
        "text": "For model ship in our production,",
        "start": 410.28,
        "duration": 2.45
    },
    {
        "text": "we have observed with our two points,",
        "start": 412.73,
        "duration": 3.105
    },
    {
        "text": "my average performance improvement",
        "start": 415.835,
        "duration": 2.595
    },
    {
        "text": "compared to the previous solution.",
        "start": 418.43,
        "duration": 3.715
    },
    {
        "text": ">> This is cool. I've already known about ONNX for some time.",
        "start": 422.145,
        "duration": 3.955
    },
    {
        "text": "I'm sure there's people that have known about",
        "start": 426.1,
        "duration": 1.59
    },
    {
        "text": "ONNX and then ONNX Runtime.",
        "start": 427.69,
        "duration": 1.74
    },
    {
        "text": "What are some of the newest features",
        "start": 429.43,
        "duration": 1.89
    },
    {
        "text": "that you're most excited about?",
        "start": 431.32,
        "duration": 2.205
    },
    {
        "text": ">> Yeah, thanks for asking that.",
        "start": 433.525,
        "duration": 3.675
    },
    {
        "text": "Previously, we have been working with",
        "start": 437.2,
        "duration": 2.97
    },
    {
        "text": "first-party measuring for Cloud side inference with ONNX Runtime.",
        "start": 440.17,
        "duration": 5.52
    },
    {
        "text": "Nowadays, on device inference becomes appealing due to",
        "start": 445.69,
        "duration": 4.575
    },
    {
        "text": "growing awareness of their privacy and the data transfer cost.",
        "start": 450.265,
        "duration": 4.71
    },
    {
        "text": "We get more and more requirements for",
        "start": 454.975,
        "duration": 3.105
    },
    {
        "text": "extending existing scenarios of from Cloud to client.",
        "start": 458.08,
        "duration": 4.905
    },
    {
        "text": "But the limited memory and computer resource",
        "start": 462.985,
        "duration": 4.14
    },
    {
        "text": "makes this very challenge to run heavy models on client.",
        "start": 467.125,
        "duration": 4.725
    },
    {
        "text": "To solve this problem,",
        "start": 471.85,
        "duration": 2.16
    },
    {
        "text": "we're glad to enable INT8 quantization in ONNX Runtime for CPU.",
        "start": 474.01,
        "duration": 6.135
    },
    {
        "text": "On quantization approximates floating-point numbers with",
        "start": 480.145,
        "duration": 4.86
    },
    {
        "text": "lower bits with numbers",
        "start": 485.005,
        "duration": 3.585
    },
    {
        "text": "automatically reduce the model size memory for the print,",
        "start": 488.59,
        "duration": 4.455
    },
    {
        "text": "as well as accelerating and performance.",
        "start": 493.045,
        "duration": 3.48
    },
    {
        "text": "Quantization might introduce accuracy loss",
        "start": 496.525,
        "duration": 4.005
    },
    {
        "text": "because low bits limit the precision and the range of the values.",
        "start": 500.53,
        "duration": 8.67
    },
    {
        "text": "You're needed to make the trade off on your scenario.",
        "start": 509.2,
        "duration": 4.605
    },
    {
        "text": "ONNX Runtime use INT8 quantization.",
        "start": 513.805,
        "duration": 3.345
    },
    {
        "text": "Compared to FP32,",
        "start": 517.15,
        "duration": 2.34
    },
    {
        "text": "INT8 representations reduce storage by four times.",
        "start": 519.49,
        "duration": 6.45
    },
    {
        "text": "In terms of inference performance,",
        "start": 525.94,
        "duration": 3.375
    },
    {
        "text": "integer computation is more efficient than a floating point math.",
        "start": 529.315,
        "duration": 5.28
    },
    {
        "text": ">> This is really interesting. So basically,",
        "start": 534.595,
        "duration": 2.715
    },
    {
        "text": "what you're doing is you're snapping",
        "start": 537.31,
        "duration": 2.13
    },
    {
        "text": "floating-point numbers to INT8,",
        "start": 539.44,
        "duration": 2.31
    },
    {
        "text": "which is going to make it better for space and for computing.",
        "start": 541.75,
        "duration": 4.98
    },
    {
        "text": "But how much of a speedup do we get doing this?",
        "start": 546.73,
        "duration": 2.85
    },
    {
        "text": "Is it a lot better?",
        "start": 549.58,
        "duration": 1.495
    },
    {
        "text": ">> Yeah, much better.",
        "start": 551.075,
        "duration": 2.005
    },
    {
        "text": "We have a benchmark we can go through.",
        "start": 553.08,
        "duration": 4.005
    },
    {
        "text": "So we applied ONNX Runtime quantization",
        "start": 557.085,
        "duration": 3.015
    },
    {
        "text": "on hacking phase transformer models,",
        "start": 560.1,
        "duration": 2.865
    },
    {
        "text": "and observed a very promising result",
        "start": 562.965,
        "duration": 3.285
    },
    {
        "text": "on both performance and the model size.",
        "start": 566.25,
        "duration": 3.17
    },
    {
        "text": "Performance varies with hardware.",
        "start": 569.42,
        "duration": 3.23
    },
    {
        "text": "We benchmark performance for BERT base,",
        "start": 572.65,
        "duration": 3.54
    },
    {
        "text": "RoBERTa base and GPT-2 on two cans of CPU machines with",
        "start": 576.19,
        "duration": 6.24
    },
    {
        "text": "AVX2 instruction and AVX of",
        "start": 582.43,
        "duration": 3.885
    },
    {
        "text": "512 and we underline instructions are separately.",
        "start": 586.315,
        "duration": 5.85
    },
    {
        "text": "So the orange in the models are [inaudible] PyTorch,",
        "start": 592.165,
        "duration": 4.035
    },
    {
        "text": "we can model them to ONNX then apply ONNX Runtime quantization.",
        "start": 596.2,
        "duration": 4.56
    },
    {
        "text": "On the performance gain compared to the orange",
        "start": 600.76,
        "duration": 3.0
    },
    {
        "text": "in our model is pretty significant.",
        "start": 603.76,
        "duration": 3.375
    },
    {
        "text": "We saw that ONNX Runtime INT8 quantization can",
        "start": 607.135,
        "duration": 4.32
    },
    {
        "text": "accelerate inference performance by",
        "start": 611.455,
        "duration": 2.535
    },
    {
        "text": "about three times on a big machine.",
        "start": 613.99,
        "duration": 3.405
    },
    {
        "text": "On smaller balanced steel loadable performance improvements",
        "start": 617.395,
        "duration": 3.914
    },
    {
        "text": "on AVX2 machine, around 60 percent.",
        "start": 621.309,
        "duration": 4.066
    },
    {
        "text": "In terms of model size,",
        "start": 625.375,
        "duration": 2.25
    },
    {
        "text": "ONNX Runtime is able to",
        "start": 627.625,
        "duration": 1.62
    },
    {
        "text": "quantize more layers and",
        "start": 629.245,
        "duration": 4.845
    },
    {
        "text": "reduce the model size by almost four times.",
        "start": 634.09,
        "duration": 3.465
    },
    {
        "text": "The final model is",
        "start": 637.555,
        "duration": 2.475
    },
    {
        "text": "about half as much as the quantize the PyTorch model.",
        "start": 640.03,
        "duration": 4.89
    },
    {
        "text": "Accuracy is also important in the evaluation.",
        "start": 644.92,
        "duration": 4.365
    },
    {
        "text": "Smaller and faster is great but we also need",
        "start": 649.285,
        "duration": 3.735
    },
    {
        "text": "to make sure the model is returning good result.",
        "start": 653.02,
        "duration": 4.005
    },
    {
        "text": "Given the accuracy is Pascal specific.",
        "start": 657.025,
        "duration": 3.495
    },
    {
        "text": "We took advantage on the BERT model,",
        "start": 660.52,
        "duration": 2.67
    },
    {
        "text": "for MPRC task and for accuracy benchmark.",
        "start": 663.19,
        "duration": 4.605
    },
    {
        "text": "MPRC task is a common NLP task",
        "start": 667.795,
        "duration": 3.99
    },
    {
        "text": "for non-bridging on parallel classification.",
        "start": 671.785,
        "duration": 4.525
    },
    {
        "text": "This model is fine tuned using bolt",
        "start": 676.38,
        "duration": 4.18
    },
    {
        "text": "based model in a Hugging Face transformer for MPRC tasks.",
        "start": 680.56,
        "duration": 6.225
    },
    {
        "text": "Compared to PyTorch quantization,",
        "start": 686.785,
        "duration": 2.925
    },
    {
        "text": "or even with a small model ONNX Runtime quantization",
        "start": 689.71,
        "duration": 4.425
    },
    {
        "text": "surely they seem accuracy and have a slightly higher F1 score.",
        "start": 694.135,
        "duration": 5.655
    },
    {
        "text": "In this task, the INT8 model can",
        "start": 699.79,
        "duration": 3.15
    },
    {
        "text": "preserve the similar accuracy as FP32 models.",
        "start": 702.94,
        "duration": 5.115
    },
    {
        "text": ">> This is really cool because quantization",
        "start": 708.055,
        "duration": 2.805
    },
    {
        "text": "apparently makes the model much",
        "start": 710.86,
        "duration": 2.25
    },
    {
        "text": "smaller but we still maintain good accuracy.",
        "start": 713.11,
        "duration": 5.52
    },
    {
        "text": "Overall, it's cool. I would love to see how this",
        "start": 718.63,
        "duration": 2.64
    },
    {
        "text": "actually works in practice.",
        "start": 721.27,
        "duration": 2.34
    },
    {
        "text": "Do you have something you can show us to get",
        "start": 723.61,
        "duration": 1.77
    },
    {
        "text": "a sense for how we would implement something like this?",
        "start": 725.38,
        "duration": 2.185
    },
    {
        "text": ">> Yeah. So let me take this notebook to",
        "start": 727.565,
        "duration": 3.94
    },
    {
        "text": "demonstrate the end to end workflow",
        "start": 731.505,
        "duration": 2.61
    },
    {
        "text": "of ONNX Runtime INT8 quantization.",
        "start": 734.115,
        "duration": 3.255
    },
    {
        "text": "You can use PyTorch,",
        "start": 737.37,
        "duration": 2.805
    },
    {
        "text": "TensorFlow to turn our model with FP32 format.",
        "start": 740.175,
        "duration": 5.235
    },
    {
        "text": "Then convert it to ONNX with our converter tools.",
        "start": 745.41,
        "duration": 4.805
    },
    {
        "text": "The converted model is STR FP32 format.",
        "start": 750.215,
        "duration": 5.029
    },
    {
        "text": "To get INT8 model,",
        "start": 755.244,
        "duration": 2.956
    },
    {
        "text": "we provide a quantization tool to",
        "start": 758.2,
        "duration": 3.57
    },
    {
        "text": "quantize ONNX model from FP32 to INT8.",
        "start": 761.77,
        "duration": 4.605
    },
    {
        "text": "Once you get a quantized model,",
        "start": 766.375,
        "duration": 2.61
    },
    {
        "text": "you can inference it in",
        "start": 768.985,
        "duration": 2.04
    },
    {
        "text": "ONNX Runtime the same way you would normally take.",
        "start": 771.025,
        "duration": 3.75
    },
    {
        "text": "So according to a model characteristics,",
        "start": 774.775,
        "duration": 3.855
    },
    {
        "text": "there are three modes enabled in",
        "start": 778.63,
        "duration": 2.43
    },
    {
        "text": "a quantization tool: dynamic quantization,",
        "start": 781.06,
        "duration": 3.18
    },
    {
        "text": "standard of quantization and quantization while training.",
        "start": 784.24,
        "duration": 4.605
    },
    {
        "text": "This notebook shows how to quantize Hugging Face,",
        "start": 788.845,
        "duration": 4.425
    },
    {
        "text": "both model trend with PyTorch using",
        "start": 793.27,
        "duration": 3.0
    },
    {
        "text": "dynamic quantization to reduce",
        "start": 796.27,
        "duration": 2.37
    },
    {
        "text": "the model size and speed up inference.",
        "start": 798.64,
        "duration": 3.45
    },
    {
        "text": "To save some time,",
        "start": 802.09,
        "duration": 1.71
    },
    {
        "text": "I've already run the notebook ahead of time.",
        "start": 803.8,
        "duration": 4.0
    },
    {
        "text": "Firstly, we need to set up environment",
        "start": 808.74,
        "duration": 3.76
    },
    {
        "text": "by installing all the necessary packages,",
        "start": 812.5,
        "duration": 3.434
    },
    {
        "text": "including PyTorch, ONNX Runtime and the Hugging Face transformer.",
        "start": 815.934,
        "duration": 6.056
    },
    {
        "text": "Then we can fine-tune Hugging Face BERT model",
        "start": 824.19,
        "duration": 4.105
    },
    {
        "text": "for MPRC task or with PyTorch,",
        "start": 828.295,
        "duration": 3.419
    },
    {
        "text": "by using GLUE data set.",
        "start": 831.714,
        "duration": 3.416
    },
    {
        "text": "So you can download the data set and run the fine tunes deck.",
        "start": 836.97,
        "duration": 6.76
    },
    {
        "text": "PyTorch also published this model of Directory usage.",
        "start": 843.73,
        "duration": 7.715
    },
    {
        "text": "Here we can download the model directory.",
        "start": 851.445,
        "duration": 5.765
    },
    {
        "text": "In order to get baseline,",
        "start": 858.45,
        "duration": 3.43
    },
    {
        "text": "we are wrong on the model or with PyTorch quantization firstly.",
        "start": 861.88,
        "duration": 6.25
    },
    {
        "text": "Firstly, we set some global configurations",
        "start": 868.77,
        "duration": 4.895
    },
    {
        "text": "and then we apply on PyTorch quantization.",
        "start": 873.665,
        "duration": 5.305
    },
    {
        "text": "Here is the model size of quantized PyTorch model.",
        "start": 879.61,
        "duration": 5.81
    },
    {
        "text": "In terms of performance evaluation,",
        "start": 887.85,
        "duration": 3.43
    },
    {
        "text": "we use the Tokenize and evaluation function from Hugging Face.",
        "start": 891.28,
        "duration": 6.25
    },
    {
        "text": "Here is performance and",
        "start": 903.0,
        "duration": 2.56
    },
    {
        "text": "the accuracy results with PyTorch quantized model.",
        "start": 905.56,
        "duration": 5.53
    },
    {
        "text": ">> That's cool. This is a PyTorch quantize model.",
        "start": 913.67,
        "duration": 6.955
    },
    {
        "text": "We saw it go from 400 down to 170 something,",
        "start": 920.625,
        "duration": 3.375
    },
    {
        "text": "but we haven't done the ONNX",
        "start": 924.0,
        "duration": 1.83
    },
    {
        "text": "Runtime quantization model is that right?",
        "start": 925.83,
        "duration": 2.055
    },
    {
        "text": ">> Yes.",
        "start": 927.885,
        "duration": 0.345
    },
    {
        "text": ">> That's okay, let's take a look at that.",
        "start": 928.23,
        "duration": 2.61
    },
    {
        "text": ">> Yeah, so let's go to the key part of ONNX Runtime quantization.",
        "start": 930.84,
        "duration": 5.61
    },
    {
        "text": "As shown in this diagram,",
        "start": 936.45,
        "duration": 2.55
    },
    {
        "text": "there are three steps on [inaudible] ,",
        "start": 939.0,
        "duration": 2.49
    },
    {
        "text": "the orange model to",
        "start": 941.49,
        "duration": 1.74
    },
    {
        "text": "ONNX FP32 model on quantize eight and with our quantization tool,",
        "start": 943.23,
        "duration": 7.125
    },
    {
        "text": "then inference into aid modal with ONNX runtime.",
        "start": 950.355,
        "duration": 3.885
    },
    {
        "text": "Since PyTorch support ONNX format latently,",
        "start": 954.24,
        "duration": 4.035
    },
    {
        "text": "it provides a built in export API.",
        "start": 958.275,
        "duration": 3.764
    },
    {
        "text": "It shows here for exporting PyTorch model to ONNX format.",
        "start": 962.039,
        "duration": 6.376
    },
    {
        "text": "Together the optimal performance with ONNX runtime for",
        "start": 968.415,
        "duration": 3.99
    },
    {
        "text": "FP32 model that is on offline transformer optimization tool.",
        "start": 972.405,
        "duration": 5.73
    },
    {
        "text": "You can now reach to get or further optimize the bottom model.",
        "start": 978.135,
        "duration": 4.65
    },
    {
        "text": "Here is the code how to use our optimization tool.",
        "start": 982.785,
        "duration": 5.865
    },
    {
        "text": ">> That optimizer is doing the quantization for us to [inaudible] .",
        "start": 988.65,
        "duration": 4.305
    },
    {
        "text": ">> No, we haven't go to the quantization step yet.",
        "start": 992.955,
        "duration": 4.995
    },
    {
        "text": "Yeah, this process is still based on FP32 from our datatype.",
        "start": 997.95,
        "duration": 6.15
    },
    {
        "text": ">> I see, so there is an optimization step before we even do",
        "start": 1004.1,
        "duration": 3.48
    },
    {
        "text": "the quantization step that is",
        "start": 1007.58,
        "duration": 1.23
    },
    {
        "text": "provided in the ONNX Runtime. Is that right?",
        "start": 1008.81,
        "duration": 2.415
    },
    {
        "text": ">> Yeah, but I will point out on this step,",
        "start": 1011.225,
        "duration": 3.03
    },
    {
        "text": "that is only for transformer models.",
        "start": 1014.255,
        "duration": 2.475
    },
    {
        "text": ">> Got it.",
        "start": 1016.73,
        "duration": 0.33
    },
    {
        "text": ">> Other types of models,",
        "start": 1017.06,
        "duration": 3.48
    },
    {
        "text": "you don't need to run this step.",
        "start": 1020.54,
        "duration": 2.685
    },
    {
        "text": "After this step, we can apply",
        "start": 1023.225,
        "duration": 3.27
    },
    {
        "text": "quantization for these for the optimized model.",
        "start": 1026.495,
        "duration": 4.785
    },
    {
        "text": "Here you can call on",
        "start": 1031.28,
        "duration": 3.21
    },
    {
        "text": "its runtime quantization API to",
        "start": 1034.49,
        "duration": 3.24
    },
    {
        "text": "quantize sine model into eight format.",
        "start": 1037.73,
        "duration": 4.98
    },
    {
        "text": "Here you can enter the model size after ONNX runtime quantization.",
        "start": 1042.71,
        "duration": 6.525
    },
    {
        "text": ">> Yeah, and that's like a quarter of the size.",
        "start": 1049.235,
        "duration": 2.7
    },
    {
        "text": "That it was originally, which is pretty cool.",
        "start": 1051.935,
        "duration": 2.22
    },
    {
        "text": ">> Yeah. Okay. Then let's",
        "start": 1054.155,
        "duration": 3.755
    },
    {
        "text": "evaluate the performance and accuracy with",
        "start": 1057.91,
        "duration": 2.37
    },
    {
        "text": "ONNX Runtime using the same way we",
        "start": 1060.28,
        "duration": 2.58
    },
    {
        "text": "normally inference ONNX FP32 model.",
        "start": 1062.86,
        "duration": 4.095
    },
    {
        "text": "You can create inference session always there is session options.",
        "start": 1066.955,
        "duration": 7.645
    },
    {
        "text": "Then you call our session [inaudible] onto score this model.",
        "start": 1075.55,
        "duration": 7.37
    },
    {
        "text": "Here is accuracy and the performance result.",
        "start": 1086.83,
        "duration": 4.06
    },
    {
        "text": "Always ONNX Runtime quantization.",
        "start": 1090.89,
        "duration": 3.4
    },
    {
        "text": ">> Yeah, it's roughly the same for both, which is really cool.",
        "start": 1094.87,
        "duration": 5.02
    },
    {
        "text": ">> Yeah, so actually we can compel on",
        "start": 1099.89,
        "duration": 3.69
    },
    {
        "text": "PyTorch result always ONNX Runtime result.",
        "start": 1103.58,
        "duration": 3.795
    },
    {
        "text": "You can see for model signs,",
        "start": 1107.375,
        "duration": 3.315
    },
    {
        "text": "here we can get better results.",
        "start": 1110.69,
        "duration": 3.735
    },
    {
        "text": "ONNX Runtime can achieve with even smaller model.",
        "start": 1114.425,
        "duration": 5.155
    },
    {
        "text": "For accuracy of ONNX Runtime quantization",
        "start": 1119.95,
        "duration": 6.355
    },
    {
        "text": "is slightly better than on PyTorch quantization on F1 score.",
        "start": 1126.305,
        "duration": 6.885
    },
    {
        "text": ">> That's cool.",
        "start": 1133.19,
        "duration": 1.77
    },
    {
        "text": ">> Yeah.",
        "start": 1134.96,
        "duration": 1.545
    },
    {
        "text": ">> Sorry. Overall we were able to get",
        "start": 1136.505,
        "duration": 3.015
    },
    {
        "text": "a better size model with",
        "start": 1139.52,
        "duration": 2.91
    },
    {
        "text": "the quantization from the ONNX Runtime than with the PyTorch one.",
        "start": 1142.43,
        "duration": 4.02
    },
    {
        "text": "It seems like in this case,",
        "start": 1146.45,
        "duration": 2.025
    },
    {
        "text": "the F1 score was even better. That's cool.",
        "start": 1148.475,
        "duration": 5.565
    },
    {
        "text": "Here's another question as we move along.",
        "start": 1154.04,
        "duration": 2.535
    },
    {
        "text": "Are there any other new features",
        "start": 1156.575,
        "duration": 1.845
    },
    {
        "text": "that you're actually really excited",
        "start": 1158.42,
        "duration": 1.68
    },
    {
        "text": "about in the ONNX Runtime that",
        "start": 1160.1,
        "duration": 2.31
    },
    {
        "text": "you'd like to share before we call it a show?",
        "start": 1162.41,
        "duration": 3.21
    },
    {
        "text": ">> Yeah, there's another exciting feature I would like to share.",
        "start": 1165.62,
        "duration": 6.675
    },
    {
        "text": "As we mentioned, there is a strict constraint of",
        "start": 1172.295,
        "duration": 4.725
    },
    {
        "text": "these core footprint and",
        "start": 1177.02,
        "duration": 1.65
    },
    {
        "text": "the inner memory size for on-device inference,",
        "start": 1178.67,
        "duration": 3.96
    },
    {
        "text": "especially on smartphone and Edge devices.",
        "start": 1182.63,
        "duration": 4.41
    },
    {
        "text": "Or with quantization, we can reduce the model size.",
        "start": 1187.04,
        "duration": 3.915
    },
    {
        "text": "Either way we can reduce the Runtime size as well.",
        "start": 1190.955,
        "duration": 4.56
    },
    {
        "text": "ONNX Runtime model is a new feature on minimizing",
        "start": 1195.515,
        "duration": 4.725
    },
    {
        "text": "the Runtime size for usage in",
        "start": 1200.24,
        "duration": 2.43
    },
    {
        "text": "a mobile and embedding the scenarios.",
        "start": 1202.67,
        "duration": 2.715
    },
    {
        "text": "On this capability extends ONNX Runtime to support",
        "start": 1205.385,
        "duration": 4.26
    },
    {
        "text": "optimize our model inference from Cloud to client.",
        "start": 1209.645,
        "duration": 4.76
    },
    {
        "text": "Now this feature is available in",
        "start": 1214.405,
        "duration": 2.91
    },
    {
        "text": "preview with ONNX Runtime release 1.5.",
        "start": 1217.315,
        "duration": 3.505
    },
    {
        "text": "There are two major techniques enabled for ONNX Runtime mobile.",
        "start": 1220.82,
        "duration": 4.635
    },
    {
        "text": "One is introducing new optimized format",
        "start": 1225.455,
        "duration": 3.915
    },
    {
        "text": "we call ONNX Runtime format,",
        "start": 1229.37,
        "duration": 1.83
    },
    {
        "text": "which support our model inference with",
        "start": 1231.2,
        "duration": 2.46
    },
    {
        "text": "ONNX Runtime with less dependencies.",
        "start": 1233.66,
        "duration": 2.97
    },
    {
        "text": "The other one is building ONNX Runtime with",
        "start": 1236.63,
        "duration": 3.54
    },
    {
        "text": "operators only needed by predefined models,",
        "start": 1240.17,
        "duration": 4.035
    },
    {
        "text": "so that at the Runtime size can be further reduced",
        "start": 1244.205,
        "duration": 3.795
    },
    {
        "text": "by getting rid off those unused operators for your scenario.",
        "start": 1248.0,
        "duration": 4.935
    },
    {
        "text": ">> This is really cool because not only are you trying to",
        "start": 1252.935,
        "duration": 2.685
    },
    {
        "text": "reduce the size of the models,",
        "start": 1255.62,
        "duration": 3.105
    },
    {
        "text": "but you're also reducing the size of the actual Runtime.",
        "start": 1258.725,
        "duration": 3.915
    },
    {
        "text": "Because you're only going to export the operations that you need.",
        "start": 1262.64,
        "duration": 3.15
    },
    {
        "text": "For example, if you're doing like only [inaudible] ,",
        "start": 1265.79,
        "duration": 1.77
    },
    {
        "text": "you don't need to do a softmax,",
        "start": 1267.56,
        "duration": 1.394
    },
    {
        "text": "that's actually really cool, because now, you have less",
        "start": 1268.954,
        "duration": 2.221
    },
    {
        "text": "of a Runtime footprint, which is really cool.",
        "start": 1271.175,
        "duration": 2.415
    },
    {
        "text": "How much does this actually help with speed, etc?",
        "start": 1273.59,
        "duration": 5.88
    },
    {
        "text": ">> The speed would not to be impact on.",
        "start": 1279.47,
        "duration": 2.265
    },
    {
        "text": ">> I mean size because it's the size matter, sorry.",
        "start": 1281.735,
        "duration": 3.345
    },
    {
        "text": "Because obviously, it's going to do",
        "start": 1285.08,
        "duration": 1.95
    },
    {
        "text": "the multiplications and do the functions,",
        "start": 1287.03,
        "duration": 1.71
    },
    {
        "text": "but you want to squeeze",
        "start": 1288.74,
        "duration": 1.575
    },
    {
        "text": "as much as you can in a little bit of space.",
        "start": 1290.315,
        "duration": 2.295
    },
    {
        "text": "How much of an improvement are we going to",
        "start": 1292.61,
        "duration": 1.62
    },
    {
        "text": "see with space. Sorry about that.",
        "start": 1294.23,
        "duration": 2.01
    },
    {
        "text": ">> Yeah. Here is the benchmark of",
        "start": 1296.24,
        "duration": 3.27
    },
    {
        "text": "the Runtime size with ONNX Runtime mobile.",
        "start": 1299.51,
        "duration": 4.63
    },
    {
        "text": "The size of the Runtime package varies from",
        "start": 1306.04,
        "duration": 3.88
    },
    {
        "text": "the number of models you want to support.",
        "start": 1309.92,
        "duration": 3.97
    },
    {
        "text": "For different platforms, here is the result.",
        "start": 1314.32,
        "duration": 7.63
    },
    {
        "text": "You can see they compress the size",
        "start": 1321.95,
        "duration": 3.735
    },
    {
        "text": "of core ONNX Runtime mobile package",
        "start": 1325.685,
        "duration": 3.18
    },
    {
        "text": "can achieve a less than 300 kilobyte",
        "start": 1328.865,
        "duration": 3.78
    },
    {
        "text": "on both Android and iOS by default.",
        "start": 1332.645,
        "duration": 3.285
    },
    {
        "text": "Which a makes own device inference,",
        "start": 1335.93,
        "duration": 2.58
    },
    {
        "text": "more feasible to meet a straight",
        "start": 1338.51,
        "duration": 2.175
    },
    {
        "text": "to a memory and the [inaudible] .",
        "start": 1340.685,
        "duration": 2.7
    },
    {
        "text": ">> Did you say, 300 kilobytes?",
        "start": 1343.385,
        "duration": 2.475
    },
    {
        "text": ">> Yeah.",
        "start": 1345.86,
        "duration": 1.155
    },
    {
        "text": ">> That's like the size of a picture.",
        "start": 1347.015,
        "duration": 1.635
    },
    {
        "text": "I mean that's very impressive, that's really cool.",
        "start": 1348.65,
        "duration": 4.05
    },
    {
        "text": "So obviously, we spent a lot of time",
        "start": 1352.7,
        "duration": 2.04
    },
    {
        "text": "talking about the ONNX and the ONNX Runtime.",
        "start": 1354.74,
        "duration": 2.489
    },
    {
        "text": "I know we're running short on time.",
        "start": 1357.229,
        "duration": 1.771
    },
    {
        "text": "Where can people go to find out more and maybe try this out?",
        "start": 1359.0,
        "duration": 3.825
    },
    {
        "text": ">> ONNX Runtime is open source project.",
        "start": 1362.825,
        "duration": 3.09
    },
    {
        "text": "All the tutorials and examples are available in the GitHub repo.",
        "start": 1365.915,
        "duration": 6.525
    },
    {
        "text": "Feel free to take a look and have a try.",
        "start": 1372.44,
        "duration": 3.345
    },
    {
        "text": ">> Awesome. Well, thank you so much, Emma, for spending",
        "start": 1375.785,
        "duration": 2.085
    },
    {
        "text": "some time with us and thank you so much for watching.",
        "start": 1377.87,
        "duration": 2.07
    },
    {
        "text": "We were learning all about ONNX and the",
        "start": 1379.94,
        "duration": 1.89
    },
    {
        "text": "ONNX Runtime, two amazing features.",
        "start": 1381.83,
        "duration": 2.28
    },
    {
        "text": "Thanks so much for watching and we'll see you next time.",
        "start": 1384.11,
        "duration": 1.92
    },
    {
        "text": "Take care.",
        "start": 1386.03,
        "duration": 0.33
    },
    {
        "text": "[MUSIC]",
        "start": 1386.36,
        "duration": 14.64
    }
]