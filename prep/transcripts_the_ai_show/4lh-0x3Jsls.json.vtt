[
    {
        "text": ">> In this special build edition of the AI Show,",
        "start": 0.29,
        "duration": 3.4
    },
    {
        "text": "we'll get to hear from MSR Researcher, Antoine Delignat-Lavaud.",
        "start": 3.69,
        "duration": 3.6
    },
    {
        "text": "Antoine will walk us through how",
        "start": 7.29,
        "duration": 1.41
    },
    {
        "text": "confidential ML can be used securely",
        "start": 8.7,
        "duration": 2.37
    },
    {
        "text": "and confidentially train and deploy machine learning models.",
        "start": 11.07,
        "duration": 4.11
    },
    {
        "text": "It's amazing. Make sure you take a look.",
        "start": 15.18,
        "duration": 2.13
    },
    {
        "text": "[MUSIC]",
        "start": 17.31,
        "duration": 9.39
    },
    {
        "text": ">> Hi. Welcome to this Confidential Machine Learning deep dive.",
        "start": 26.7,
        "duration": 3.255
    },
    {
        "text": "My name is Antoine Delignat-Lavaud.",
        "start": 29.955,
        "duration": 1.8
    },
    {
        "text": "I work as a Security Researcher in the Convolute UK lab.",
        "start": 31.755,
        "duration": 3.295
    },
    {
        "text": "First of all, what exactly is Confidential Machine Learning?",
        "start": 35.05,
        "duration": 3.33
    },
    {
        "text": "To put it simply,",
        "start": 38.38,
        "duration": 1.165
    },
    {
        "text": "Confidential Machine Learning is the application of",
        "start": 39.545,
        "duration": 2.025
    },
    {
        "text": "Confidential Computing technologies to Machine Learning Services.",
        "start": 41.57,
        "duration": 3.69
    },
    {
        "text": "Our goal is to enable the training and deployment of",
        "start": 45.26,
        "duration": 2.79
    },
    {
        "text": "machine learning models whose end-to-end data confidentiality,",
        "start": 48.05,
        "duration": 3.285
    },
    {
        "text": "based on the hardware root of trust.",
        "start": 51.335,
        "duration": 2.01
    },
    {
        "text": "This provides the highest level of",
        "start": 53.345,
        "duration": 1.725
    },
    {
        "text": "Cloud data processing security available in the industry today.",
        "start": 55.07,
        "duration": 3.54
    },
    {
        "text": "Current machine learning services on",
        "start": 58.61,
        "duration": 1.71
    },
    {
        "text": "Azure and also Cloud providers,",
        "start": 60.32,
        "duration": 2.1
    },
    {
        "text": "protect the confidentiality of data at rest and in transit.",
        "start": 62.42,
        "duration": 3.5
    },
    {
        "text": "Which Confidential Computing, data remains encrypted even",
        "start": 65.92,
        "duration": 3.43
    },
    {
        "text": "during processing, when it's loaded into RAM, and on the CPU.",
        "start": 69.35,
        "duration": 3.48
    },
    {
        "text": "So many cryptographic secrets are embedded into",
        "start": 72.83,
        "duration": 2.55
    },
    {
        "text": "the silicon and cannot be extracted without destroying the chip.",
        "start": 75.38,
        "duration": 3.795
    },
    {
        "text": "There are two main benefits to",
        "start": 79.175,
        "duration": 1.545
    },
    {
        "text": "Confidential Computing in the context of machine learning.",
        "start": 80.72,
        "duration": 3.0
    },
    {
        "text": "The first is to limit the trust into the platform and",
        "start": 83.72,
        "duration": 2.61
    },
    {
        "text": "services when running machine learning tasks on Azure.",
        "start": 86.33,
        "duration": 3.21
    },
    {
        "text": "Even though Azure were first some of",
        "start": 89.54,
        "duration": 1.68
    },
    {
        "text": "the most protective privacy policies for customer data,",
        "start": 91.22,
        "duration": 3.0
    },
    {
        "text": "customers must still trust",
        "start": 94.22,
        "duration": 1.74
    },
    {
        "text": "Microsoft to properly enforce these policies.",
        "start": 95.96,
        "duration": 2.715
    },
    {
        "text": "With Confidential Computing, the Trusted Computing Base of",
        "start": 98.675,
        "duration": 3.255
    },
    {
        "text": "our services becomes much smaller and precisely attestable,",
        "start": 101.93,
        "duration": 3.69
    },
    {
        "text": "which turns data confidentiality into",
        "start": 105.62,
        "duration": 1.74
    },
    {
        "text": "a technical, verifiable guarantee.",
        "start": 107.36,
        "duration": 2.52
    },
    {
        "text": "The second main benefit is to enable",
        "start": 109.88,
        "duration": 2.52
    },
    {
        "text": "new multi-party data processing scenarios where",
        "start": 112.4,
        "duration": 2.58
    },
    {
        "text": "participants want to run some analysis over the pooled data,",
        "start": 114.98,
        "duration": 3.555
    },
    {
        "text": "but they don't trust each other to share their datasets.",
        "start": 118.535,
        "duration": 2.96
    },
    {
        "text": "Such scenarios are particularly interesting for industries, such as",
        "start": 121.495,
        "duration": 4.18
    },
    {
        "text": "Healthcare and Financial Services",
        "start": 125.675,
        "duration": 2.205
    },
    {
        "text": "where data privacy is heavily regulated.",
        "start": 127.88,
        "duration": 2.91
    },
    {
        "text": "Let's take a closer look at the precise trust assumptions,",
        "start": 130.79,
        "duration": 3.135
    },
    {
        "text": "looking at both the software TCB and the trusted users.",
        "start": 133.925,
        "duration": 3.23
    },
    {
        "text": "In the case of machine learning,",
        "start": 137.155,
        "duration": 1.705
    },
    {
        "text": "the application typically consists of a framework such as PyTorch",
        "start": 138.86,
        "duration": 3.87
    },
    {
        "text": "or TensorFlow with many dependencies",
        "start": 142.73,
        "duration": 2.309
    },
    {
        "text": "and third-party Python packages.",
        "start": 145.039,
        "duration": 2.001
    },
    {
        "text": "Privilege users, such as data scientists or IT employees,",
        "start": 147.04,
        "duration": 4.24
    },
    {
        "text": "managing the virtual machines may have access to the data.",
        "start": 151.28,
        "duration": 3.24
    },
    {
        "text": "At the infrastructure level,",
        "start": 154.52,
        "duration": 1.83
    },
    {
        "text": "Microsoft manages the Host operating system and hypervisor",
        "start": 156.35,
        "duration": 3.3
    },
    {
        "text": "of the container deployment platform",
        "start": 159.65,
        "duration": 2.13
    },
    {
        "text": "such as Kubernetes or the Service Fabric.",
        "start": 161.78,
        "duration": 2.355
    },
    {
        "text": "This Cloud is trusted to enforce isolation between tenants.",
        "start": 164.135,
        "duration": 3.525
    },
    {
        "text": "The hardware must be trusted to unfold",
        "start": 167.66,
        "duration": 2.1
    },
    {
        "text": "the technical isolation mechanisms such as privilege reverse.",
        "start": 169.76,
        "duration": 3.825
    },
    {
        "text": "Azure must also be trusted to",
        "start": 173.585,
        "duration": 1.815
    },
    {
        "text": "protect physical access to the hardware.",
        "start": 175.4,
        "duration": 2.24
    },
    {
        "text": "With Confidential Computing,",
        "start": 177.64,
        "duration": 1.955
    },
    {
        "text": "the main ID is to isolate the parts of the application that are",
        "start": 179.595,
        "duration": 2.915
    },
    {
        "text": "critical to data confidentiality into",
        "start": 182.51,
        "duration": 2.58
    },
    {
        "text": "a private region called",
        "start": 185.09,
        "duration": 1.2
    },
    {
        "text": "the Trusted Execution Environment or enclave,",
        "start": 186.29,
        "duration": 2.37
    },
    {
        "text": "that is isolated from the rest of the software stack,",
        "start": 188.66,
        "duration": 2.835
    },
    {
        "text": "including the mock privilege operating system and hypervisor.",
        "start": 191.495,
        "duration": 3.465
    },
    {
        "text": "The isolation is enforced by the CPU with a new set of",
        "start": 194.96,
        "duration": 3.45
    },
    {
        "text": "instructions to create, enter and exit",
        "start": 198.41,
        "duration": 2.73
    },
    {
        "text": "enclaves, and communicate with the rest of the system.",
        "start": 201.14,
        "duration": 3.14
    },
    {
        "text": "In addition to the isolation,",
        "start": 204.28,
        "duration": 2.035
    },
    {
        "text": "a critical feature of TEEs is the ability to create and sign",
        "start": 206.315,
        "duration": 3.87
    },
    {
        "text": "measurements of the enclaves code",
        "start": 210.185,
        "duration": 1.905
    },
    {
        "text": "in a process called remote attestation.",
        "start": 212.09,
        "duration": 2.565
    },
    {
        "text": "This feature allows users a Confidential Computing services to",
        "start": 214.655,
        "duration": 3.465
    },
    {
        "text": "validate the exact binaries authorized to process the data.",
        "start": 218.12,
        "duration": 3.735
    },
    {
        "text": "In some cases, the measurement can be mapped to",
        "start": 221.855,
        "duration": 2.985
    },
    {
        "text": "codes if the software is built on opensource.",
        "start": 224.84,
        "duration": 2.955
    },
    {
        "text": "Or it can be signed by an auditing authority trusted by the user.",
        "start": 227.795,
        "duration": 3.735
    },
    {
        "text": "As a concrete illustration,",
        "start": 231.53,
        "duration": 1.89
    },
    {
        "text": "consider a consortium of banks trying to train",
        "start": 233.42,
        "duration": 2.07
    },
    {
        "text": "a classifier for fraudulent transactions.",
        "start": 235.49,
        "duration": 2.385
    },
    {
        "text": "Because each bank only see one side of every transaction,",
        "start": 237.875,
        "duration": 3.675
    },
    {
        "text": "it is possible to get a much more accurate model",
        "start": 241.55,
        "duration": 2.505
    },
    {
        "text": "if the banks can pull the data.",
        "start": 244.055,
        "duration": 1.955
    },
    {
        "text": "However, this may not be practical",
        "start": 246.01,
        "duration": 1.96
    },
    {
        "text": "both for regulatory and business reasons,",
        "start": 247.97,
        "duration": 2.37
    },
    {
        "text": "as sharing the data violate the privacy of",
        "start": 250.34,
        "duration": 2.1
    },
    {
        "text": "customers and reveal sensitive information to competitors.",
        "start": 252.44,
        "duration": 3.71
    },
    {
        "text": "With Confidential Computing, the banks connected",
        "start": 256.15,
        "duration": 2.89
    },
    {
        "text": "together on the code that will run into TEE to times a modern.",
        "start": 259.04,
        "duration": 3.485
    },
    {
        "text": "Then they can encrypt the data with",
        "start": 262.525,
        "duration": 1.795
    },
    {
        "text": "a key only known to see enclave,",
        "start": 264.32,
        "duration": 1.905
    },
    {
        "text": "after taking the distinction evidence",
        "start": 266.225,
        "duration": 2.055
    },
    {
        "text": "proves the key was created by the accurate code.",
        "start": 268.28,
        "duration": 3.16
    },
    {
        "text": "So banks may decide that the model is shared after training,",
        "start": 271.44,
        "duration": 3.36
    },
    {
        "text": "or that it is never allowed to leave",
        "start": 274.8,
        "duration": 1.67
    },
    {
        "text": "the trusted enclave and can only be queried.",
        "start": 276.47,
        "duration": 2.37
    },
    {
        "text": "In the rest of the video,",
        "start": 278.84,
        "duration": 1.635
    },
    {
        "text": "I am going to show how to train and query your model to classify",
        "start": 280.475,
        "duration": 3.405
    },
    {
        "text": "chest X-ray pictures as",
        "start": 283.88,
        "duration": 1.86
    },
    {
        "text": "normal consistent with pneumonia or COVID-19.",
        "start": 285.74,
        "duration": 3.755
    },
    {
        "text": "Please note that this demo is meant to illustrate",
        "start": 289.495,
        "duration": 3.025
    },
    {
        "text": "the use-cases that Confidential Machine Learning conserve.",
        "start": 292.52,
        "duration": 3.15
    },
    {
        "text": "We do not claim that the model reproduces",
        "start": 295.67,
        "duration": 2.16
    },
    {
        "text": "can be used for medical diagnoses.",
        "start": 297.83,
        "duration": 2.415
    },
    {
        "text": "We use two public datasets.",
        "start": 300.245,
        "duration": 2.34
    },
    {
        "text": "The first is a case database of",
        "start": 302.585,
        "duration": 2.085
    },
    {
        "text": "COVID-19 patients published by",
        "start": 304.67,
        "duration": 2.01
    },
    {
        "text": "the Italian Society of Medical Radiology.",
        "start": 306.68,
        "duration": 2.8
    },
    {
        "text": "The other is a Kaggle datasets for",
        "start": 309.48,
        "duration": 2.37
    },
    {
        "text": "pneumonia detection published by",
        "start": 311.85,
        "duration": 2.16
    },
    {
        "text": "the Radiological Society of North America.",
        "start": 314.01,
        "duration": 2.775
    },
    {
        "text": "Those datasets have been collected and",
        "start": 316.785,
        "duration": 2.315
    },
    {
        "text": "published with informed patient consents.",
        "start": 319.1,
        "duration": 2.505
    },
    {
        "text": "We will need to perform some pre-processing operations to",
        "start": 321.605,
        "duration": 3.135
    },
    {
        "text": "convert the codes to a common format that we can aggregate.",
        "start": 324.74,
        "duration": 3.23
    },
    {
        "text": "Then we use existing datasets to train with TensorFlow,",
        "start": 327.97,
        "duration": 4.06
    },
    {
        "text": "a Deep residual network or ResNet,",
        "start": 332.03,
        "duration": 2.715
    },
    {
        "text": "which is a popular class of models for image classification tasks.",
        "start": 334.745,
        "duration": 4.035
    },
    {
        "text": "Before diving into the Confidential Computing specific questions,",
        "start": 338.78,
        "duration": 3.945
    },
    {
        "text": "let's look at how these scenario can",
        "start": 342.725,
        "duration": 1.815
    },
    {
        "text": "be expressed on Azure ML today.",
        "start": 344.54,
        "duration": 2.145
    },
    {
        "text": "The concept that we use to represent the sequence of",
        "start": 346.685,
        "duration": 2.685
    },
    {
        "text": "data processing steps is known as a Pipeline.",
        "start": 349.37,
        "duration": 3.03
    },
    {
        "text": "The Pipeline is a graph where nodes represent",
        "start": 352.4,
        "duration": 2.61
    },
    {
        "text": "operations and edges represents data flow.",
        "start": 355.01,
        "duration": 3.585
    },
    {
        "text": "In this case, is a Pipeline has",
        "start": 358.595,
        "duration": 2.295
    },
    {
        "text": "two input datasets and could it in different formats.",
        "start": 360.89,
        "duration": 3.365
    },
    {
        "text": "Each dataset is fed to",
        "start": 364.255,
        "duration": 2.005
    },
    {
        "text": "a specific prepossessing script that will convert",
        "start": 366.26,
        "duration": 2.88
    },
    {
        "text": "the data to a common format that can be",
        "start": 369.14,
        "duration": 2.07
    },
    {
        "text": "aggregated and fed to data flow for training.",
        "start": 371.21,
        "duration": 3.42
    },
    {
        "text": "Here is what this pipeline would look in the Azure ML designer.",
        "start": 374.63,
        "duration": 4.395
    },
    {
        "text": "To understand the demo,",
        "start": 379.025,
        "duration": 1.5
    },
    {
        "text": "we need to look at all Azure ML executes pipelines.",
        "start": 380.525,
        "duration": 2.969
    },
    {
        "text": "First, we need to specify your turning compute's targets.",
        "start": 383.494,
        "duration": 3.336
    },
    {
        "text": "Azure ML supports many types of compute,",
        "start": 386.83,
        "duration": 2.485
    },
    {
        "text": "including Azure Batch and Azure ML compute clusters.",
        "start": 389.315,
        "duration": 2.985
    },
    {
        "text": "But for Confidential Pipelines, will require",
        "start": 392.3,
        "duration": 1.83
    },
    {
        "text": "Confidential Computing hardware supports.",
        "start": 394.13,
        "duration": 2.145
    },
    {
        "text": "So we will use an attached DC-series v2.0 VM",
        "start": 396.275,
        "duration": 2.595
    },
    {
        "text": "as our Compute targets.",
        "start": 398.87,
        "duration": 1.46
    },
    {
        "text": "Let's log into Pipeline, actually,",
        "start": 400.33,
        "duration": 2.02
    },
    {
        "text": "represents a Docker container that is pulled",
        "start": 402.35,
        "duration": 2.19
    },
    {
        "text": "from the registry by the management got",
        "start": 404.54,
        "duration": 1.995
    },
    {
        "text": "installed by the AML execution service.",
        "start": 406.535,
        "duration": 2.615
    },
    {
        "text": "Similarly, the data represented by",
        "start": 409.15,
        "duration": 2.08
    },
    {
        "text": "the incoming edges is denoted from",
        "start": 411.23,
        "duration": 1.95
    },
    {
        "text": "storage by the management codes and",
        "start": 413.18,
        "duration": 2.01
    },
    {
        "text": "the outputs are uploaded after execution.",
        "start": 415.19,
        "duration": 2.43
    },
    {
        "text": "The console outputs from the container are also collected by",
        "start": 417.62,
        "duration": 3.39
    },
    {
        "text": "the telemetry service and can be",
        "start": 421.01,
        "duration": 1.68
    },
    {
        "text": "inspected during and after execution.",
        "start": 422.69,
        "duration": 2.64
    },
    {
        "text": "Earlier, I said that",
        "start": 425.33,
        "duration": 1.725
    },
    {
        "text": "only the critical part of",
        "start": 427.055,
        "duration": 1.095
    },
    {
        "text": "the application code runs in the enclave.",
        "start": 428.15,
        "duration": 2.325
    },
    {
        "text": "This approach requires you to refactor",
        "start": 430.475,
        "duration": 2.535
    },
    {
        "text": "the application to isolate",
        "start": 433.01,
        "duration": 1.41
    },
    {
        "text": "its parts critical to data confidentiality.",
        "start": 434.42,
        "duration": 2.585
    },
    {
        "text": "Due to restrictions for the enclave code,",
        "start": 437.005,
        "duration": 2.545
    },
    {
        "text": "such as having to exit the enclaves to do system codes.",
        "start": 439.55,
        "duration": 3.165
    },
    {
        "text": "It is difficult to run applications like TensorFlow,",
        "start": 442.715,
        "duration": 2.925
    },
    {
        "text": "which come with large runtime and use many system codes.",
        "start": 445.64,
        "duration": 3.17
    },
    {
        "text": "For Confidential Pipelines, we rely instead on",
        "start": 448.81,
        "duration": 3.65
    },
    {
        "text": "the user space implementation of",
        "start": 452.46,
        "duration": 1.43
    },
    {
        "text": "the operating system inside the enclave,",
        "start": 453.89,
        "duration": 2.31
    },
    {
        "text": "which enables a lift and shift model for running",
        "start": 456.2,
        "duration": 2.475
    },
    {
        "text": "arbitrary applications, as the cost of a larger TCB.",
        "start": 458.675,
        "duration": 3.77
    },
    {
        "text": "We use an open enclave ports of the Linux Kernel Library or",
        "start": 462.445,
        "duration": 3.665
    },
    {
        "text": "LKL to execute arbitrary Linux Containers inside the enclave.",
        "start": 466.11,
        "duration": 4.925
    },
    {
        "text": "Based learning project originally",
        "start": 471.035,
        "duration": 1.785
    },
    {
        "text": "developed at Imperial College in London.",
        "start": 472.82,
        "duration": 2.64
    },
    {
        "text": "Due to the overhead of exiting and resuming an enclave,",
        "start": 475.46,
        "duration": 3.54
    },
    {
        "text": "we use shared memory to",
        "start": 479.0,
        "duration": 1.68
    },
    {
        "text": "communicate with untrusted hosts for console,",
        "start": 480.68,
        "duration": 2.805
    },
    {
        "text": "networking, and virtual block devices.",
        "start": 483.485,
        "duration": 2.64
    },
    {
        "text": "With LKL, we can express a Confidential Pipeline as",
        "start": 486.125,
        "duration": 3.465
    },
    {
        "text": "a normal pipeline where every node",
        "start": 489.59,
        "duration": 1.755
    },
    {
        "text": "is running the confidential container runtime.",
        "start": 491.345,
        "duration": 2.405
    },
    {
        "text": "So actual operation performed by the nodes",
        "start": 493.75,
        "duration": 2.47
    },
    {
        "text": "is encoded in the encrypted containers",
        "start": 496.22,
        "duration": 2.1
    },
    {
        "text": "that LKL runtime will execute",
        "start": 498.32,
        "duration": 1.895
    },
    {
        "text": "and is treated as an additional data inputs.",
        "start": 500.215,
        "duration": 2.965
    },
    {
        "text": "We also need a mechanism to make the data available to",
        "start": 503.18,
        "duration": 3.15
    },
    {
        "text": "the encrypted container and ensure",
        "start": 506.33,
        "duration": 1.78
    },
    {
        "text": "its confidentiality and integrity.",
        "start": 508.11,
        "duration": 2.155
    },
    {
        "text": "In line with the list and shift model,",
        "start": 510.265,
        "duration": 2.195
    },
    {
        "text": "we want to ensure that data encryption is managed by",
        "start": 512.46,
        "duration": 2.58
    },
    {
        "text": "the LKL runtime and transparent to the application.",
        "start": 515.04,
        "duration": 3.195
    },
    {
        "text": "In other words, the Python script in",
        "start": 518.235,
        "duration": 2.315
    },
    {
        "text": "the container should be able to read",
        "start": 520.55,
        "duration": 1.38
    },
    {
        "text": "and write data files normally.",
        "start": 521.93,
        "duration": 1.985
    },
    {
        "text": "We store all data in Sparse X4 disk images and use a Linux device",
        "start": 523.915,
        "duration": 4.885
    },
    {
        "text": "mapper to encrypt and integrity protects",
        "start": 528.8,
        "duration": 2.49
    },
    {
        "text": "the disk images with dm-crypt and dm-verity.",
        "start": 531.29,
        "duration": 3.275
    },
    {
        "text": "In fact, since containers are essentially disc images,",
        "start": 534.565,
        "duration": 3.94
    },
    {
        "text": "we can use the same mechanism for the roots and data file systems.",
        "start": 538.505,
        "duration": 3.69
    },
    {
        "text": "In multiparty scenarios, it is essential that",
        "start": 542.195,
        "duration": 2.895
    },
    {
        "text": "all participants agree on",
        "start": 545.09,
        "duration": 1.38
    },
    {
        "text": "every step of the pipeline that uses a data.",
        "start": 546.47,
        "duration": 2.625
    },
    {
        "text": "This is why confidential pipelines must be",
        "start": 549.095,
        "duration": 2.325
    },
    {
        "text": "specified in adjacent manifests,",
        "start": 551.42,
        "duration": 1.949
    },
    {
        "text": "which describes all of the containers,",
        "start": 553.369,
        "duration": 1.996
    },
    {
        "text": "data images and steps.",
        "start": 555.365,
        "duration": 1.955
    },
    {
        "text": "To write a manifest,",
        "start": 557.32,
        "duration": 1.69
    },
    {
        "text": "participants must first create and encrypt the containers and",
        "start": 559.01,
        "duration": 2.75
    },
    {
        "text": "data images as a dm-verity root hash is used to identify them.",
        "start": 561.76,
        "duration": 4.645
    },
    {
        "text": "We provide a key management service that can import",
        "start": 566.405,
        "duration": 2.91
    },
    {
        "text": "the disk encryption keys together with a key export policy.",
        "start": 569.315,
        "duration": 3.36
    },
    {
        "text": "To export the key from the Key Manager to the container runtime,",
        "start": 572.675,
        "duration": 3.645
    },
    {
        "text": "the LKL enclave creates an attestation",
        "start": 576.32,
        "duration": 2.37
    },
    {
        "text": "reports that includes its local configuration,",
        "start": 578.69,
        "duration": 2.76
    },
    {
        "text": "including all of the root hashes of the disk images to be mounted.",
        "start": 581.45,
        "duration": 3.84
    },
    {
        "text": "The Key Management enclave checks the report and that",
        "start": 585.29,
        "duration": 3.31
    },
    {
        "text": "its information satisfies the export policy",
        "start": 588.6,
        "duration": 2.465
    },
    {
        "text": "before releasing the key.",
        "start": 591.065,
        "duration": 1.64
    },
    {
        "text": "For the data image keys,",
        "start": 592.705,
        "duration": 1.955
    },
    {
        "text": "the policy should consider posts,",
        "start": 594.66,
        "duration": 1.55
    },
    {
        "text": "the measurements of the runtime code,",
        "start": 596.21,
        "duration": 1.755
    },
    {
        "text": "as well as, the specific configuration of",
        "start": 597.965,
        "duration": 1.905
    },
    {
        "text": "the node as measured by attestation.",
        "start": 599.87,
        "duration": 2.19
    },
    {
        "text": "This ensures that the data is only available to",
        "start": 602.06,
        "duration": 2.7
    },
    {
        "text": "the container specified in the agreed upon manifest.",
        "start": 604.76,
        "duration": 3.33
    },
    {
        "text": "During execution, the LKL runtime will export the keys",
        "start": 608.09,
        "duration": 3.6
    },
    {
        "text": "on the Key Manager in order to model containers and disk images.",
        "start": 611.69,
        "duration": 4.14
    },
    {
        "text": "It may also create a new disk image for writing",
        "start": 615.83,
        "duration": 3.0
    },
    {
        "text": "output device and import",
        "start": 618.83,
        "duration": 1.74
    },
    {
        "text": "these keys into Key Manager enclave for the next stage.",
        "start": 620.57,
        "duration": 3.12
    },
    {
        "text": "So export policy for the output of the last stage is specified",
        "start": 623.69,
        "duration": 3.72
    },
    {
        "text": "into manifests and controls who can access the trained model.",
        "start": 627.41,
        "duration": 4.08
    },
    {
        "text": "To prove that the output image was",
        "start": 631.49,
        "duration": 2.01
    },
    {
        "text": "created by the right container and pipeline,",
        "start": 633.5,
        "duration": 2.355
    },
    {
        "text": "we attach a proof of execution that is",
        "start": 635.855,
        "duration": 2.85
    },
    {
        "text": "an attestation report for the root hash of the output disk image.",
        "start": 638.705,
        "duration": 3.655
    },
    {
        "text": "This ensures the integrity of",
        "start": 642.36,
        "duration": 1.76
    },
    {
        "text": "the intermediate and final outputs of the pipeline.",
        "start": 644.12,
        "duration": 3.44
    },
    {
        "text": "Let's move back to the COVID-19 use case.",
        "start": 647.56,
        "duration": 3.715
    },
    {
        "text": "So one simplification we want to make, is instead",
        "start": 651.275,
        "duration": 2.805
    },
    {
        "text": "of having one container for every scripts,",
        "start": 654.08,
        "duration": 2.94
    },
    {
        "text": "we're going to put all of the four scripts in the same container.",
        "start": 657.02,
        "duration": 3.495
    },
    {
        "text": "So here we are looking at",
        "start": 660.515,
        "duration": 2.025
    },
    {
        "text": "the pre-processing script for the first dataset.",
        "start": 662.54,
        "duration": 3.255
    },
    {
        "text": "This is from the Italian Institute of Radiology.",
        "start": 665.795,
        "duration": 4.5
    },
    {
        "text": "So we have this metadata file which",
        "start": 670.295,
        "duration": 2.625
    },
    {
        "text": "contains some demographic information about",
        "start": 672.92,
        "duration": 2.415
    },
    {
        "text": "patients and the name of",
        "start": 675.335,
        "duration": 2.145
    },
    {
        "text": "the JPEG file that contains chest picture.",
        "start": 677.48,
        "duration": 3.9
    },
    {
        "text": "So for these datasets,",
        "start": 681.38,
        "duration": 2.73
    },
    {
        "text": "we essentially need to perform three operations.",
        "start": 684.11,
        "duration": 2.535
    },
    {
        "text": "We need to normalize,",
        "start": 686.645,
        "duration": 3.21
    },
    {
        "text": "downscale, and we need to create proper labels.",
        "start": 689.855,
        "duration": 5.435
    },
    {
        "text": "Some of the data as inconsistent labels.",
        "start": 695.29,
        "duration": 3.34
    },
    {
        "text": "So we are going to normalize to three labels.",
        "start": 698.63,
        "duration": 3.15
    },
    {
        "text": "One for normal, one for pneumonia, and one for COVID.",
        "start": 701.78,
        "duration": 3.995
    },
    {
        "text": "So then we are going to read datasets and for",
        "start": 705.775,
        "duration": 5.665
    },
    {
        "text": "every image we are going to decode, standardize, and resize.",
        "start": 711.44,
        "duration": 6.04
    },
    {
        "text": ">> This give us the first preprocess dataset.",
        "start": 717.48,
        "duration": 5.18
    },
    {
        "text": "The second dataset is the one from",
        "start": 723.78,
        "duration": 2.92
    },
    {
        "text": "the Radiological Society of North America,",
        "start": 726.7,
        "duration": 3.585
    },
    {
        "text": "and it also comes with metadata file,",
        "start": 730.285,
        "duration": 3.975
    },
    {
        "text": "which has a name of the picture and a label.",
        "start": 734.26,
        "duration": 3.585
    },
    {
        "text": "These datasets only contains normal and pneumonia labels,",
        "start": 737.845,
        "duration": 5.805
    },
    {
        "text": "which is why we need to aggregate with all the datasets for COVID.",
        "start": 743.65,
        "duration": 5.16
    },
    {
        "text": "In terms of preprocessing operations,",
        "start": 748.81,
        "duration": 3.27
    },
    {
        "text": "we also need to normalize the labels, as is shown here,",
        "start": 752.08,
        "duration": 3.225
    },
    {
        "text": "and we want to create a single dataset file,",
        "start": 755.305,
        "duration": 4.395
    },
    {
        "text": "where we do a first dicom decodes,",
        "start": 759.7,
        "duration": 3.03
    },
    {
        "text": "then this other decision resize,",
        "start": 762.73,
        "duration": 2.715
    },
    {
        "text": "and we write to the data file.",
        "start": 765.445,
        "duration": 3.025
    },
    {
        "text": "We also have an aggregation scripts,",
        "start": 769.71,
        "duration": 4.33
    },
    {
        "text": "which is going to take both of",
        "start": 774.04,
        "duration": 2.64
    },
    {
        "text": "the data files produced by the two prepossessing scripts,",
        "start": 776.68,
        "duration": 3.21
    },
    {
        "text": "and shuffle them, writes",
        "start": 779.89,
        "duration": 3.18
    },
    {
        "text": "a single merge datasets in",
        "start": 783.07,
        "duration": 1.38
    },
    {
        "text": "HD file format that is ready for training.",
        "start": 784.45,
        "duration": 2.94
    },
    {
        "text": "This script is verified for words,",
        "start": 787.39,
        "duration": 2.31
    },
    {
        "text": "so you can see here that it feeds into datasets,",
        "start": 789.7,
        "duration": 2.85
    },
    {
        "text": "it does a shuffle,",
        "start": 792.55,
        "duration": 1.5
    },
    {
        "text": "and it writes the outputs to a single file.",
        "start": 794.05,
        "duration": 4.72
    },
    {
        "text": "Finally, we have the training script itself,",
        "start": 799.62,
        "duration": 4.3
    },
    {
        "text": "which is a simple helper around the function to invoke the model.",
        "start": 803.92,
        "duration": 4.785
    },
    {
        "text": "Just as a extra arguments to set the inputs and",
        "start": 808.705,
        "duration": 3.645
    },
    {
        "text": "output folders to the proper mounting points for the disk images.",
        "start": 812.35,
        "duration": 4.785
    },
    {
        "text": "Here we have the Docker file that contains all of the script,",
        "start": 817.135,
        "duration": 4.89
    },
    {
        "text": "and it needs to contain all of the dependencies including",
        "start": 822.025,
        "duration": 3.93
    },
    {
        "text": "TensorFlow and all of",
        "start": 825.955,
        "duration": 1.545
    },
    {
        "text": "the Python packages that we use in the preprocessing scripts.",
        "start": 827.5,
        "duration": 3.405
    },
    {
        "text": "Next, we need to build the Docker container,",
        "start": 830.905,
        "duration": 3.105
    },
    {
        "text": "which normally takes a while,",
        "start": 834.01,
        "duration": 1.44
    },
    {
        "text": "but here we have most of the operations already cached.",
        "start": 835.45,
        "duration": 4.56
    },
    {
        "text": "When this is done, we need to",
        "start": 840.01,
        "duration": 2.58
    },
    {
        "text": "encrypt and upload your container to storage.",
        "start": 842.59,
        "duration": 3.91
    },
    {
        "text": "Normally, encrypting the full container",
        "start": 849.3,
        "duration": 2.89
    },
    {
        "text": "and uploading it takes a few minutes.",
        "start": 852.19,
        "duration": 2.37
    },
    {
        "text": "So I'm going to skip ahead to show you",
        "start": 854.56,
        "duration": 3.78
    },
    {
        "text": "that here we have computed the root hash of the container,",
        "start": 858.34,
        "duration": 3.96
    },
    {
        "text": "and this can be used in the pipeline manifest,",
        "start": 862.3,
        "duration": 3.675
    },
    {
        "text": "which is shown here.",
        "start": 865.975,
        "duration": 1.74
    },
    {
        "text": "The main purpose of manifest includes",
        "start": 867.715,
        "duration": 2.355
    },
    {
        "text": "the definition of the container images that are used,",
        "start": 870.07,
        "duration": 3.405
    },
    {
        "text": "and as you can see, we only have one,",
        "start": 873.475,
        "duration": 2.295
    },
    {
        "text": "which is the one that we just created.",
        "start": 875.77,
        "duration": 2.235
    },
    {
        "text": "The declaration of the encrypted disk images.",
        "start": 878.005,
        "duration": 3.45
    },
    {
        "text": "So you can see the two images corresponding to the input datasets,",
        "start": 881.455,
        "duration": 4.56
    },
    {
        "text": "as well as, additional images",
        "start": 886.015,
        "duration": 3.315
    },
    {
        "text": "that corresponds to the intermediate outputs of every nodes.",
        "start": 889.33,
        "duration": 3.75
    },
    {
        "text": "We need that to know what size to",
        "start": 893.08,
        "duration": 2.4
    },
    {
        "text": "allocate when we create the outputs' disk images.",
        "start": 895.48,
        "duration": 4.185
    },
    {
        "text": "There is also a special case for",
        "start": 899.665,
        "duration": 2.235
    },
    {
        "text": "the very last stage of the pipeline",
        "start": 901.9,
        "duration": 2.01
    },
    {
        "text": "to define who can access the encryption key for the results,",
        "start": 903.91,
        "duration": 4.365
    },
    {
        "text": "in this case, the trained model.",
        "start": 908.275,
        "duration": 2.67
    },
    {
        "text": "In this example, we are using the identity of",
        "start": 910.945,
        "duration": 3.285
    },
    {
        "text": "the confidential inference enclave to deploy the model.",
        "start": 914.23,
        "duration": 3.525
    },
    {
        "text": "Finally, we have the definition of the steps in the pipeline",
        "start": 917.755,
        "duration": 3.765
    },
    {
        "text": "which define what command we are going",
        "start": 921.52,
        "duration": 1.92
    },
    {
        "text": "to run inside the container,",
        "start": 923.44,
        "duration": 2.25
    },
    {
        "text": "and what are the mounted inputs and outputs' disk images.",
        "start": 925.69,
        "duration": 5.099
    },
    {
        "text": "So as you can see,",
        "start": 930.789,
        "duration": 1.471
    },
    {
        "text": "we have the two prepossessing nodes,",
        "start": 932.26,
        "duration": 3.165
    },
    {
        "text": "followed by some merging nodes which cause aggregation script.",
        "start": 935.425,
        "duration": 5.295
    },
    {
        "text": "Then we have the training nodes",
        "start": 940.72,
        "duration": 2.685
    },
    {
        "text": "which is going to cause the training script.",
        "start": 943.405,
        "duration": 2.835
    },
    {
        "text": "Next, I'm going to encrypt and upload the first dataset.",
        "start": 946.24,
        "duration": 4.77
    },
    {
        "text": "For that, you only need to provide",
        "start": 951.01,
        "duration": 2.325
    },
    {
        "text": "the folder that contains the data.",
        "start": 953.335,
        "duration": 2.79
    },
    {
        "text": "The script is going to create,",
        "start": 956.125,
        "duration": 2.01
    },
    {
        "text": "locate the disk image,",
        "start": 958.135,
        "duration": 1.365
    },
    {
        "text": "compute the root hash,",
        "start": 959.5,
        "duration": 1.575
    },
    {
        "text": "and upload the image to storage.",
        "start": 961.075,
        "duration": 3.359
    },
    {
        "text": "Finally, it's going to install",
        "start": 964.434,
        "duration": 2.626
    },
    {
        "text": "the encryption key for the image and the accumulator enclave.",
        "start": 967.06,
        "duration": 4.425
    },
    {
        "text": "Similarly, we are going to do",
        "start": 971.485,
        "duration": 2.025
    },
    {
        "text": "the same operation for the second dataset.",
        "start": 973.51,
        "duration": 2.46
    },
    {
        "text": "It's going to create the disc image, encrypt it,",
        "start": 975.97,
        "duration": 4.395
    },
    {
        "text": "upload it to storage,",
        "start": 980.365,
        "duration": 1.649
    },
    {
        "text": "and install the encryption key to the accumulator service.",
        "start": 982.014,
        "duration": 3.626
    },
    {
        "text": "Because this is a larger datasets,",
        "start": 985.64,
        "duration": 2.154
    },
    {
        "text": "I'm going to skip ahead.",
        "start": 987.794,
        "duration": 1.531
    },
    {
        "text": "As you can see, this takes a few minutes to complete.",
        "start": 989.325,
        "duration": 3.925
    },
    {
        "text": "Note that we have encrypted and uploaded",
        "start": 994.38,
        "duration": 3.34
    },
    {
        "text": "both the container and the disk images,",
        "start": 997.72,
        "duration": 2.775
    },
    {
        "text": "we are actually ready to execute the pipeline.",
        "start": 1000.495,
        "duration": 2.73
    },
    {
        "text": "So I'm going to cause this Python script that is going to",
        "start": 1003.225,
        "duration": 4.2
    },
    {
        "text": "turn the manifest into an actual AML pipeline definition,",
        "start": 1007.425,
        "duration": 4.665
    },
    {
        "text": "and here you can see that it's submitting the pipeline to AML.",
        "start": 1012.09,
        "duration": 4.51
    },
    {
        "text": "So we get a link to the Azure Machine Learning portal,",
        "start": 1024.86,
        "duration": 3.7
    },
    {
        "text": "which essentially allows us to track the execution.",
        "start": 1028.56,
        "duration": 4.09
    },
    {
        "text": "Normally, this pipeline takes about 20 minutes to execute,",
        "start": 1032.69,
        "duration": 4.705
    },
    {
        "text": "so I'm going to skip ahead and directly show you the results.",
        "start": 1037.395,
        "duration": 5.695
    },
    {
        "text": "This is a page that shows you the status of",
        "start": 1043.31,
        "duration": 3.82
    },
    {
        "text": "the execution of a pipeline on the Azure ML portal.",
        "start": 1047.13,
        "duration": 3.615
    },
    {
        "text": "In this case, you can see that",
        "start": 1050.745,
        "duration": 2.805
    },
    {
        "text": "every node in the pipeline has finished executing,",
        "start": 1053.55,
        "duration": 3.21
    },
    {
        "text": "which is indicated by the green check mark.",
        "start": 1056.76,
        "duration": 3.795
    },
    {
        "text": "We can go ahead and click on one of these nodes to look",
        "start": 1060.555,
        "duration": 4.755
    },
    {
        "text": "at the outputs that have been collected by the execution engine.",
        "start": 1065.31,
        "duration": 4.455
    },
    {
        "text": "Some of these outputs have been",
        "start": 1069.765,
        "duration": 2.025
    },
    {
        "text": "generated by some management code of AML,",
        "start": 1071.79,
        "duration": 2.49
    },
    {
        "text": "while others are coming from the LKL launcher,",
        "start": 1074.28,
        "duration": 3.255
    },
    {
        "text": "including configuration of LKL that you can see here,",
        "start": 1077.535,
        "duration": 3.48
    },
    {
        "text": "and we show that we are calling",
        "start": 1081.015,
        "duration": 1.365
    },
    {
        "text": "the first preprocessing scripts, and this configuration.",
        "start": 1082.38,
        "duration": 4.455
    },
    {
        "text": "We also have the output of LKL itself because LKL is",
        "start": 1086.835,
        "duration": 4.68
    },
    {
        "text": "attaching the console outputs of the enclave to the host.",
        "start": 1091.515,
        "duration": 4.32
    },
    {
        "text": "We can use that to actually look at",
        "start": 1095.835,
        "duration": 2.235
    },
    {
        "text": "the outputs of the preprocessing script itself.",
        "start": 1098.07,
        "duration": 3.06
    },
    {
        "text": "So let's go ahead and do that for the merging step.",
        "start": 1101.13,
        "duration": 4.8
    },
    {
        "text": "So you can see the configuration.",
        "start": 1105.93,
        "duration": 3.06
    },
    {
        "text": "We showed that we have caused a merged script.",
        "start": 1108.99,
        "duration": 2.43
    },
    {
        "text": "You can see the two mounted disk images,",
        "start": 1111.42,
        "duration": 2.925
    },
    {
        "text": "and you can see the actual output of the script,",
        "start": 1114.345,
        "duration": 4.95
    },
    {
        "text": "which tells you that we have 244 images in the dataset.",
        "start": 1119.295,
        "duration": 5.265
    },
    {
        "text": "Finally, we look at the output from the training script, itself.",
        "start": 1124.56,
        "duration": 5.37
    },
    {
        "text": "So as you can see,",
        "start": 1129.93,
        "duration": 2.1
    },
    {
        "text": "this is the training command that was used, train.py.",
        "start": 1132.03,
        "duration": 5.01
    },
    {
        "text": "We can look at the output on TensorFlow,",
        "start": 1137.04,
        "duration": 4.74
    },
    {
        "text": "which is going to give us a lot of debug information,",
        "start": 1141.78,
        "duration": 4.68
    },
    {
        "text": "but also allows us to track the progress of the training.",
        "start": 1146.46,
        "duration": 5.35
    },
    {
        "text": "Now that we've trained the model,",
        "start": 1151.97,
        "duration": 2.29
    },
    {
        "text": "we want to evaluate it by submitting some inference requests.",
        "start": 1154.26,
        "duration": 4.215
    },
    {
        "text": "To do that, we will use",
        "start": 1158.475,
        "duration": 1.725
    },
    {
        "text": "the confidential inference service design",
        "start": 1160.2,
        "duration": 2.085
    },
    {
        "text": "that was already presented at the Ignites last year.",
        "start": 1162.285,
        "duration": 3.555
    },
    {
        "text": "This is the common line that we",
        "start": 1165.84,
        "duration": 2.13
    },
    {
        "text": "use to submit an inference request.",
        "start": 1167.97,
        "duration": 2.715
    },
    {
        "text": "So in this case, we have",
        "start": 1170.685,
        "duration": 1.98
    },
    {
        "text": "five sample patients that we want to run against the model.",
        "start": 1172.665,
        "duration": 5.04
    },
    {
        "text": "For that, we first need to create an encrypted and",
        "start": 1177.705,
        "duration": 3.285
    },
    {
        "text": "a tested secure channel between",
        "start": 1180.99,
        "duration": 1.56
    },
    {
        "text": "the clients and the inference service.",
        "start": 1182.55,
        "duration": 2.49
    },
    {
        "text": "So here, you see that we obtain the quotes based enclave that",
        "start": 1185.04,
        "duration": 4.38
    },
    {
        "text": "confirms that the key has been produced by the inference service.",
        "start": 1189.42,
        "duration": 5.355
    },
    {
        "text": "We are going to submit the inference requests",
        "start": 1194.775,
        "duration": 2.805
    },
    {
        "text": "and get back the prediction.",
        "start": 1197.58,
        "duration": 1.62
    },
    {
        "text": "So here you can see for each patient,",
        "start": 1199.2,
        "duration": 2.549
    },
    {
        "text": "which is a probability for each label.",
        "start": 1201.749,
        "duration": 2.851
    },
    {
        "text": "If you want more information on Azure Confidential Computing,",
        "start": 1204.6,
        "duration": 4.185
    },
    {
        "text": "please visit aka.ms/AzureCC or contact",
        "start": 1208.785,
        "duration": 4.305
    },
    {
        "text": "the Azure Confidential Computing",
        "start": 1213.09,
        "duration": 1.26
    },
    {
        "text": "team at azconfidentialpm@Microsoft.com.",
        "start": 1214.35,
        "duration": 3.3
    },
    {
        "text": "[MUSIC]",
        "start": 1217.65,
        "duration": 13.35
    }
]