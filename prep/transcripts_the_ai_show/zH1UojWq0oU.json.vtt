[
    {
        "text": ">> You're not going to want to miss",
        "start": 0.0,
        "duration": 1.2
    },
    {
        "text": "this episode of The AI Show where",
        "start": 1.2,
        "duration": 1.56
    },
    {
        "text": "Shivani Patel shows us how machine-learning open-source,",
        "start": 2.76,
        "duration": 3.15
    },
    {
        "text": "and Azure Machine Learning Service go",
        "start": 5.91,
        "duration": 1.59
    },
    {
        "text": "really well together. Make sure you tune in.",
        "start": 7.5,
        "duration": 2.1
    },
    {
        "text": "[MUSIC]",
        "start": 9.6,
        "duration": 7.65
    },
    {
        "text": ">> Hello and welcome to this episode of The AI Show.",
        "start": 17.25,
        "duration": 2.07
    },
    {
        "text": "As you recall, last week we got to talk",
        "start": 19.32,
        "duration": 2.84
    },
    {
        "text": "to my new BFF Andy who works on Scikit-learn.",
        "start": 22.16,
        "duration": 3.045
    },
    {
        "text": "He's a core contributor there,",
        "start": 25.205,
        "duration": 1.605
    },
    {
        "text": "and we signed an action briefly",
        "start": 26.81,
        "duration": 2.1
    },
    {
        "text": "in some notebooks with Azure machine learning.",
        "start": 28.91,
        "duration": 2.34
    },
    {
        "text": "I have here someone who I'm going to ask the hard questions to,",
        "start": 31.25,
        "duration": 3.54
    },
    {
        "text": "Shivani how you doing my friend?",
        "start": 34.79,
        "duration": 1.26
    },
    {
        "text": ">> Good how are you?",
        "start": 36.05,
        "duration": 1.065
    },
    {
        "text": ">> Better. For those that don't know you,",
        "start": 37.115,
        "duration": 1.905
    },
    {
        "text": "tell us who you are and what you do.",
        "start": 39.02,
        "duration": 1.745
    },
    {
        "text": ">> I am a PM on the Azure Machine Learning platform team,",
        "start": 40.765,
        "duration": 4.045
    },
    {
        "text": "and I specifically work on some fun open-source work,",
        "start": 44.81,
        "duration": 4.185
    },
    {
        "text": "and also integration MLOps, all that stuff.",
        "start": 48.995,
        "duration": 4.165
    },
    {
        "text": ">> Let's start with the hard question.",
        "start": 53.16,
        "duration": 1.79
    },
    {
        "text": "Andy, last week showed a little bit of Scikit-learn,",
        "start": 54.95,
        "duration": 2.43
    },
    {
        "text": "but how integrated are",
        "start": 57.38,
        "duration": 1.83
    },
    {
        "text": "the open source machine learning",
        "start": 59.21,
        "duration": 1.59
    },
    {
        "text": "frameworks inside of Azure Machine Learning?",
        "start": 60.8,
        "duration": 2.6
    },
    {
        "text": "Is it something that you think about?",
        "start": 63.4,
        "duration": 2.385
    },
    {
        "text": ">> Yeah, I'm really glad you asked. I actually have.",
        "start": 65.785,
        "duration": 3.085
    },
    {
        "text": "Let's switch over to a slide real quick.",
        "start": 68.87,
        "duration": 2.73
    },
    {
        "text": "As you can see here,",
        "start": 71.6,
        "duration": 1.649
    },
    {
        "text": "we do focus on supporting open-source frameworks.",
        "start": 73.249,
        "duration": 2.491
    },
    {
        "text": "Our whole goal as a platform is to be open and interoperable.",
        "start": 75.74,
        "duration": 2.99
    },
    {
        "text": "So that means is that we do have support and",
        "start": 78.73,
        "duration": 2.95
    },
    {
        "text": "make it a lot simpler to submit training jobs",
        "start": 81.68,
        "duration": 3.65
    },
    {
        "text": "as well as do model deployments with",
        "start": 85.33,
        "duration": 3.6
    },
    {
        "text": "these pre-installed packages and libraries",
        "start": 88.93,
        "duration": 2.65
    },
    {
        "text": "for all the common Open source frameworks that you're using.",
        "start": 91.58,
        "duration": 2.925
    },
    {
        "text": "We definitely have a strong focus on",
        "start": 94.505,
        "duration": 2.235
    },
    {
        "text": "making sure that we're supporting these Open source standards,",
        "start": 96.74,
        "duration": 2.58
    },
    {
        "text": "as well as the frameworks.",
        "start": 99.32,
        "duration": 1.64
    },
    {
        "text": ">> That's awesome. Generally, when I see Open source things",
        "start": 100.96,
        "duration": 2.75
    },
    {
        "text": "running in a Notebook for example,",
        "start": 103.71,
        "duration": 2.51
    },
    {
        "text": "it's always running in the context of the Notebooks.",
        "start": 106.22,
        "duration": 3.57
    },
    {
        "text": "Do you have an example or examples of",
        "start": 109.79,
        "duration": 2.76
    },
    {
        "text": "Open source frameworks and how they",
        "start": 112.55,
        "duration": 1.935
    },
    {
        "text": "interact with Azure machine learning?",
        "start": 114.485,
        "duration": 1.965
    },
    {
        "text": "Then for those that don't even know what",
        "start": 116.45,
        "duration": 1.38
    },
    {
        "text": "Azure Machine Learning is can you give us an elevator pitch?",
        "start": 117.83,
        "duration": 2.16
    },
    {
        "text": "Let's start with that one first.",
        "start": 119.99,
        "duration": 1.355
    },
    {
        "text": ">> Yeah, definitely. Azure Machine Learning Platform is basically",
        "start": 121.345,
        "duration": 3.835
    },
    {
        "text": "a platform that enables you to track,",
        "start": 125.18,
        "duration": 3.63
    },
    {
        "text": "monitor all of your training, runs your experiments,",
        "start": 128.81,
        "duration": 2.88
    },
    {
        "text": "manage your data as well as manage",
        "start": 131.69,
        "duration": 2.79
    },
    {
        "text": "the models as well as the deployments that you are creating.",
        "start": 134.48,
        "duration": 3.15
    },
    {
        "text": "So pretty much we support end-to-end ML life cycle.",
        "start": 137.63,
        "duration": 5.595
    },
    {
        "text": ">> That's an awesome elevator pitch.",
        "start": 143.225,
        "duration": 2.19
    },
    {
        "text": "But again, like I just said earlier,",
        "start": 145.415,
        "duration": 1.695
    },
    {
        "text": "when I'm running these TOSS things, usually, all the computation is",
        "start": 147.11,
        "duration": 2.85
    },
    {
        "text": "happening on the Notebook itself.",
        "start": 149.96,
        "duration": 3.18
    },
    {
        "text": "How hard is it to move this open source stuff",
        "start": 153.14,
        "duration": 3.135
    },
    {
        "text": "into Azure ML and not just be running",
        "start": 156.275,
        "duration": 2.535
    },
    {
        "text": "on my local host Notebook for example.",
        "start": 158.81,
        "duration": 1.88
    },
    {
        "text": "Do you have any examples of that?",
        "start": 160.69,
        "duration": 1.03
    },
    {
        "text": ">> Right. Definitely, I can show you some examples there.",
        "start": 161.72,
        "duration": 3.22
    },
    {
        "text": "I'm going to switch over to",
        "start": 164.94,
        "duration": 1.67
    },
    {
        "text": "my screen and pop up my example here real quick.",
        "start": 166.61,
        "duration": 4.78
    },
    {
        "text": "Let me actually first show you what I'm running.",
        "start": 171.39,
        "duration": 3.67
    },
    {
        "text": "We have a set of these examples in Notebooks that provide",
        "start": 175.06,
        "duration": 3.22
    },
    {
        "text": "these curated environments that will",
        "start": 178.28,
        "duration": 3.3
    },
    {
        "text": "essentially have all the requirements to run,",
        "start": 181.58,
        "duration": 3.92
    },
    {
        "text": "whichever framework you have.",
        "start": 185.5,
        "duration": 1.775
    },
    {
        "text": "If we click into one of these for example,",
        "start": 187.275,
        "duration": 2.279
    },
    {
        "text": "we just talked about Scikit-learn, so let's pop into there.",
        "start": 189.554,
        "duration": 2.476
    },
    {
        "text": "You have all these packages and dependencies already installed.",
        "start": 192.03,
        "duration": 3.17
    },
    {
        "text": "So you're going from your local environment",
        "start": 195.2,
        "duration": 2.55
    },
    {
        "text": "and now, you want to run it on let's",
        "start": 197.75,
        "duration": 1.62
    },
    {
        "text": "say, Azure compute instance or use GPU CPU.",
        "start": 199.37,
        "duration": 4.71
    },
    {
        "text": "As long as you are calling these environments,",
        "start": 204.08,
        "duration": 2.2
    },
    {
        "text": "you'll have all those packages already there ready for you,",
        "start": 206.28,
        "duration": 2.9
    },
    {
        "text": "and I can show you an example of that. Do I make sense?",
        "start": 209.18,
        "duration": 3.22
    },
    {
        "text": ">> Yeah, so let me see if I'm understanding right.",
        "start": 212.4,
        "duration": 2.415
    },
    {
        "text": "Basically, what you do is in order to run",
        "start": 214.815,
        "duration": 2.93
    },
    {
        "text": "pretty much anything it looks like",
        "start": 217.745,
        "duration": 1.585
    },
    {
        "text": "because now that I'm understanding it,",
        "start": 219.33,
        "duration": 1.76
    },
    {
        "text": "if you have this notion of a thing called an environment,",
        "start": 221.09,
        "duration": 2.955
    },
    {
        "text": "and you explicitly declare that environment,",
        "start": 224.045,
        "duration": 2.67
    },
    {
        "text": "you can use that environment and run it like carbon,",
        "start": 226.715,
        "duration": 3.25
    },
    {
        "text": "like to stamp out that environment and run the things",
        "start": 229.965,
        "duration": 2.54
    },
    {
        "text": "inside of those environments, is that right?",
        "start": 232.505,
        "duration": 2.81
    },
    {
        "text": ">> Yeah, exactly.",
        "start": 235.315,
        "duration": 1.235
    },
    {
        "text": "You could go ahead and use ones that we support.",
        "start": 236.55,
        "duration": 2.34
    },
    {
        "text": "Like I said, we pay attention to what",
        "start": 238.89,
        "duration": 2.06
    },
    {
        "text": "common frameworks people are using,",
        "start": 240.95,
        "duration": 1.33
    },
    {
        "text": "we want to make sure it's as easy as",
        "start": 242.28,
        "duration": 1.61
    },
    {
        "text": "possible to go from local to Cloud.",
        "start": 243.89,
        "duration": 2.145
    },
    {
        "text": "We do have a curated,",
        "start": 246.035,
        "duration": 2.28
    },
    {
        "text": "that's what we call them curated environments,",
        "start": 248.315,
        "duration": 1.955
    },
    {
        "text": "or you can go ahead and create your custom ones,",
        "start": 250.27,
        "duration": 1.96
    },
    {
        "text": "we support that if you have a different framework.",
        "start": 252.23,
        "duration": 2.195
    },
    {
        "text": ">> This is awesome but it's unclear to me.",
        "start": 254.425,
        "duration": 1.645
    },
    {
        "text": "I understand the YAML file and describing it.",
        "start": 256.07,
        "duration": 1.83
    },
    {
        "text": "It looks like basically a",
        "start": 257.9,
        "duration": 1.98
    },
    {
        "text": "requirements.txt file which you'd",
        "start": 259.88,
        "duration": 1.59
    },
    {
        "text": "find but it's a little bit different.",
        "start": 261.47,
        "duration": 1.92
    },
    {
        "text": "How do you translate this YAML file or this basically",
        "start": 263.39,
        "duration": 4.18
    },
    {
        "text": "requirements.txt file into something that's",
        "start": 267.57,
        "duration": 2.24
    },
    {
        "text": "running in a different compute contexts?",
        "start": 269.81,
        "duration": 3.09
    },
    {
        "text": "That part isn't clear to me.",
        "start": 272.9,
        "duration": 1.305
    },
    {
        "text": ">> Definitely, and I totally understand that and I will show you",
        "start": 274.205,
        "duration": 3.165
    },
    {
        "text": "an example of it that'll make it a lot easier to see.",
        "start": 277.37,
        "duration": 3.66
    },
    {
        "text": "Right here, basically I have this",
        "start": 281.03,
        "duration": 3.165
    },
    {
        "text": "diabetes Scikit-learn, super common example.",
        "start": 284.195,
        "duration": 3.64
    },
    {
        "text": "So I have launched this Notebook in VS code,",
        "start": 287.835,
        "duration": 3.3
    },
    {
        "text": "and these are the same examples that I showed you right here.",
        "start": 291.135,
        "duration": 2.645
    },
    {
        "text": "These are just the examples that I cloned into this workspace.",
        "start": 293.78,
        "duration": 4.335
    },
    {
        "text": "Now, all I'm doing here is calling",
        "start": 298.115,
        "duration": 2.325
    },
    {
        "text": "the workspace that I'm working in, fun stuff.",
        "start": 300.44,
        "duration": 3.12
    },
    {
        "text": "I am grabbing that GitHub repo and then",
        "start": 303.56,
        "duration": 2.85
    },
    {
        "text": "I'm calling the environment file that you just saw.",
        "start": 306.41,
        "duration": 2.96
    },
    {
        "text": "So just for a reference,",
        "start": 309.37,
        "duration": 1.575
    },
    {
        "text": "here's environment file, here is the Scikit-learn example.",
        "start": 310.945,
        "duration": 4.375
    },
    {
        "text": "I'm literally just calling that file,",
        "start": 315.32,
        "duration": 2.415
    },
    {
        "text": "and then I'm defining the environment as that,",
        "start": 317.735,
        "duration": 3.405
    },
    {
        "text": "great, and then defining whatever compute target.",
        "start": 321.14,
        "duration": 2.52
    },
    {
        "text": "For this one, I'm just going to use",
        "start": 323.66,
        "duration": 1.47
    },
    {
        "text": "CPU and now you can see what's going on in here.",
        "start": 325.13,
        "duration": 3.46
    },
    {
        "text": "I have my script, all that fun stuff.",
        "start": 328.59,
        "duration": 2.04
    },
    {
        "text": "Now, that I defined the environment,",
        "start": 330.63,
        "duration": 2.325
    },
    {
        "text": "I am basically calling that environment here,",
        "start": 332.955,
        "duration": 3.33
    },
    {
        "text": "I have this run defined,",
        "start": 336.285,
        "duration": 2.609
    },
    {
        "text": "the compute target I chose,",
        "start": 338.894,
        "duration": 1.516
    },
    {
        "text": "and I'm just going to run it.",
        "start": 340.41,
        "duration": 2.79
    },
    {
        "text": ">> I see. Let's see,",
        "start": 343.2,
        "duration": 1.995
    },
    {
        "text": "if we can back out for a second,",
        "start": 345.195,
        "duration": 1.305
    },
    {
        "text": "to see if I'm understanding.",
        "start": 346.5,
        "duration": 1.125
    },
    {
        "text": "So basically there's this notion of",
        "start": 347.625,
        "duration": 1.265
    },
    {
        "text": "a workspace where all the work happens because it's",
        "start": 348.89,
        "duration": 2.07
    },
    {
        "text": "a workspace, these are the jokes.",
        "start": 350.96,
        "duration": 4.03
    },
    {
        "text": "So it's a workspace and inside",
        "start": 354.99,
        "duration": 1.46
    },
    {
        "text": "of that workspace you have any number",
        "start": 356.45,
        "duration": 1.485
    },
    {
        "text": "of things that you call experiment's running,",
        "start": 357.935,
        "duration": 2.915
    },
    {
        "text": "and each of these experiments need something to",
        "start": 360.85,
        "duration": 2.38
    },
    {
        "text": "run and an environment run in. Am I getting this right?",
        "start": 363.23,
        "duration": 3.06
    },
    {
        "text": ">> Exactly.",
        "start": 366.29,
        "duration": 1.56
    },
    {
        "text": ">> Where do these runs actually live or go?",
        "start": 367.85,
        "duration": 5.12
    },
    {
        "text": ">> When you see all of these runs,",
        "start": 372.97,
        "duration": 2.345
    },
    {
        "text": "there's a link here and I've already pulled it up right here.",
        "start": 375.315,
        "duration": 3.05
    },
    {
        "text": "This is the workspace that I was talking about.",
        "start": 378.365,
        "duration": 2.835
    },
    {
        "text": "Soplease zoom in a little bit here so you can see it.",
        "start": 381.2,
        "duration": 3.625
    },
    {
        "text": "We have all the artifacts that you would generally create in",
        "start": 384.825,
        "duration": 3.395
    },
    {
        "text": "your ML life cycle where as I mentioned we're",
        "start": 388.22,
        "duration": 2.13
    },
    {
        "text": "tracking end to end here with Azure Machine Learning.",
        "start": 390.35,
        "duration": 2.68
    },
    {
        "text": "If I go into my experiments and I",
        "start": 393.03,
        "duration": 2.33
    },
    {
        "text": "see I've ran this quite a number of times,",
        "start": 395.36,
        "duration": 2.925
    },
    {
        "text": "you can go in and see the latest run that I",
        "start": 398.285,
        "duration": 2.055
    },
    {
        "text": "just submitted right here,",
        "start": 400.34,
        "duration": 2.38
    },
    {
        "text": "and you can see the compute target that was submitted by me.",
        "start": 402.72,
        "duration": 2.84
    },
    {
        "text": "It was a script,",
        "start": 405.56,
        "duration": 1.53
    },
    {
        "text": "and then I have the script name,",
        "start": 407.09,
        "duration": 2.46
    },
    {
        "text": "information about it, which",
        "start": 409.55,
        "duration": 1.98
    },
    {
        "text": "Git Commit it came from as well as any other properties.",
        "start": 411.53,
        "duration": 2.92
    },
    {
        "text": "So you can track all those runs in this single workspace.",
        "start": 414.45,
        "duration": 5.29
    },
    {
        "text": ">> As I'm starting to think about this,",
        "start": 420.02,
        "duration": 2.26
    },
    {
        "text": "one of the biggest problems that we have in",
        "start": 422.28,
        "duration": 1.73
    },
    {
        "text": "machine learning is this notion of reproducibility",
        "start": 424.01,
        "duration": 3.24
    },
    {
        "text": "because I might come up with",
        "start": 427.25,
        "duration": 1.26
    },
    {
        "text": "the best model in the universe on my machine that",
        "start": 428.51,
        "duration": 2.88
    },
    {
        "text": "had my particular Conda environment with my particular things,",
        "start": 431.39,
        "duration": 4.11
    },
    {
        "text": "and not being able to replicate it can be a huge problem.",
        "start": 435.5,
        "duration": 3.78
    },
    {
        "text": "But in this case,",
        "start": 439.28,
        "duration": 1.625
    },
    {
        "text": "is this running just in a generic compute context?",
        "start": 440.905,
        "duration": 3.715
    },
    {
        "text": "How is this actually running in its own special environment?",
        "start": 444.62,
        "duration": 4.1
    },
    {
        "text": ">> That environment that we're creating,",
        "start": 448.72,
        "duration": 2.89
    },
    {
        "text": "that shared environment that you're creating,",
        "start": 451.61,
        "duration": 1.62
    },
    {
        "text": "we actually log those environments.",
        "start": 453.23,
        "duration": 1.86
    },
    {
        "text": "So when we're creating this environment up here,",
        "start": 455.09,
        "duration": 3.72
    },
    {
        "text": "we scroll back right here,",
        "start": 458.81,
        "duration": 6.16
    },
    {
        "text": "so we're defining a environment.",
        "start": 464.97,
        "duration": 2.04
    },
    {
        "text": "It's not only just the environment file",
        "start": 467.01,
        "duration": 1.55
    },
    {
        "text": "but we're registering it as",
        "start": 468.56,
        "duration": 1.08
    },
    {
        "text": "type of an asset or artifact.",
        "start": 469.64,
        "duration": 3.225
    },
    {
        "text": "It enables you to snapshot all those packages,",
        "start": 472.865,
        "duration": 3.525
    },
    {
        "text": "all those libraries so that someone else could go in and reuse it.",
        "start": 476.39,
        "duration": 3.105
    },
    {
        "text": ">> That's awesome. If I'm understanding this right is that",
        "start": 479.495,
        "duration": 4.215
    },
    {
        "text": "in the workspace, you're also able to",
        "start": 483.71,
        "duration": 2.37
    },
    {
        "text": "collaborate with other people.",
        "start": 486.08,
        "duration": 1.51
    },
    {
        "text": "It's not just you that can log into that workspace. Is that right?",
        "start": 487.59,
        "duration": 2.66
    },
    {
        "text": ">> Yeah, exactly. Again, another emphasis",
        "start": 490.25,
        "duration": 3.12
    },
    {
        "text": "here on the RunConfiguration is saving that environment.",
        "start": 493.37,
        "duration": 2.91
    },
    {
        "text": "So the actual run that you've created is saving that environment.",
        "start": 496.28,
        "duration": 3.18
    },
    {
        "text": "But yeah, right here, you can see in the example that",
        "start": 499.46,
        "duration": 4.11
    },
    {
        "text": "not only me but a few of my colleagues have",
        "start": 503.57,
        "duration": 1.89
    },
    {
        "text": "also submitted their runs here.",
        "start": 505.46,
        "duration": 2.595
    },
    {
        "text": "You could pretty much go in and see that information.",
        "start": 508.055,
        "duration": 4.13
    },
    {
        "text": "This was created by a service principle, that is not a name,",
        "start": 512.185,
        "duration": 2.92
    },
    {
        "text": "it's another concept but essentially,",
        "start": 515.105,
        "duration": 2.235
    },
    {
        "text": "someone else would come in and create these runs.",
        "start": 517.34,
        "duration": 2.26
    },
    {
        "text": "As long as you have access to this workspace,",
        "start": 519.6,
        "duration": 2.719
    },
    {
        "text": "your admin has given you access,",
        "start": 522.319,
        "duration": 1.621
    },
    {
        "text": "you can go in and resubmit this run.",
        "start": 523.94,
        "duration": 2.465
    },
    {
        "text": ">> Awesome. Do you have examples like for example,",
        "start": 526.405,
        "duration": 2.305
    },
    {
        "text": "because you just showed a Scikit-learn,",
        "start": 528.71,
        "duration": 1.38
    },
    {
        "text": "do you have something in PyTorch or TensorFlow?",
        "start": 530.09,
        "duration": 2.025
    },
    {
        "text": ">> Yeah.",
        "start": 532.115,
        "duration": 0.345
    },
    {
        "text": ">> To give us the same concepts over again to",
        "start": 532.46,
        "duration": 1.77
    },
    {
        "text": "help them solidify in my head?",
        "start": 534.23,
        "duration": 2.12
    },
    {
        "text": ">> Yes, for sure.",
        "start": 536.35,
        "duration": 1.68
    },
    {
        "text": "I can actually extend this one and show you",
        "start": 538.03,
        "duration": 2.08
    },
    {
        "text": "a little bit how the deployments out of the world works,",
        "start": 540.11,
        "duration": 2.52
    },
    {
        "text": "and make sure that we support those frameworks as well.",
        "start": 542.63,
        "duration": 2.445
    },
    {
        "text": "Right here, we have a PyTorch model,",
        "start": 545.075,
        "duration": 2.01
    },
    {
        "text": "going through the same actions.",
        "start": 547.085,
        "duration": 2.085
    },
    {
        "text": "You can see you've got the same workspace,",
        "start": 549.17,
        "duration": 2.355
    },
    {
        "text": "I've got this time a GPU cluster that I've defined.",
        "start": 551.525,
        "duration": 3.48
    },
    {
        "text": "So I have that same PyTorch example, again, right here,",
        "start": 555.005,
        "duration": 5.575
    },
    {
        "text": "\"Environments\" and pretty simple.",
        "start": 560.58,
        "duration": 4.005
    },
    {
        "text": "We have all these Torch version,",
        "start": 564.585,
        "duration": 2.125
    },
    {
        "text": "I don't have to worry about reinstalling it,",
        "start": 566.71,
        "duration": 1.96
    },
    {
        "text": "fun stuff, and then I have that environment defined.",
        "start": 568.67,
        "duration": 4.225
    },
    {
        "text": "Great. So I will just click around here,",
        "start": 572.895,
        "duration": 4.02
    },
    {
        "text": "and then defining that",
        "start": 576.915,
        "duration": 2.095
    },
    {
        "text": "this run is going to have that environment registered with it.",
        "start": 579.01,
        "duration": 4.395
    },
    {
        "text": "When you go in and do a nice little run here,",
        "start": 583.405,
        "duration": 3.04
    },
    {
        "text": "and so I have completed this runs.",
        "start": 586.445,
        "duration": 2.895
    },
    {
        "text": "I can show you in the workspace for that like exactly looks like.",
        "start": 589.34,
        "duration": 3.21
    },
    {
        "text": "Here, we've got the PyTorch example.",
        "start": 592.55,
        "duration": 4.1
    },
    {
        "text": "Again, numerous runs, working really hard here.",
        "start": 596.65,
        "duration": 2.43
    },
    {
        "text": "So you see the one that I just queued.",
        "start": 599.08,
        "duration": 2.61
    },
    {
        "text": "So here's a previous one that I had completed.",
        "start": 601.69,
        "duration": 2.805
    },
    {
        "text": "Again, you can see where all this information came from.",
        "start": 604.495,
        "duration": 2.895
    },
    {
        "text": "I also have the arguments defined here,",
        "start": 607.39,
        "duration": 3.495
    },
    {
        "text": "and you can see all this information.",
        "start": 610.885,
        "duration": 1.965
    },
    {
        "text": "I'll go look, didn't log anything there.",
        "start": 612.85,
        "duration": 2.085
    },
    {
        "text": "You can actually go in and see all the logs,",
        "start": 614.935,
        "duration": 2.955
    },
    {
        "text": "all information as well as the output model.",
        "start": 617.89,
        "duration": 3.165
    },
    {
        "text": "Now this is where it gets pretty interesting.",
        "start": 621.055,
        "duration": 2.955
    },
    {
        "text": "So if you go ahead and see,",
        "start": 624.01,
        "duration": 3.21
    },
    {
        "text": "this one doesn't have any registered models.",
        "start": 627.22,
        "duration": 2.445
    },
    {
        "text": "But if we go back,",
        "start": 629.665,
        "duration": 1.425
    },
    {
        "text": "I can show you an example",
        "start": 631.09,
        "duration": 2.265
    },
    {
        "text": "of one that does have a registered model tied to it.",
        "start": 633.355,
        "duration": 2.475
    },
    {
        "text": "So here's another example of that sklearn run.",
        "start": 635.83,
        "duration": 4.095
    },
    {
        "text": "So I see the run ID here.",
        "start": 639.925,
        "duration": 2.34
    },
    {
        "text": "So you can see that this run had an output of this model.",
        "start": 642.265,
        "duration": 5.775
    },
    {
        "text": "So you could do the same thing without PyTorch run.",
        "start": 648.04,
        "duration": 2.97
    },
    {
        "text": "This is just an example.",
        "start": 651.01,
        "duration": 1.11
    },
    {
        "text": "So it works the same way.",
        "start": 652.12,
        "duration": 1.62
    },
    {
        "text": ">> This is really cool.",
        "start": 653.74,
        "duration": 1.62
    },
    {
        "text": "It's like almost like DevOps",
        "start": 655.36,
        "duration": 2.49
    },
    {
        "text": "but for machine-learning types of services.",
        "start": 657.85,
        "duration": 2.88
    },
    {
        "text": ">> Yes, exactly. MLOps.",
        "start": 660.73,
        "duration": 2.43
    },
    {
        "text": ">> This brings me to the next question because I'm familiar",
        "start": 663.16,
        "duration": 2.76
    },
    {
        "text": "with frameworks that generate models for machine learning.",
        "start": 665.92,
        "duration": 2.925
    },
    {
        "text": "But then there's other frameworks that",
        "start": 668.845,
        "duration": 2.67
    },
    {
        "text": "manage the flow of how things go in machine learning,",
        "start": 671.515,
        "duration": 4.335
    },
    {
        "text": "and that was like the greatest segue way of all.",
        "start": 675.85,
        "duration": 4.095
    },
    {
        "text": "I'm thinking about MLflow,",
        "start": 679.945,
        "duration": 1.665
    },
    {
        "text": "which isn't necessarily something",
        "start": 681.61,
        "duration": 2.37
    },
    {
        "text": "that you would do to build a model.",
        "start": 683.98,
        "duration": 2.58
    },
    {
        "text": "It doesn't really fit in the context of like experiments and runs,",
        "start": 686.56,
        "duration": 3.75
    },
    {
        "text": "but it does because it spans across them.",
        "start": 690.31,
        "duration": 2.085
    },
    {
        "text": "Could you describe what MLflow is and then maybe tell us",
        "start": 692.395,
        "duration": 2.775
    },
    {
        "text": "that's something that Azure Machine Learning can support?",
        "start": 695.17,
        "duration": 3.465
    },
    {
        "text": ">> I see you spying on my little browser here.",
        "start": 698.635,
        "duration": 2.235
    },
    {
        "text": "No, for sure.",
        "start": 700.87,
        "duration": 0.96
    },
    {
        "text": "That's why I mentioned we support open-source standards.",
        "start": 701.83,
        "duration": 3.165
    },
    {
        "text": "We're not over here creating our own thing.",
        "start": 704.995,
        "duration": 1.785
    },
    {
        "text": "We understand that there's a lot of Open Source packages,",
        "start": 706.78,
        "duration": 4.56
    },
    {
        "text": "services out there, and people are already using them.",
        "start": 711.34,
        "duration": 2.64
    },
    {
        "text": "One of the most common ones is MLflow.",
        "start": 713.98,
        "duration": 2.76
    },
    {
        "text": "So I'm going to just switch over to the slide so",
        "start": 716.74,
        "duration": 2.85
    },
    {
        "text": "you guys can easily visualize what I'm talking about here.",
        "start": 719.59,
        "duration": 3.165
    },
    {
        "text": "So MLflow's basically another Open Source platform,",
        "start": 722.755,
        "duration": 3.45
    },
    {
        "text": "and it enables you to manage",
        "start": 726.205,
        "duration": 2.265
    },
    {
        "text": "your lifecycle as Azure Machine Learning does.",
        "start": 728.47,
        "duration": 2.64
    },
    {
        "text": "So similar to what I showed you in the workspace.",
        "start": 731.11,
        "duration": 3.0
    },
    {
        "text": "We track experiments, we track deployments,",
        "start": 734.11,
        "duration": 2.549
    },
    {
        "text": "we track models, very similar in that sense.",
        "start": 736.659,
        "duration": 3.556
    },
    {
        "text": "So they have four components to it.",
        "start": 740.215,
        "duration": 2.355
    },
    {
        "text": "They have their tracking for their code, their experiments.",
        "start": 742.57,
        "duration": 3.78
    },
    {
        "text": "They are again very similar to what we do,",
        "start": 746.35,
        "duration": 2.52
    },
    {
        "text": "the way that they submit runs, there's projects.",
        "start": 748.87,
        "duration": 3.509
    },
    {
        "text": "So you can think of that as our",
        "start": 752.379,
        "duration": 1.516
    },
    {
        "text": "run configuration that I just showed you.",
        "start": 753.895,
        "duration": 2.22
    },
    {
        "text": "Then they have MLflow models,",
        "start": 756.115,
        "duration": 2.115
    },
    {
        "text": "so you have registered models like we do.",
        "start": 758.23,
        "duration": 2.505
    },
    {
        "text": "So instead of battling it out,",
        "start": 760.735,
        "duration": 3.435
    },
    {
        "text": "we are working closely to make sure",
        "start": 764.17,
        "duration": 2.49
    },
    {
        "text": "that both of these can integrate seamlessly.",
        "start": 766.66,
        "duration": 2.67
    },
    {
        "text": "So what that means is that if you are someone that it's running",
        "start": 769.33,
        "duration": 3.48
    },
    {
        "text": "your experimentation or running",
        "start": 772.81,
        "duration": 1.86
    },
    {
        "text": "your training jobs in multiple Clouds.",
        "start": 774.67,
        "duration": 2.505
    },
    {
        "text": "You are able to use something like MLflow to track in Azure,",
        "start": 777.175,
        "duration": 5.34
    },
    {
        "text": "your Azure Machine Learning runs as well",
        "start": 782.515,
        "duration": 2.175
    },
    {
        "text": "as if you're running it in some other Cloud.",
        "start": 784.69,
        "duration": 2.01
    },
    {
        "text": "You can use them MLflow to maintain that source of truth.",
        "start": 786.7,
        "duration": 3.48
    },
    {
        "text": "So whatever you are tracking with MLflow and Azure ML,",
        "start": 790.18,
        "duration": 4.575
    },
    {
        "text": "you can see both of those runs in both of those services.",
        "start": 794.755,
        "duration": 4.47
    },
    {
        "text": ">> How does that work? I mean,",
        "start": 799.225,
        "duration": 2.445
    },
    {
        "text": "you already showed me some code where you did some stuff.",
        "start": 801.67,
        "duration": 3.435
    },
    {
        "text": "Is it pretty invasive to introduce it or if you already have it?",
        "start": 805.105,
        "duration": 3.57
    },
    {
        "text": "How does it work now?",
        "start": 808.675,
        "duration": 1.965
    },
    {
        "text": ">> Yeah, I can actually show you right here.",
        "start": 810.64,
        "duration": 3.645
    },
    {
        "text": "So let's take the existing example we have.",
        "start": 814.285,
        "duration": 2.43
    },
    {
        "text": "I know MLflow for those that aren't very familiar,",
        "start": 816.715,
        "duration": 2.595
    },
    {
        "text": "it's a little bit of a confusing concept,",
        "start": 819.31,
        "duration": 1.74
    },
    {
        "text": "but they essentially do the same thing that we do with tracking.",
        "start": 821.05,
        "duration": 3.54
    },
    {
        "text": "There are scenarios where",
        "start": 824.59,
        "duration": 2.055
    },
    {
        "text": "using both MLflow and Azure Machine Learning",
        "start": 826.645,
        "duration": 2.955
    },
    {
        "text": "provides our data scientists more of",
        "start": 829.6,
        "duration": 2.64
    },
    {
        "text": "an end-to-end view of what's actually",
        "start": 832.24,
        "duration": 1.23
    },
    {
        "text": "going on in their whole process.",
        "start": 833.47,
        "duration": 1.92
    },
    {
        "text": "So let's take that same PyTorch example,",
        "start": 835.39,
        "duration": 2.85
    },
    {
        "text": "and I'll show you how you can start tracking with MLflow.",
        "start": 838.24,
        "duration": 4.26
    },
    {
        "text": "So what you basically do here",
        "start": 842.5,
        "duration": 2.91
    },
    {
        "text": "is point this MLflow tracking your IT, your workspace.",
        "start": 845.41,
        "duration": 4.26
    },
    {
        "text": "So what that enables is that all the data, or",
        "start": 849.67,
        "duration": 2.82
    },
    {
        "text": "all the runs that are happening in",
        "start": 852.49,
        "duration": 1.86
    },
    {
        "text": "Azure ML will also get tracked in MLflow.",
        "start": 854.35,
        "duration": 3.255
    },
    {
        "text": "So you're pointing to the specific workspace.",
        "start": 857.605,
        "duration": 3.295
    },
    {
        "text": "Then I'm doing the same activities.",
        "start": 861.54,
        "duration": 2.845
    },
    {
        "text": "The only difference here is that I'm also",
        "start": 864.385,
        "duration": 2.235
    },
    {
        "text": "setting the MLflow experiment name so",
        "start": 866.62,
        "duration": 2.7
    },
    {
        "text": "that it knows that this is",
        "start": 869.32,
        "duration": 2.4
    },
    {
        "text": "the experiment that I want to track in MLflow.",
        "start": 871.72,
        "duration": 2.415
    },
    {
        "text": "Very similar concepts and context.",
        "start": 874.135,
        "duration": 3.285
    },
    {
        "text": "So you don't really have to learn a lot of",
        "start": 877.42,
        "duration": 1.8
    },
    {
        "text": "different concepts to use both of the services together.",
        "start": 879.22,
        "duration": 3.45
    },
    {
        "text": ">> My sense is that that's actually really cool because,",
        "start": 882.67,
        "duration": 3.36
    },
    {
        "text": "maybe you want to do experiments somewhere else and you want to",
        "start": 886.03,
        "duration": 3.27
    },
    {
        "text": "do model management and registry with Azure Machine Learning.",
        "start": 889.3,
        "duration": 3.555
    },
    {
        "text": "Using something that's Open Source like MLflow",
        "start": 892.855,
        "duration": 2.535
    },
    {
        "text": "allows you to pick and choose where you want to do your stuff,",
        "start": 895.39,
        "duration": 2.52
    },
    {
        "text": "for example Azure Databricks.",
        "start": 897.91,
        "duration": 2.025
    },
    {
        "text": ">> Right. Exactly. We have tons of customers that",
        "start": 899.935,
        "duration": 2.085
    },
    {
        "text": "use all 3D services;",
        "start": 902.02,
        "duration": 1.935
    },
    {
        "text": "Databricks, MLflow, and Azure ML,",
        "start": 903.955,
        "duration": 1.98
    },
    {
        "text": "because their scenario is so complex in that sense.",
        "start": 905.935,
        "duration": 2.475
    },
    {
        "text": "They need that flexibility across the different services as well",
        "start": 908.41,
        "duration": 3.36
    },
    {
        "text": "as the ability to trace across Databricks Azure.",
        "start": 911.77,
        "duration": 3.45
    },
    {
        "text": "So MLflow is the answer pretty much for that,",
        "start": 915.22,
        "duration": 3.375
    },
    {
        "text": "and so it makes it a lot simpler in that case.",
        "start": 918.595,
        "duration": 3.63
    },
    {
        "text": "So right here, we see that",
        "start": 922.225,
        "duration": 3.465
    },
    {
        "text": "we've run that experiment with the experiment name.",
        "start": 925.69,
        "duration": 4.11
    },
    {
        "text": "Everything else is pretty much standard.",
        "start": 929.8,
        "duration": 3.9
    },
    {
        "text": "We define the run here.",
        "start": 933.7,
        "duration": 1.68
    },
    {
        "text": "You have to wait for the run to complete.",
        "start": 935.38,
        "duration": 4.125
    },
    {
        "text": "One thing I wanted to highlight here on the deployment side,",
        "start": 939.505,
        "duration": 2.85
    },
    {
        "text": "it's very, very similar to the way that you",
        "start": 942.355,
        "duration": 2.145
    },
    {
        "text": "deploy in Azure ML right now.",
        "start": 944.5,
        "duration": 2.835
    },
    {
        "text": "So what we've introduced here is doing an MLflow Azure ML deploy.",
        "start": 947.335,
        "duration": 5.205
    },
    {
        "text": "So in this case,",
        "start": 952.54,
        "duration": 1.305
    },
    {
        "text": "after the run is completed",
        "start": 953.845,
        "duration": 1.995
    },
    {
        "text": "we're just pointing to the path of the run.",
        "start": 955.84,
        "duration": 1.875
    },
    {
        "text": "Even though it is MLflow track,",
        "start": 957.715,
        "duration": 1.98
    },
    {
        "text": "you don't have to go in and separately register.",
        "start": 959.695,
        "duration": 2.235
    },
    {
        "text": "The MLflow.AzureML deploy will take",
        "start": 961.93,
        "duration": 2.85
    },
    {
        "text": "that model that you saw in the run,",
        "start": 964.78,
        "duration": 3.09
    },
    {
        "text": "let me just show you here.",
        "start": 967.87,
        "duration": 1.89
    },
    {
        "text": "So if we see the outputs here.",
        "start": 969.76,
        "duration": 2.49
    },
    {
        "text": "So very similar, I mean,",
        "start": 972.25,
        "duration": 1.92
    },
    {
        "text": "I'm using this as an example here",
        "start": 974.17,
        "duration": 1.59
    },
    {
        "text": "but it will look the same in PyTorch.",
        "start": 975.76,
        "duration": 1.935
    },
    {
        "text": "You have this models folder.",
        "start": 977.695,
        "duration": 1.665
    },
    {
        "text": "So it'll take the path of this and understand the syntax,",
        "start": 979.36,
        "duration": 7.35
    },
    {
        "text": "the schema of the model here,",
        "start": 986.71,
        "duration": 1.83
    },
    {
        "text": "and then go ahead and deploy that model.",
        "start": 988.54,
        "duration": 3.72
    },
    {
        "text": "So right here, we show you,",
        "start": 992.26,
        "duration": 4.38
    },
    {
        "text": "pop back over here. Let's look at the endpoints.",
        "start": 996.64,
        "duration": 3.18
    },
    {
        "text": "So that one was the PyTorch example,",
        "start": 999.82,
        "duration": 2.31
    },
    {
        "text": "so I can show you the MLflow PyTorch example.",
        "start": 1002.13,
        "duration": 3.99
    },
    {
        "text": "So right here, you can see that I have that model ID,",
        "start": 1006.12,
        "duration": 4.515
    },
    {
        "text": "the model output that was created from that run,",
        "start": 1010.635,
        "duration": 2.655
    },
    {
        "text": "as well as this nice metadata that is being tracked.",
        "start": 1013.29,
        "duration": 3.33
    },
    {
        "text": "So we don't just enable you to track your runs in MLflow,",
        "start": 1016.62,
        "duration": 3.69
    },
    {
        "text": "we want to make sure that you have that view",
        "start": 1020.31,
        "duration": 3.135
    },
    {
        "text": "of the metadata and",
        "start": 1023.445,
        "duration": 1.485
    },
    {
        "text": "the properties that MLflow also tracks on there.",
        "start": 1024.93,
        "duration": 2.265
    },
    {
        "text": "So you'll have the URI,",
        "start": 1027.195,
        "duration": 1.245
    },
    {
        "text": "the runs, data you're using.",
        "start": 1028.44,
        "duration": 3.915
    },
    {
        "text": "This is in the endpoints.",
        "start": 1032.355,
        "duration": 1.905
    },
    {
        "text": "In the models, we do the same exact thing.",
        "start": 1034.26,
        "duration": 1.8
    },
    {
        "text": "You click into let's say that same PyTorch model,",
        "start": 1036.06,
        "duration": 3.06
    },
    {
        "text": "you have all this information already filled for you.",
        "start": 1039.12,
        "duration": 5.25
    },
    {
        "text": ">> So this has been awesome.",
        "start": 1044.37,
        "duration": 1.935
    },
    {
        "text": "You basically showed us how",
        "start": 1046.305,
        "duration": 2.07
    },
    {
        "text": "with Open Source you can create environments.",
        "start": 1048.375,
        "duration": 2.175
    },
    {
        "text": "Well, you create environments",
        "start": 1050.55,
        "duration": 1.32
    },
    {
        "text": "so that open-source can live in them.",
        "start": 1051.87,
        "duration": 1.35
    },
    {
        "text": "We've looked at sklearn in PyTorch,",
        "start": 1053.22,
        "duration": 1.605
    },
    {
        "text": "and there were some other demos in there we'll",
        "start": 1054.825,
        "duration": 1.545
    },
    {
        "text": "put the links below so everyone can see them.",
        "start": 1056.37,
        "duration": 1.935
    },
    {
        "text": "Then you showed us an Open Source framework that looked at",
        "start": 1058.305,
        "duration": 3.24
    },
    {
        "text": "a different aspect of machine learning",
        "start": 1061.545,
        "duration": 1.515
    },
    {
        "text": "that combined multiple things,",
        "start": 1063.06,
        "duration": 1.89
    },
    {
        "text": "not just training runs for example,",
        "start": 1064.95,
        "duration": 1.95
    },
    {
        "text": "and that was ML flow.",
        "start": 1066.9,
        "duration": 1.395
    },
    {
        "text": "That was pretty awesome. So where can people go to find",
        "start": 1068.295,
        "duration": 2.205
    },
    {
        "text": "out more or if they want to learn more about these things?",
        "start": 1070.5,
        "duration": 2.415
    },
    {
        "text": ">> Yeah, we have tonnes of examples",
        "start": 1072.915,
        "duration": 2.055
    },
    {
        "text": "and additional examples that I showed you.",
        "start": 1074.97,
        "duration": 1.8
    },
    {
        "text": "We have some MLflow examples that go a little bit",
        "start": 1076.77,
        "duration": 2.49
    },
    {
        "text": "deeper into [inaudible] or TensorFlow doing auto logging.",
        "start": 1079.26,
        "duration": 4.575
    },
    {
        "text": "I will put the links in the description",
        "start": 1083.835,
        "duration": 1.815
    },
    {
        "text": "below and you can try those out yourselves.",
        "start": 1085.65,
        "duration": 2.595
    },
    {
        "text": ">> Awesome. Well, thanks so much",
        "start": 1088.245,
        "duration": 1.695
    },
    {
        "text": "for spending some time with us, Shivani.",
        "start": 1089.94,
        "duration": 1.35
    },
    {
        "text": "Thank you so much for watching and learning all about",
        "start": 1091.29,
        "duration": 2.46
    },
    {
        "text": "Open Source frameworks inside of Azure Machine Learning Service.",
        "start": 1093.75,
        "duration": 3.39
    },
    {
        "text": "Thank you so much, you have an awesome day.",
        "start": 1097.14,
        "duration": 1.53
    },
    {
        "text": "[MUSIC]",
        "start": 1098.67,
        "duration": 14.33
    }
]