[
    {
        "text": ">> Welcome to this episode of The AI Show where we",
        "start": 0.2,
        "duration": 3.55
    },
    {
        "text": "talk about GreenerAI with Will Buchanan.",
        "start": 3.75,
        "duration": 2.67
    },
    {
        "text": "[MUSIC]",
        "start": 6.42,
        "duration": 8.22
    },
    {
        "text": ">> Hi Will, welcome to the AI Show, and can you",
        "start": 14.64,
        "duration": 2.8
    },
    {
        "text": "introduce yourself and tell us what you do at Microsoft.",
        "start": 17.44,
        "duration": 3.635
    },
    {
        "text": ">> Hi, everyone. My name is Will Buchanan.",
        "start": 21.075,
        "duration": 2.37
    },
    {
        "text": "I'm a PM on the Azure Machine Learning Team,",
        "start": 23.445,
        "duration": 2.385
    },
    {
        "text": "specifically focusing on user experience",
        "start": 25.83,
        "duration": 2.505
    },
    {
        "text": "and decarbonization of AI.",
        "start": 28.335,
        "duration": 2.655
    },
    {
        "text": ">> That is great. What are",
        "start": 30.99,
        "duration": 2.73
    },
    {
        "text": "some of the problems that you're solving?",
        "start": 33.72,
        "duration": 2.83
    },
    {
        "text": ">> I've been working on something called",
        "start": 37.43,
        "duration": 2.33
    },
    {
        "text": "GreenerAI which has become",
        "start": 39.76,
        "duration": 1.6
    },
    {
        "text": "a recent hot topic in the news and it's really",
        "start": 41.36,
        "duration": 3.75
    },
    {
        "text": "specifically focused on providing",
        "start": 45.11,
        "duration": 2.01
    },
    {
        "text": "a set of tactics that data scientists",
        "start": 47.12,
        "duration": 2.1
    },
    {
        "text": "and developers can use to reduce",
        "start": 49.22,
        "duration": 1.74
    },
    {
        "text": "the carbon footprint of their machine learning workloads.",
        "start": 50.96,
        "duration": 3.02
    },
    {
        "text": ">> That is great and very timely indeed.",
        "start": 53.98,
        "duration": 3.275
    },
    {
        "text": ">> It is. We're out of time in fact.",
        "start": 57.255,
        "duration": 3.925
    },
    {
        "text": ">> Can you tell us little bit more about these services,",
        "start": 62.09,
        "duration": 3.46
    },
    {
        "text": "the GreenerAI services, and what we do?",
        "start": 65.55,
        "duration": 2.895
    },
    {
        "text": ">> Certainly. Within Azure Machine Learning",
        "start": 68.445,
        "duration": 4.175
    },
    {
        "text": "we have launched a set of",
        "start": 72.62,
        "duration": 1.485
    },
    {
        "text": "resource metrics which provide transparency into",
        "start": 74.105,
        "duration": 2.445
    },
    {
        "text": "the computational costs as well as",
        "start": 76.55,
        "duration": 1.86
    },
    {
        "text": "the energetic costs of your workloads.",
        "start": 78.41,
        "duration": 2.295
    },
    {
        "text": "Really we're trying to provide transparency",
        "start": 80.705,
        "duration": 2.73
    },
    {
        "text": "to help empower developers to make decisions",
        "start": 83.435,
        "duration": 2.565
    },
    {
        "text": "about when and where they should be running their models",
        "start": 86.0,
        "duration": 4.125
    },
    {
        "text": "and how to optimize or make",
        "start": 90.125,
        "duration": 1.965
    },
    {
        "text": "decisions which would be more environmentally friendly.",
        "start": 92.09,
        "duration": 3.76
    },
    {
        "text": ">> How can developers do that,",
        "start": 96.5,
        "duration": 3.115
    },
    {
        "text": "make more informed decisions?",
        "start": 99.615,
        "duration": 3.03
    },
    {
        "text": ">> The first step is transparency around those metrics.",
        "start": 102.645,
        "duration": 6.48
    },
    {
        "text": "We're providing those metrics to help",
        "start": 109.125,
        "duration": 2.0
    },
    {
        "text": "users understand the costs of the models.",
        "start": 111.125,
        "duration": 2.075
    },
    {
        "text": "But soon we also have a set of",
        "start": 113.2,
        "duration": 1.66
    },
    {
        "text": "recipes or samples that any practitioner can",
        "start": 114.86,
        "duration": 2.76
    },
    {
        "text": "use and start understanding the savings of their workloads.",
        "start": 117.62,
        "duration": 5.2
    },
    {
        "text": "I can walk through that in a second.",
        "start": 122.82,
        "duration": 2.79
    },
    {
        "text": ">> Great. We would like to see it.",
        "start": 125.61,
        "duration": 3.585
    },
    {
        "text": ">> Sure. Before I dive in I will give a bit of context.",
        "start": 129.195,
        "duration": 4.945
    },
    {
        "text": ">> Go ahead.",
        "start": 134.51,
        "duration": 2.48
    },
    {
        "text": ">> I've been leading an initiative called GreenerAI at",
        "start": 137.66,
        "duration": 3.81
    },
    {
        "text": "Microsoft and it has about 200 people in it across the company,",
        "start": 141.47,
        "duration": 3.495
    },
    {
        "text": "and the reason we're doing this is",
        "start": 144.965,
        "duration": 2.265
    },
    {
        "text": "because you really can't manage what you can't measure.",
        "start": 147.23,
        "duration": 2.835
    },
    {
        "text": "We're showing a set of early research and",
        "start": 150.065,
        "duration": 2.265
    },
    {
        "text": "innovation that's happening across Microsoft.",
        "start": 152.33,
        "duration": 2.735
    },
    {
        "text": "We're going to talk about some new capabilities",
        "start": 155.065,
        "duration": 2.44
    },
    {
        "text": "as well as a few different use cases and",
        "start": 157.505,
        "duration": 2.235
    },
    {
        "text": "validation tactics and then a few tips on getting started.",
        "start": 159.74,
        "duration": 3.88
    },
    {
        "text": "In terms of where we are today,",
        "start": 164.08,
        "duration": 3.095
    },
    {
        "text": "we're in a state of something known as",
        "start": 167.175,
        "duration": 1.655
    },
    {
        "text": "red AI and we've created a computational mega fauna.",
        "start": 168.83,
        "duration": 3.345
    },
    {
        "text": "You have state of the art approaches that are buying",
        "start": 172.175,
        "duration": 2.28
    },
    {
        "text": "incrementally better results with high hidden costs.",
        "start": 174.455,
        "duration": 3.75
    },
    {
        "text": "Computational costs have increased in machine learning about",
        "start": 178.205,
        "duration": 2.745
    },
    {
        "text": "300,000 times from 2012-2018.",
        "start": 180.95,
        "duration": 3.315
    },
    {
        "text": "According to an article by Wired",
        "start": 184.265,
        "duration": 1.83
    },
    {
        "text": "only about 11 percent of firms are",
        "start": 186.095,
        "duration": 2.055
    },
    {
        "text": "seeing what can be called a significant",
        "start": 188.15,
        "duration": 1.845
    },
    {
        "text": "return on investment for their workloads.",
        "start": 189.995,
        "duration": 2.485
    },
    {
        "text": "It's pretty shocking when you think about that.",
        "start": 192.48,
        "duration": 2.135
    },
    {
        "text": "Some recent studies have shown that",
        "start": 194.615,
        "duration": 1.725
    },
    {
        "text": "the GPT3 training emits as much carbon",
        "start": 196.34,
        "duration": 3.54
    },
    {
        "text": "as three round-trip transcontinental flights",
        "start": 199.88,
        "duration": 3.75
    },
    {
        "text": "from San Francisco to New York.",
        "start": 203.63,
        "duration": 1.53
    },
    {
        "text": "Emma Strubell about a year or two",
        "start": 205.16,
        "duration": 2.52
    },
    {
        "text": "ago also published a seminal paper which",
        "start": 207.68,
        "duration": 3.06
    },
    {
        "text": "compared a transformer model with",
        "start": 210.74,
        "duration": 2.655
    },
    {
        "text": "a round-trip flight or a human life or US car,",
        "start": 213.395,
        "duration": 4.425
    },
    {
        "text": "so 213 million parameter transformer model training",
        "start": 217.82,
        "duration": 4.455
    },
    {
        "text": "consumes as much carbon as five cars including gas.",
        "start": 222.275,
        "duration": 5.035
    },
    {
        "text": "As you can see in this chart,",
        "start": 228.8,
        "duration": 2.29
    },
    {
        "text": "we have surpassed what",
        "start": 231.09,
        "duration": 3.32
    },
    {
        "text": "could be seen as Moore's law and we're in more of",
        "start": 234.41,
        "duration": 2.16
    },
    {
        "text": "the extreme modern era of",
        "start": 236.57,
        "duration": 2.88
    },
    {
        "text": "computation where it's doubling about every 3.4 months.",
        "start": 239.45,
        "duration": 4.48
    },
    {
        "text": "The AI market is going to grow",
        "start": 244.28,
        "duration": 2.44
    },
    {
        "text": "about 44 percent annually through 2025.",
        "start": 246.72,
        "duration": 3.855
    },
    {
        "text": "We believe that there will be a point of",
        "start": 250.575,
        "duration": 1.955
    },
    {
        "text": "inflection where there are diminishing returns where",
        "start": 252.53,
        "duration": 2.28
    },
    {
        "text": "the marginal cost-benefit ratio",
        "start": 254.81,
        "duration": 2.295
    },
    {
        "text": "are no longer worth the additional investment.",
        "start": 257.105,
        "duration": 2.935
    },
    {
        "text": "A bit of background on AI pipelines.",
        "start": 261.46,
        "duration": 3.56
    },
    {
        "text": "It consist of both training and inference.",
        "start": 265.02,
        "duration": 2.28
    },
    {
        "text": "In training you're teaching a model of how to make",
        "start": 267.3,
        "duration": 2.03
    },
    {
        "text": "predictions based on existing data and",
        "start": 269.33,
        "duration": 2.1
    },
    {
        "text": "generally you're doing long runs and a few of",
        "start": 271.43,
        "duration": 2.04
    },
    {
        "text": "them and then when you go to scale you",
        "start": 273.47,
        "duration": 2.64
    },
    {
        "text": "infer your model and you are applying",
        "start": 276.11,
        "duration": 2.04
    },
    {
        "text": "this trained model to make predictions",
        "start": 278.15,
        "duration": 1.53
    },
    {
        "text": "from data that it has not seen before.",
        "start": 279.68,
        "duration": 1.86
    },
    {
        "text": "Generally, it has short runs and lots of them.",
        "start": 281.54,
        "duration": 3.035
    },
    {
        "text": "Emergently, there's specialized hardware such as GPU which is",
        "start": 284.575,
        "duration": 4.63
    },
    {
        "text": "optimizing your workloads there and they're often power hungry,",
        "start": 289.205,
        "duration": 4.68
    },
    {
        "text": "consuming about 300 watts on average.",
        "start": 293.885,
        "duration": 3.145
    },
    {
        "text": "As I mentioned earlier it's pretty hot topic",
        "start": 297.92,
        "duration": 2.78
    },
    {
        "text": "in the media these days.",
        "start": 300.7,
        "duration": 1.71
    },
    {
        "text": "I had a chance to chat with Timnit Gebru who",
        "start": 302.41,
        "duration": 2.805
    },
    {
        "text": "published this stochastic parrots paper",
        "start": 305.215,
        "duration": 1.845
    },
    {
        "text": "and she's really saying how it is",
        "start": 307.06,
        "duration": 1.41
    },
    {
        "text": "past time for researchers to prioritize",
        "start": 308.47,
        "duration": 2.22
    },
    {
        "text": "energy efficiency and costs so",
        "start": 310.69,
        "duration": 2.4
    },
    {
        "text": "that we can reduce environmental impact and",
        "start": 313.09,
        "duration": 2.19
    },
    {
        "text": "increase equitable access to resources.",
        "start": 315.28,
        "duration": 2.825
    },
    {
        "text": "You can think of the AI industry in",
        "start": 318.105,
        "duration": 1.915
    },
    {
        "text": "some ways as similar to the oil industry.",
        "start": 320.02,
        "duration": 2.16
    },
    {
        "text": "Once you mine and refine your data,",
        "start": 322.18,
        "duration": 2.31
    },
    {
        "text": "it can be highly lucrative and it seems that",
        "start": 324.49,
        "duration": 1.77
    },
    {
        "text": "this metaphor could extend even further.",
        "start": 326.26,
        "duration": 2.88
    },
    {
        "text": "The way that we're addressing this is",
        "start": 329.15,
        "duration": 2.36
    },
    {
        "text": "through a cost metric framework.",
        "start": 331.51,
        "duration": 2.02
    },
    {
        "text": "In terms of the costs we want to expose both monetary,",
        "start": 333.53,
        "duration": 3.96
    },
    {
        "text": "the computational runtime,",
        "start": 337.49,
        "duration": 1.59
    },
    {
        "text": "the energetic cost,",
        "start": 339.08,
        "duration": 1.2
    },
    {
        "text": "as well as utilization for both training and inference.",
        "start": 340.28,
        "duration": 4.49
    },
    {
        "text": "It's important with training because only about 12 percent of",
        "start": 344.77,
        "duration": 3.25
    },
    {
        "text": "the models that you're making actually make it into production.",
        "start": 348.02,
        "duration": 3.34
    },
    {
        "text": "For inference according to an article by NVIDIA,",
        "start": 351.36,
        "duration": 2.87
    },
    {
        "text": "it accounts for about 80-90 percent of the carbon cost of model.",
        "start": 354.23,
        "duration": 3.78
    },
    {
        "text": "The vision that we're working towards is",
        "start": 358.01,
        "duration": 2.13
    },
    {
        "text": "an operational lifecycle analysis monitoring toolkit.",
        "start": 360.14,
        "duration": 3.425
    },
    {
        "text": "You can assess these different cost frameworks",
        "start": 363.565,
        "duration": 4.21
    },
    {
        "text": "for both training and inference and",
        "start": 367.775,
        "duration": 1.575
    },
    {
        "text": "then set a set of triggers or",
        "start": 369.35,
        "duration": 2.52
    },
    {
        "text": "tools you can start to optimize your return on investment.",
        "start": 371.87,
        "duration": 4.09
    },
    {
        "text": "The first capability that we've done,",
        "start": 376.7,
        "duration": 2.505
    },
    {
        "text": "you can see in Azure Machine Learning Studio.",
        "start": 379.205,
        "duration": 2.66
    },
    {
        "text": "In this interface, you can add an opt-in metric to",
        "start": 381.865,
        "duration": 2.895
    },
    {
        "text": "see the GPU energy cost for your runs.",
        "start": 384.76,
        "duration": 2.96
    },
    {
        "text": "This helps you find the most energetic culprits",
        "start": 387.72,
        "duration": 2.784
    },
    {
        "text": "and then you can discover which ones were the most expensive.",
        "start": 390.504,
        "duration": 3.386
    },
    {
        "text": "We're also offering the same capabilities for",
        "start": 393.89,
        "duration": 2.27
    },
    {
        "text": "both utilization GPU and memory.",
        "start": 396.16,
        "duration": 3.25
    },
    {
        "text": "Next, you can see",
        "start": 403.28,
        "duration": 2.57
    },
    {
        "text": "some additional screenshots in",
        "start": 405.85,
        "duration": 1.32
    },
    {
        "text": "the monitoring tab which is in preview.",
        "start": 407.17,
        "duration": 2.13
    },
    {
        "text": "You can view a real-time plot of your GPU energy usage broken",
        "start": 409.3,
        "duration": 3.6
    },
    {
        "text": "down by node and you can also see CPU utilization,",
        "start": 412.9,
        "duration": 3.825
    },
    {
        "text": "GPU utilization, and memory,",
        "start": 416.725,
        "duration": 2.07
    },
    {
        "text": "and of course energy.",
        "start": 418.795,
        "duration": 1.9
    },
    {
        "text": "We've done a study where you perform a batched inference run",
        "start": 420.695,
        "duration": 4.185
    },
    {
        "text": "and we calculated the cost",
        "start": 424.88,
        "duration": 3.03
    },
    {
        "text": "per million adults which is a standard framework for inference.",
        "start": 427.91,
        "duration": 2.61
    },
    {
        "text": "It consumes about 13.8-kilowatt",
        "start": 430.52,
        "duration": 3.09
    },
    {
        "text": "hours and that's about half",
        "start": 433.61,
        "duration": 2.49
    },
    {
        "text": "of a US home's daily energy consumption.",
        "start": 436.1,
        "duration": 2.16
    },
    {
        "text": ">> A lot.",
        "start": 438.26,
        "duration": 1.08
    },
    {
        "text": ">> It's crazy.",
        "start": 439.34,
        "duration": 1.44
    },
    {
        "text": ">> It is.",
        "start": 440.78,
        "duration": 1.085
    },
    {
        "text": ">> If you were to convert that to greenhouse gas equivalencies,",
        "start": 441.865,
        "duration": 3.715
    },
    {
        "text": "they'd be about the same as 24 miles driven or",
        "start": 445.58,
        "duration": 2.7
    },
    {
        "text": "11 pounds of coal or a gallon of gas.",
        "start": 448.28,
        "duration": 2.55
    },
    {
        "text": "You can imagine this really adds up over time.",
        "start": 450.83,
        "duration": 2.87
    },
    {
        "text": "The analysis we did was for",
        "start": 453.7,
        "duration": 1.9
    },
    {
        "text": "the BERT model using a Tensor-flow framework on a V100",
        "start": 455.6,
        "duration": 3.09
    },
    {
        "text": "with a 32 floating-point precision and batch size of 16.",
        "start": 458.69,
        "duration": 5.445
    },
    {
        "text": "We ran it for 2.5 hours and we",
        "start": 464.135,
        "duration": 1.905
    },
    {
        "text": "calculated the throughput as well as",
        "start": 466.04,
        "duration": 2.25
    },
    {
        "text": "the energy cost and then we extrapolated that",
        "start": 468.29,
        "duration": 2.19
    },
    {
        "text": "for a million inferences.",
        "start": 470.48,
        "duration": 3.37
    },
    {
        "text": "As part of our work we've been working",
        "start": 475.94,
        "duration": 2.49
    },
    {
        "text": "with NVIDIA on something called",
        "start": 478.43,
        "duration": 1.68
    },
    {
        "text": "the Triton Inference Server and this is",
        "start": 480.11,
        "duration": 1.89
    },
    {
        "text": "specifically designed for high throughput inference workloads.",
        "start": 482.0,
        "duration": 3.39
    },
    {
        "text": "It comes with a number of benefits",
        "start": 485.39,
        "duration": 2.07
    },
    {
        "text": "such as model concurrency where you can load",
        "start": 487.46,
        "duration": 2.31
    },
    {
        "text": "multiple models or copies of that model",
        "start": 489.77,
        "duration": 2.205
    },
    {
        "text": "onto a GPU and then execute them simultaneously.",
        "start": 491.975,
        "duration": 3.6
    },
    {
        "text": "Another exciting feature is dynamic batching where you",
        "start": 495.575,
        "duration": 2.595
    },
    {
        "text": "can dynamically group together",
        "start": 498.17,
        "duration": 2.76
    },
    {
        "text": "your requests on the server-side to maximize",
        "start": 500.93,
        "duration": 2.37
    },
    {
        "text": "performance and mixed weights precision.",
        "start": 503.3,
        "duration": 3.625
    },
    {
        "text": "Due to the vanishing gradients phenomenon during your training,",
        "start": 506.925,
        "duration": 3.5
    },
    {
        "text": "a backward pass,",
        "start": 510.425,
        "duration": 1.275
    },
    {
        "text": "your models you typically trained using",
        "start": 511.7,
        "duration": 1.53
    },
    {
        "text": "floating point 32 precision.",
        "start": 513.23,
        "duration": 2.32
    },
    {
        "text": "However, given for inference, only the forward pass is necessary,",
        "start": 515.55,
        "duration": 3.85
    },
    {
        "text": "you can use a lower precision such as",
        "start": 519.4,
        "duration": 2.11
    },
    {
        "text": "16 and Triton allows use of both of these.",
        "start": 521.51,
        "duration": 3.6
    },
    {
        "text": "On a similar analysis, we found that if you",
        "start": 525.11,
        "duration": 3.09
    },
    {
        "text": "use mixed weights precision you can cut your costs by",
        "start": 528.2,
        "duration": 2.76
    },
    {
        "text": "a greater than 50 percent just by switching from",
        "start": 530.96,
        "duration": 2.51
    },
    {
        "text": "floating point 32-16 using the Triton Inference Server.",
        "start": 533.47,
        "duration": 4.025
    },
    {
        "text": "You can see the cost per",
        "start": 537.495,
        "duration": 2.285
    },
    {
        "text": "million inference is significantly less, it's almost a third,",
        "start": 539.78,
        "duration": 3.03
    },
    {
        "text": "GPU power consumption is from about 75-60 watts.",
        "start": 542.81,
        "duration": 6.85
    },
    {
        "text": ">> This is great.",
        "start": 550.04,
        "duration": 2.75
    },
    {
        "text": ">> Something else we've also been working",
        "start": 554.33,
        "duration": 2.34
    },
    {
        "text": "on and will soon be releasing,",
        "start": 556.67,
        "duration": 1.89
    },
    {
        "text": "there's work with the NVIDIA Model Analyzer.",
        "start": 558.56,
        "duration": 3.21
    },
    {
        "text": "You can analyze and develop the optimal configurations for",
        "start": 561.77,
        "duration": 5.1
    },
    {
        "text": "your model and we also charted",
        "start": 566.87,
        "duration": 2.28
    },
    {
        "text": "the relationship between power efficiency and GPU utilization.",
        "start": 569.15,
        "duration": 4.035
    },
    {
        "text": "If you use Model Analyzer to optimize",
        "start": 573.185,
        "duration": 3.525
    },
    {
        "text": "your configuration you can reduce your cost per",
        "start": 576.71,
        "duration": 2.07
    },
    {
        "text": "million by about 12.6X.",
        "start": 578.78,
        "duration": 2.73
    },
    {
        "text": "Here you see the chart with the baseline there.",
        "start": 581.51,
        "duration": 2.34
    },
    {
        "text": "The baseline is compared against ORT and CUDA as the backend",
        "start": 583.85,
        "duration": 4.02
    },
    {
        "text": "versus TRT backend optimized by Model Analyzer.",
        "start": 587.87,
        "duration": 4.795
    },
    {
        "text": "Using the Model Analyzer can automatically maximize",
        "start": 592.665,
        "duration": 3.395
    },
    {
        "text": "both your GPU utilization and",
        "start": 596.06,
        "duration": 1.77
    },
    {
        "text": "your throughput and consequently save energy.",
        "start": 597.83,
        "duration": 3.69
    },
    {
        "text": "Finally, an additional case study that we've done.",
        "start": 605.24,
        "duration": 3.78
    },
    {
        "text": "For image classification scenario for training,",
        "start": 609.02,
        "duration": 4.02
    },
    {
        "text": "we compared two different convolutional neural networks",
        "start": 613.04,
        "duration": 2.76
    },
    {
        "text": ", InceptionV3 and DenseNet.",
        "start": 615.8,
        "duration": 2.115
    },
    {
        "text": "Both of these classify images with",
        "start": 617.915,
        "duration": 2.025
    },
    {
        "text": "greater than 75 percent accuracy on your ImageNet dataset.",
        "start": 619.94,
        "duration": 4.175
    },
    {
        "text": "We compare these two models for both energy usage",
        "start": 624.115,
        "duration": 2.785
    },
    {
        "text": "and cost savings to help determine which models",
        "start": 626.9,
        "duration": 2.805
    },
    {
        "text": "give you the results you need at",
        "start": 629.705,
        "duration": 1.515
    },
    {
        "text": "the lowest costs both monetary and environmental.",
        "start": 631.22,
        "duration": 4.13
    },
    {
        "text": "We found that InceptionV3 outperforms",
        "start": 635.35,
        "duration": 3.08
    },
    {
        "text": "DenseNet in terms of accuracy coming in about 10 percent higher.",
        "start": 638.43,
        "duration": 3.725
    },
    {
        "text": "Also cost about 13 percent less and uses",
        "start": 642.155,
        "duration": 2.685
    },
    {
        "text": "20 percent less energy and about 10 percent less training time.",
        "start": 644.84,
        "duration": 4.15
    },
    {
        "text": "Really key to this and this is the foundation of",
        "start": 648.99,
        "duration": 2.21
    },
    {
        "text": "the tactic is choose the right tool for the job.",
        "start": 651.2,
        "duration": 2.57
    },
    {
        "text": "Start to think about the networks that you are",
        "start": 653.77,
        "duration": 2.29
    },
    {
        "text": "choosing and the cost that incurs for your business.",
        "start": 656.06,
        "duration": 4.36
    },
    {
        "text": ">> You're not only saving the environment",
        "start": 662.03,
        "duration": 2.88
    },
    {
        "text": "but you're also paying less which is great.",
        "start": 664.91,
        "duration": 2.7
    },
    {
        "text": ">> Yeah, I agree. It's the real business case for this work.",
        "start": 667.61,
        "duration": 4.93
    },
    {
        "text": "We're just getting started doing GreenerAI and we hope",
        "start": 672.65,
        "duration": 3.6
    },
    {
        "text": "to bring additional people in to work on this.",
        "start": 676.25,
        "duration": 3.285
    },
    {
        "text": "We'd like to work with any customers to define what",
        "start": 679.535,
        "duration": 2.835
    },
    {
        "text": "carbon awareness and energy efficiency",
        "start": 682.37,
        "duration": 1.8
    },
    {
        "text": "might need for their organization.",
        "start": 684.17,
        "duration": 2.175
    },
    {
        "text": "Specifically we'd like to point you to",
        "start": 686.345,
        "duration": 1.725
    },
    {
        "text": "the Triton Inference Server to get started doing that.",
        "start": 688.07,
        "duration": 3.035
    },
    {
        "text": "Also, we will share some recipes and samples",
        "start": 691.105,
        "duration": 2.755
    },
    {
        "text": "to help compare these cost-benefit trade-offs.",
        "start": 693.86,
        "duration": 2.685
    },
    {
        "text": "I did want to throw in a plug for",
        "start": 696.545,
        "duration": 1.695
    },
    {
        "text": "the Green Software Foundation where we're starting to define",
        "start": 698.24,
        "duration": 2.82
    },
    {
        "text": "this concept called a software carbon intensity specification",
        "start": 701.06,
        "duration": 4.59
    },
    {
        "text": "and that is currently the Alpha version is live.",
        "start": 705.65,
        "duration": 2.445
    },
    {
        "text": "If you look at Green Software Foundation on GitHub,",
        "start": 708.095,
        "duration": 2.505
    },
    {
        "text": "you will see some really exciting work there.",
        "start": 710.6,
        "duration": 3.01
    },
    {
        "text": "That concludes my presentation.",
        "start": 713.81,
        "duration": 4.25
    },
    {
        "text": ">> Great. Thank you so much.",
        "start": 718.16,
        "duration": 2.14
    },
    {
        "text": "This is really exciting.",
        "start": 720.3,
        "duration": 1.935
    },
    {
        "text": "Where else people can learn more about GreenerAI?",
        "start": 722.235,
        "duration": 4.225
    },
    {
        "text": ">> We have a blog on the tech community",
        "start": 727.04,
        "duration": 3.21
    },
    {
        "text": "which outlines some of this that I showed here,",
        "start": 730.25,
        "duration": 3.195
    },
    {
        "text": "specifically the resource metrics.",
        "start": 733.445,
        "duration": 2.995
    },
    {
        "text": "Also, I believe we're posting",
        "start": 737.24,
        "duration": 2.64
    },
    {
        "text": "the link to the Triton Inference Server.",
        "start": 739.88,
        "duration": 2.685
    },
    {
        "text": "I think that's the best starting point,",
        "start": 742.565,
        "duration": 2.055
    },
    {
        "text": "but stay tuned we'll have a lot more exciting news soon,",
        "start": 744.62,
        "duration": 3.15
    },
    {
        "text": "especially with the Green Software Foundation.",
        "start": 747.77,
        "duration": 2.3
    },
    {
        "text": ">> Thank you for joining Will,",
        "start": 750.07,
        "duration": 1.91
    },
    {
        "text": "and thank you for telling us about the GreenerAI.",
        "start": 751.98,
        "duration": 3.33
    },
    {
        "text": "We'll see you next time.",
        "start": 755.31,
        "duration": 1.53
    },
    {
        "text": "[MUSIC]",
        "start": 756.84,
        "duration": 8.17
    }
]