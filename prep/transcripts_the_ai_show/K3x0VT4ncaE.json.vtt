[
    {
        "text": ">> You're not going to want to miss",
        "start": 0.0,
        "duration": 1.29
    },
    {
        "text": "this episode of The AI show where we talk",
        "start": 1.29,
        "duration": 1.66
    },
    {
        "text": "about combining OpenAI models with the power of Azure.",
        "start": 2.95,
        "duration": 3.75
    },
    {
        "text": "New from build.",
        "start": 6.7,
        "duration": 1.245
    },
    {
        "text": "Make sure you tune in.",
        "start": 7.945,
        "duration": 1.035
    },
    {
        "text": ">> Hello and welcome to this episode of The AI Show,",
        "start": 8.98,
        "duration": 11.1
    },
    {
        "text": "where we're going to talk about combining",
        "start": 20.08,
        "duration": 1.31
    },
    {
        "text": "OpenAI models with the power of Azure with Chris.",
        "start": 21.39,
        "duration": 2.425
    },
    {
        "text": "My friend, how are you?",
        "start": 23.815,
        "duration": 1.885
    },
    {
        "text": ">> I'm doing great. Yeah, thanks for having me.",
        "start": 25.7,
        "duration": 2.095
    },
    {
        "text": ">> Fantastic. So tell us who you are and what you do.",
        "start": 27.795,
        "duration": 2.23
    },
    {
        "text": ">> Yeah. I'm Chris Hoder.",
        "start": 30.025,
        "duration": 1.305
    },
    {
        "text": "I am the Lead product manager on the Azure OpenAI Service,",
        "start": 31.33,
        "duration": 3.18
    },
    {
        "text": "and I'm excited here to talk about what we are building.",
        "start": 34.51,
        "duration": 3.095
    },
    {
        "text": ">> Let's talk about the Azure OpenAI service.",
        "start": 37.605,
        "duration": 4.18
    },
    {
        "text": "People probably have heard of OpenAI,",
        "start": 41.785,
        "duration": 1.905
    },
    {
        "text": "but now we're putting it together with Azure.",
        "start": 43.69,
        "duration": 1.83
    },
    {
        "text": "Can you tell us about what we announced at Build?",
        "start": 45.52,
        "duration": 1.81
    },
    {
        "text": ">> Yeah. This week at Build, we are really excited to announce",
        "start": 47.33,
        "duration": 3.26
    },
    {
        "text": "the limited access public preview of the Azure OpenAI Service.",
        "start": 50.59,
        "duration": 3.375
    },
    {
        "text": "What that means, is customers out there can",
        "start": 53.965,
        "duration": 1.875
    },
    {
        "text": "start to apply for access,",
        "start": 55.84,
        "duration": 1.68
    },
    {
        "text": "to be able to use OpenAI's models in Azure today",
        "start": 57.52,
        "duration": 4.55
    },
    {
        "text": "to use that for a huge number of language use cases.",
        "start": 62.07,
        "duration": 4.04
    },
    {
        "text": ">> That's really cool. But they could have used that in OpenAI,",
        "start": 66.11,
        "duration": 3.71
    },
    {
        "text": "what makes it doing it in Azure even better?",
        "start": 69.82,
        "duration": 4.065
    },
    {
        "text": ">> Yeah. What makes it even better in Azure,",
        "start": 73.885,
        "duration": 3.255
    },
    {
        "text": "is that we provide a lot of what is",
        "start": 77.14,
        "duration": 1.89
    },
    {
        "text": "often thought of as just the core goodness of Azure.",
        "start": 79.03,
        "duration": 2.34
    },
    {
        "text": "We have security with Azure",
        "start": 81.37,
        "duration": 2.73
    },
    {
        "text": "Active Directory based authentication for managed identities.",
        "start": 84.1,
        "duration": 3.315
    },
    {
        "text": "You can do private networking with",
        "start": 87.415,
        "duration": 1.755
    },
    {
        "text": "virtual networks and private links,",
        "start": 89.17,
        "duration": 1.86
    },
    {
        "text": "and a whole swath of other features that really empower",
        "start": 91.03,
        "duration": 3.0
    },
    {
        "text": "enterprises to take advantage",
        "start": 94.03,
        "duration": 1.32
    },
    {
        "text": "of this tech in a really secure way.",
        "start": 95.35,
        "duration": 2.65
    },
    {
        "text": ">> Interesting. If you want to try all the latest and",
        "start": 98.0,
        "duration": 2.93
    },
    {
        "text": "greatest OpenAI, still go there.",
        "start": 100.93,
        "duration": 2.85
    },
    {
        "text": "But if you're trying to put it into an enterprise setting,",
        "start": 103.78,
        "duration": 2.91
    },
    {
        "text": "you want to do it with Azure. Am I getting this right?",
        "start": 106.69,
        "duration": 1.98
    },
    {
        "text": ">> Yeah, exactly.",
        "start": 108.67,
        "duration": 1.32
    },
    {
        "text": "That's a great way to think about it.",
        "start": 109.99,
        "duration": 1.21
    },
    {
        "text": ">> Fantastic. I was hoping you could",
        "start": 111.2,
        "duration": 1.82
    },
    {
        "text": "show us a little bit about how it works.",
        "start": 113.02,
        "duration": 1.95
    },
    {
        "text": ">> Yeah, definitely. Let's hop into a quick demo.",
        "start": 114.97,
        "duration": 2.5
    },
    {
        "text": ">> All right, let's do it.",
        "start": 117.47,
        "duration": 1.09
    },
    {
        "text": ">> What you're seeing here is what we",
        "start": 118.56,
        "duration": 1.6
    },
    {
        "text": "call the Azure OpenAI Studio,",
        "start": 120.16,
        "duration": 1.5
    },
    {
        "text": "and this is our new UI that lets users",
        "start": 121.66,
        "duration": 2.43
    },
    {
        "text": "quickly iterate and experiment with their use case.",
        "start": 124.09,
        "duration": 2.43
    },
    {
        "text": "That becomes really important with",
        "start": 126.52,
        "duration": 1.53
    },
    {
        "text": "something like OpenAI's models,",
        "start": 128.05,
        "duration": 1.74
    },
    {
        "text": "because they are super flexible.",
        "start": 129.79,
        "duration": 2.1
    },
    {
        "text": "I want to show you a little bit about how flexible they are.",
        "start": 131.89,
        "duration": 3.63
    },
    {
        "text": "What we have here is a simple text interface",
        "start": 135.52,
        "duration": 2.43
    },
    {
        "text": "where I can copy in text, and call it against the model.",
        "start": 137.95,
        "duration": 4.105
    },
    {
        "text": "What's unique about these types of launch language models,",
        "start": 142.055,
        "duration": 3.755
    },
    {
        "text": "is you actually define the task with what you",
        "start": 145.81,
        "duration": 2.16
    },
    {
        "text": "put into the prompt and send to the model to generate text.",
        "start": 147.97,
        "duration": 2.635
    },
    {
        "text": "In this example that I'm showing, first,",
        "start": 150.605,
        "duration": 1.885
    },
    {
        "text": "I have an insurance, an e-mail.",
        "start": 152.49,
        "duration": 2.64
    },
    {
        "text": "You can imagine you're an insurance company,",
        "start": 155.13,
        "duration": 2.5
    },
    {
        "text": "and you want to understand what's being",
        "start": 157.63,
        "duration": 2.34
    },
    {
        "text": "said in e-mails and other communications with your customers.",
        "start": 159.97,
        "duration": 2.66
    },
    {
        "text": "We have a fictitious e-mail here,",
        "start": 162.63,
        "duration": 1.54
    },
    {
        "text": "where somebody is e-mailing about",
        "start": 164.17,
        "duration": 1.47
    },
    {
        "text": "an accident their son had with another driver.",
        "start": 165.64,
        "duration": 2.02
    },
    {
        "text": "You can imagine you want to",
        "start": 167.66,
        "duration": 1.37
    },
    {
        "text": "start to ask questions on this data-set,",
        "start": 169.03,
        "duration": 1.53
    },
    {
        "text": "such as who was involved in the accident?",
        "start": 170.56,
        "duration": 1.5
    },
    {
        "text": "Can I extract the names?",
        "start": 172.06,
        "duration": 1.545
    },
    {
        "text": "All I have to do here is ask who are",
        "start": 173.605,
        "duration": 2.055
    },
    {
        "text": "the names of the people involved in the accident?",
        "start": 175.66,
        "duration": 1.735
    },
    {
        "text": "You can see that the service analyzes",
        "start": 177.395,
        "duration": 2.585
    },
    {
        "text": "that text and generated the correct answer with Jacob and Marina,",
        "start": 179.98,
        "duration": 3.27
    },
    {
        "text": "which if you read through this, son that's driver is Jacob,",
        "start": 183.25,
        "duration": 3.435
    },
    {
        "text": "the other driver is named Marina.",
        "start": 186.685,
        "duration": 2.1
    },
    {
        "text": "It's really just that easy.",
        "start": 188.785,
        "duration": 1.87
    },
    {
        "text": ">> That's amazing. It's amazing for me,",
        "start": 190.655,
        "duration": 2.795
    },
    {
        "text": "because when I was studying Machine Learning,",
        "start": 193.45,
        "duration": 2.355
    },
    {
        "text": "my first NLP task was extracting entities.",
        "start": 195.805,
        "duration": 5.215
    },
    {
        "text": "Just that the model does all of it is really cool.",
        "start": 201.02,
        "duration": 3.89
    },
    {
        "text": ">> Yeah, it's fantastic.",
        "start": 204.91,
        "duration": 1.9
    },
    {
        "text": "It can save a ton of time to",
        "start": 206.81,
        "duration": 1.26
    },
    {
        "text": "really get started and iterate quickly,",
        "start": 208.07,
        "duration": 1.65
    },
    {
        "text": "and to show you even how powerful it is.",
        "start": 209.72,
        "duration": 2.805
    },
    {
        "text": "Next, I want to use the same model,",
        "start": 212.525,
        "duration": 1.695
    },
    {
        "text": "same interface, and do a different task.",
        "start": 214.22,
        "duration": 2.435
    },
    {
        "text": "Think about extracting entities.",
        "start": 216.655,
        "duration": 1.975
    },
    {
        "text": "Sometimes you want to extract like this example,",
        "start": 218.63,
        "duration": 2.205
    },
    {
        "text": "but other times you want that in a structured format.",
        "start": 220.835,
        "duration": 2.115
    },
    {
        "text": "Here, I have a set of text that",
        "start": 222.95,
        "duration": 1.77
    },
    {
        "text": "describes a new planet called Goocrux,",
        "start": 224.72,
        "duration": 3.57
    },
    {
        "text": "and some of the fruits that are available.",
        "start": 228.29,
        "duration": 1.74
    },
    {
        "text": ">> I've been there and it's wonderful.",
        "start": 230.03,
        "duration": 1.58
    },
    {
        "text": ">> Wonderful place. If you click \"Generate\",",
        "start": 231.61,
        "duration": 2.44
    },
    {
        "text": "you can see that it starts to create",
        "start": 234.05,
        "duration": 1.92
    },
    {
        "text": "a table summarizing the fruits from Goocrux",
        "start": 235.97,
        "duration": 2.13
    },
    {
        "text": "in a structured format.",
        "start": 238.1,
        "duration": 1.37
    },
    {
        "text": "I'll just run that one more time, so people can see.",
        "start": 239.47,
        "duration": 3.16
    },
    {
        "text": "I gave it a couple of examples.",
        "start": 242.63,
        "duration": 1.875
    },
    {
        "text": "I click \"Generate\" and it",
        "start": 244.505,
        "duration": 2.085
    },
    {
        "text": "understands that structure and start to pulls it out.",
        "start": 246.59,
        "duration": 3.315
    },
    {
        "text": "You can see loopnovas, neon pink, cotton candy-like,",
        "start": 249.905,
        "duration": 3.93
    },
    {
        "text": "there's plenty of loopnovas which are",
        "start": 253.835,
        "duration": 1.785
    },
    {
        "text": "neon pink flavor tastes like cotton candy.",
        "start": 255.62,
        "duration": 2.78
    },
    {
        "text": ">> That's really cool.",
        "start": 258.4,
        "duration": 1.87
    },
    {
        "text": "Even things that may seem nonsensical,",
        "start": 260.27,
        "duration": 3.28
    },
    {
        "text": "it still makes a little bit of sense on it.",
        "start": 263.55,
        "duration": 2.67
    },
    {
        "text": ">> Yes, exactly. Yeah. We can go even further.",
        "start": 266.22,
        "duration": 4.095
    },
    {
        "text": "We've shown structured texts.",
        "start": 270.315,
        "duration": 1.245
    },
    {
        "text": "We've shown asking questions.",
        "start": 271.56,
        "duration": 1.74
    },
    {
        "text": "It can also do more classical tasks like classification.",
        "start": 273.3,
        "duration": 4.155
    },
    {
        "text": "A simple example here,",
        "start": 277.455,
        "duration": 1.935
    },
    {
        "text": "we can classify the following article",
        "start": 279.39,
        "duration": 2.07
    },
    {
        "text": "into one of these categories.",
        "start": 281.46,
        "duration": 1.08
    },
    {
        "text": "I'm defining these categories at run-time.",
        "start": 282.54,
        "duration": 2.46
    },
    {
        "text": "It's dynamic classification that's",
        "start": 285.0,
        "duration": 1.62
    },
    {
        "text": "not going through that historic training loop,",
        "start": 286.62,
        "duration": 2.28
    },
    {
        "text": "passing the news article,",
        "start": 288.9,
        "duration": 1.65
    },
    {
        "text": "and just click \"Generate\" and it'll come",
        "start": 290.55,
        "duration": 1.77
    },
    {
        "text": "back with the correct classification.",
        "start": 292.32,
        "duration": 2.73
    },
    {
        "text": ">> That's really cool.",
        "start": 295.05,
        "duration": 3.3
    },
    {
        "text": "This is just a model without any frills.",
        "start": 298.35,
        "duration": 3.93
    },
    {
        "text": "It is just a standard Davinci model.",
        "start": 302.28,
        "duration": 2.855
    },
    {
        "text": ">> Yeah. It's a great question. Yeah.",
        "start": 305.135,
        "duration": 1.365
    },
    {
        "text": "Same model in all three cases.",
        "start": 306.5,
        "duration": 1.54
    },
    {
        "text": "It's called our text-davinci-001 model,",
        "start": 308.04,
        "duration": 2.28
    },
    {
        "text": "which is one of the standard based GPT3 models.",
        "start": 310.32,
        "duration": 2.68
    },
    {
        "text": ">> I see. I'm guessing there's other models in there, too,",
        "start": 313.0,
        "duration": 3.08
    },
    {
        "text": "that you can work against that have varying degrees",
        "start": 316.08,
        "duration": 2.76
    },
    {
        "text": "of accuracy, size, whatever.",
        "start": 318.84,
        "duration": 2.88
    },
    {
        "text": ">> Yeah, it's a great question.",
        "start": 321.72,
        "duration": 2.4
    },
    {
        "text": "So there's actually two different",
        "start": 324.12,
        "duration": 1.17
    },
    {
        "text": "pivots on this you could think about.",
        "start": 325.29,
        "duration": 1.14
    },
    {
        "text": "There are models that have various degrees of size,",
        "start": 326.43,
        "duration": 2.46
    },
    {
        "text": "and we call those the Ada,",
        "start": 328.89,
        "duration": 1.44
    },
    {
        "text": "Babbage, Curie, and Davinci model series.",
        "start": 330.33,
        "duration": 2.325
    },
    {
        "text": "Davinci being the most powerful,",
        "start": 332.655,
        "duration": 1.935
    },
    {
        "text": "biggest model, Ada being much smaller.",
        "start": 334.59,
        "duration": 2.43
    },
    {
        "text": "What we really see is that the trade-off between inferencing times,",
        "start": 337.02,
        "duration": 3.45
    },
    {
        "text": "the smaller models will compute faster,",
        "start": 340.47,
        "duration": 1.56
    },
    {
        "text": "you get higher throughput versus capabilities and performance.",
        "start": 342.03,
        "duration": 4.44
    },
    {
        "text": "Davinci, you get the best performance,",
        "start": 346.47,
        "duration": 1.8
    },
    {
        "text": "but it's going to take a little bit longer.",
        "start": 348.27,
        "duration": 1.41
    },
    {
        "text": "What we see is at the least,",
        "start": 349.68,
        "duration": 3.645
    },
    {
        "text": "there's a bit of an optimization step where you might look at,",
        "start": 353.325,
        "duration": 3.15
    },
    {
        "text": "can I solve my task with",
        "start": 356.475,
        "duration": 2.355
    },
    {
        "text": "Curie and maybe do it a little bit faster, a little bit cheaper?",
        "start": 358.83,
        "duration": 2.88
    },
    {
        "text": "Or can I go all the way down to Ada?",
        "start": 361.71,
        "duration": 1.88
    },
    {
        "text": "You start to optimize down as you",
        "start": 363.59,
        "duration": 1.45
    },
    {
        "text": "progress through that production journey.",
        "start": 365.04,
        "duration": 1.965
    },
    {
        "text": ">> That's cool.",
        "start": 367.005,
        "duration": 1.56
    },
    {
        "text": "As we're looking at this,",
        "start": 368.565,
        "duration": 1.71
    },
    {
        "text": "if you want to show more examples, that's awesome.",
        "start": 370.275,
        "duration": 2.43
    },
    {
        "text": "Show them. But I also want to ask the question.",
        "start": 372.705,
        "duration": 2.49
    },
    {
        "text": "There's probably a time when",
        "start": 375.195,
        "duration": 1.395
    },
    {
        "text": "these models get you most of the way there,",
        "start": 376.59,
        "duration": 3.405
    },
    {
        "text": "but it feels like you need to do",
        "start": 379.995,
        "duration": 1.725
    },
    {
        "text": "something a little bit more specific.",
        "start": 381.72,
        "duration": 2.415
    },
    {
        "text": "After you show us more examples, if you have any,",
        "start": 384.135,
        "duration": 2.295
    },
    {
        "text": "I'd love to know how do we make these models",
        "start": 386.43,
        "duration": 2.79
    },
    {
        "text": "more attuned to data that I might have, for example?",
        "start": 389.22,
        "duration": 2.955
    },
    {
        "text": ">> Yeah, that is a great question.",
        "start": 392.175,
        "duration": 1.815
    },
    {
        "text": "I have one more example and then we'll hop into that.",
        "start": 393.99,
        "duration": 1.86
    },
    {
        "text": "The other example looks at,",
        "start": 395.85,
        "duration": 2.415
    },
    {
        "text": "you mentioned there's multiple different flavors of the models.",
        "start": 398.265,
        "duration": 2.13
    },
    {
        "text": "We looked at the text-GPT-3 series,",
        "start": 400.395,
        "duration": 2.625
    },
    {
        "text": "which is the base series you've probably heard about with OpenAI.",
        "start": 403.02,
        "duration": 3.405
    },
    {
        "text": "But I also wanted to show quickly was our Codex model series,",
        "start": 406.425,
        "duration": 3.015
    },
    {
        "text": "and the Codex Model series came from a partnership with GitHub,",
        "start": 409.44,
        "duration": 3.135
    },
    {
        "text": "really looking at training these models",
        "start": 412.575,
        "duration": 2.505
    },
    {
        "text": "on large corpuses of open source code.",
        "start": 415.08,
        "duration": 2.85
    },
    {
        "text": "It's how do you do code base tasks.",
        "start": 417.93,
        "duration": 1.89
    },
    {
        "text": "Natural language to code,",
        "start": 419.82,
        "duration": 1.35
    },
    {
        "text": "code to natural language for code commenting.",
        "start": 421.17,
        "duration": 2.34
    },
    {
        "text": "It is one of the underlying model type that's",
        "start": 423.51,
        "duration": 3.24
    },
    {
        "text": "used to power our GitHub co-pilot feature, for example.",
        "start": 426.75,
        "duration": 4.21
    },
    {
        "text": "I will change the model,",
        "start": 431.36,
        "duration": 2.155
    },
    {
        "text": "but I won't change anything else here.",
        "start": 433.515,
        "duration": 1.425
    },
    {
        "text": "We are using the same API structure,",
        "start": 434.94,
        "duration": 1.92
    },
    {
        "text": "same type of call, and I will copy in a little example here,",
        "start": 436.86,
        "duration": 3.9
    },
    {
        "text": "where imagine I want to do some code commenting",
        "start": 440.76,
        "duration": 2.25
    },
    {
        "text": "which I, personally, always leave till the very end of my projects,",
        "start": 443.01,
        "duration": 3.94
    },
    {
        "text": "and so there's super helpful tool.",
        "start": 446.95,
        "duration": 2.485
    },
    {
        "text": "We have a method here that'll randomly split a dataset,",
        "start": 449.435,
        "duration": 4.955
    },
    {
        "text": "and I'm basically asking for",
        "start": 454.39,
        "duration": 1.11
    },
    {
        "text": "an elaborate high quality docstring for the above function",
        "start": 455.5,
        "duration": 3.79
    },
    {
        "text": ">> This is documentation that we would always write.",
        "start": 461.1,
        "duration": 4.705
    },
    {
        "text": "That's pretty cool.",
        "start": 465.805,
        "duration": 3.085
    },
    {
        "text": ">> Yeah. Always write this.",
        "start": 468.99,
        "duration": 2.87
    },
    {
        "text": ">> That's pretty cool.",
        "start": 472.65,
        "duration": 3.02
    },
    {
        "text": "Because this is code that we've all written,",
        "start": 478.77,
        "duration": 3.655
    },
    {
        "text": "but the fact that it writes",
        "start": 482.425,
        "duration": 1.71
    },
    {
        "text": "this good of a docstring is delightful.",
        "start": 484.135,
        "duration": 3.895
    },
    {
        "text": ">> It's awesome. It can be",
        "start": 488.13,
        "duration": 2.08
    },
    {
        "text": "a big time saver for tons of different use cases.",
        "start": 490.21,
        "duration": 3.075
    },
    {
        "text": "But I think it is also",
        "start": 493.285,
        "duration": 1.305
    },
    {
        "text": "important to call out what you were mentioning earlier.",
        "start": 494.59,
        "duration": 1.5
    },
    {
        "text": "In some cases, if you have",
        "start": 496.09,
        "duration": 1.11
    },
    {
        "text": "a complex task or it's really industry specific,",
        "start": 497.2,
        "duration": 2.73
    },
    {
        "text": "you find that out of the box",
        "start": 499.93,
        "duration": 1.47
    },
    {
        "text": "it's not going to quite get you there.",
        "start": 501.4,
        "duration": 1.605
    },
    {
        "text": "That's where we have the fine tuning process,",
        "start": 503.005,
        "duration": 2.685
    },
    {
        "text": "which is taking these same models,",
        "start": 505.69,
        "duration": 1.59
    },
    {
        "text": "customizing the actual weights.",
        "start": 507.28,
        "duration": 1.89
    },
    {
        "text": "Everything here that we're showing,",
        "start": 509.17,
        "duration": 1.68
    },
    {
        "text": "you're not changing the actual model or any of its weights,",
        "start": 510.85,
        "duration": 2.61
    },
    {
        "text": "you're just providing in context at that call time.",
        "start": 513.46,
        "duration": 3.315
    },
    {
        "text": "What you can do right in this UI or VAPIs,",
        "start": 516.775,
        "duration": 3.135
    },
    {
        "text": "you can go over to this model section",
        "start": 519.91,
        "duration": 2.8
    },
    {
        "text": "and actually go and create a customized model.",
        "start": 523.17,
        "duration": 3.265
    },
    {
        "text": "The way you create that customized model is",
        "start": 526.435,
        "duration": 1.785
    },
    {
        "text": "starting with one of these base model series.",
        "start": 528.22,
        "duration": 1.995
    },
    {
        "text": "I'll choose out of here, which is one of",
        "start": 530.215,
        "duration": 1.515
    },
    {
        "text": "the smaller models that we'll train real fast.",
        "start": 531.73,
        "duration": 2.4
    },
    {
        "text": "Then you update your training data.",
        "start": 534.13,
        "duration": 2.46
    },
    {
        "text": "You're training your validation data set.",
        "start": 536.59,
        "duration": 2.1
    },
    {
        "text": "Here, I have a training set and a validation set.",
        "start": 538.69,
        "duration": 3.96
    },
    {
        "text": "You can also then start to tune some of the hyper parameters.",
        "start": 542.65,
        "duration": 3.42
    },
    {
        "text": "If you don't quite know what these are,",
        "start": 546.07,
        "duration": 1.47
    },
    {
        "text": "you can go with some of the defaults and it'll train",
        "start": 547.54,
        "duration": 1.89
    },
    {
        "text": "a decent model right out of the box.",
        "start": 549.43,
        "duration": 2.73
    },
    {
        "text": "We'll kick that off and that'll start to train.",
        "start": 552.16,
        "duration": 4.33
    },
    {
        "text": "Once that's complete, you can then go and",
        "start": 557.04,
        "duration": 2.83
    },
    {
        "text": "deploy these models for inference and what I'm training here is",
        "start": 559.87,
        "duration": 6.75
    },
    {
        "text": "a set of data that trains the models to speak like Homer.",
        "start": 566.62,
        "duration": 3.945
    },
    {
        "text": "We've added a lot of training,",
        "start": 570.565,
        "duration": 1.215
    },
    {
        "text": "different examples of Homer's writings",
        "start": 571.78,
        "duration": 2.4
    },
    {
        "text": "and scripts to try to convert how the model talks and provide",
        "start": 574.18,
        "duration": 3.72
    },
    {
        "text": "a more ancient Greek flavor to our conversations.",
        "start": 577.9,
        "duration": 4.335
    },
    {
        "text": ">> This is something we did in one of our breakouts and",
        "start": 582.235,
        "duration": 3.48
    },
    {
        "text": "in the keynote at Microsoft Build",
        "start": 585.715,
        "duration": 3.555
    },
    {
        "text": "because, literally, I was interested to",
        "start": 589.27,
        "duration": 3.3
    },
    {
        "text": "see what marketing copy would look like in the style of Homer,",
        "start": 592.57,
        "duration": 3.72
    },
    {
        "text": "not Simpsons, by the way.",
        "start": 596.29,
        "duration": 1.245
    },
    {
        "text": "Homer from Homer, Greek Homer.",
        "start": 597.535,
        "duration": 3.255
    },
    {
        "text": ">> Exactly. I've taken that same data here,",
        "start": 600.79,
        "duration": 3.045
    },
    {
        "text": "walked it through the process just to",
        "start": 603.835,
        "duration": 1.425
    },
    {
        "text": "show you how that would look and that we have it",
        "start": 605.26,
        "duration": 1.59
    },
    {
        "text": "running here and so you can start to ask",
        "start": 606.85,
        "duration": 2.19
    },
    {
        "text": "very interesting questions to",
        "start": 609.04,
        "duration": 2.7
    },
    {
        "text": "get some of that Homerian flavor out of it.",
        "start": 611.74,
        "duration": 3.81
    },
    {
        "text": "We can ask it, \"Why is the AI show awesome?\"",
        "start": 615.55,
        "duration": 4.18
    },
    {
        "text": ">> Let's see. These two questions are synonymous.",
        "start": 622.2,
        "duration": 9.1
    },
    {
        "text": "Why is the AI show awesome?",
        "start": 631.3,
        "duration": 1.365
    },
    {
        "text": "Why the dark frowns cast over his brows?",
        "start": 632.665,
        "duration": 3.42
    },
    {
        "text": "Because they weren't watching the AI show, I think Chris.",
        "start": 636.085,
        "duration": 2.49
    },
    {
        "text": "Is what we're trying to figure out what's going on here.",
        "start": 638.575,
        "duration": 2.295
    },
    {
        "text": "This is really funny.",
        "start": 640.87,
        "duration": 2.32
    },
    {
        "text": ">> You could start to play around with this.",
        "start": 643.2,
        "duration": 2.425
    },
    {
        "text": "You can also see the iterative nature where you do experiment with",
        "start": 645.625,
        "duration": 2.655
    },
    {
        "text": "those prompts to figure out",
        "start": 648.28,
        "duration": 1.02
    },
    {
        "text": "the right way to use it or the use case.",
        "start": 649.3,
        "duration": 2.58
    },
    {
        "text": "Maybe I should have asked, why should I watch this?",
        "start": 651.88,
        "duration": 3.49
    },
    {
        "text": ">> Someone actually said that to me the other day.",
        "start": 663.57,
        "duration": 3.445
    },
    {
        "text": "I have watched enough of it.",
        "start": 667.015,
        "duration": 1.635
    },
    {
        "text": "Thank you for the confidence boost there especially.",
        "start": 668.65,
        "duration": 3.435
    },
    {
        "text": "Now, this is all very humorous,",
        "start": 672.085,
        "duration": 2.145
    },
    {
        "text": "but there may be a situation where",
        "start": 674.23,
        "duration": 2.01
    },
    {
        "text": "organizations have specialized vocabulary,",
        "start": 676.24,
        "duration": 3.165
    },
    {
        "text": "a specialized way of putting things together",
        "start": 679.405,
        "duration": 3.105
    },
    {
        "text": "that fine tuning will bear this out because I'm looking at this,",
        "start": 682.51,
        "duration": 3.39
    },
    {
        "text": "it looks like I'm reading the Iliad from Homer or something.",
        "start": 685.9,
        "duration": 2.58
    },
    {
        "text": ">> Right.",
        "start": 688.48,
        "duration": 1.001
    },
    {
        "text": ">> Mike, are we seeing organizations do stuff like this?",
        "start": 692.16,
        "duration": 5.27
    },
    {
        "text": ">> We are, yes. We're seeing lots of",
        "start": 698.22,
        "duration": 2.68
    },
    {
        "text": "organizations start to pick up fine tuning,",
        "start": 700.9,
        "duration": 1.8
    },
    {
        "text": "really apply it to their specific use cases.",
        "start": 702.7,
        "duration": 2.505
    },
    {
        "text": "Typically, you get a lot of",
        "start": 705.205,
        "duration": 2.145
    },
    {
        "text": "mileage, understanding what's possible,",
        "start": 707.35,
        "duration": 2.1
    },
    {
        "text": "how we might use these models by iterating on",
        "start": 709.45,
        "duration": 2.55
    },
    {
        "text": "these prop designs with the base models,",
        "start": 712.0,
        "duration": 1.53
    },
    {
        "text": "and then you start to learn,",
        "start": 713.53,
        "duration": 1.14
    },
    {
        "text": "develop the dataset to go and train",
        "start": 714.67,
        "duration": 1.65
    },
    {
        "text": "with at least a couple of hundred examples",
        "start": 716.32,
        "duration": 2.04
    },
    {
        "text": "for things like summarization in very specific use cases,",
        "start": 718.36,
        "duration": 3.9
    },
    {
        "text": "whether that's financial information, financial statements,",
        "start": 722.26,
        "duration": 3.39
    },
    {
        "text": "customer support conversations, or even medical documents.",
        "start": 725.65,
        "duration": 4.215
    },
    {
        "text": "All of those have their own flavor of jargon,",
        "start": 729.865,
        "duration": 3.72
    },
    {
        "text": "different types of words are being used as well",
        "start": 733.585,
        "duration": 2.235
    },
    {
        "text": "as you might have different sets of important information.",
        "start": 735.82,
        "duration": 3.18
    },
    {
        "text": "That really becomes easier",
        "start": 739.0,
        "duration": 3.62
    },
    {
        "text": "to extract with the fine tuning process.",
        "start": 742.62,
        "duration": 1.89
    },
    {
        "text": ">> Awesome. This is all really cool.",
        "start": 744.51,
        "duration": 3.18
    },
    {
        "text": "But one of the things that is always top of mind, at least for me,",
        "start": 747.69,
        "duration": 3.15
    },
    {
        "text": "when I'm using these kinds of generative models,",
        "start": 750.84,
        "duration": 2.715
    },
    {
        "text": "is the notion of using them responsibly.",
        "start": 753.555,
        "duration": 2.915
    },
    {
        "text": "How are we helping folks use this responsibly?",
        "start": 756.47,
        "duration": 3.965
    },
    {
        "text": ">> That's a great question. It's something that's",
        "start": 760.435,
        "duration": 1.875
    },
    {
        "text": "really top of mind for us as we start",
        "start": 762.31,
        "duration": 1.62
    },
    {
        "text": "to broaden access to the service in the public preview.",
        "start": 763.93,
        "duration": 3.42
    },
    {
        "text": "It's one of the main reasons why",
        "start": 767.35,
        "duration": 2.19
    },
    {
        "text": "it progressed very slowly with invite only and even now,",
        "start": 769.54,
        "duration": 3.72
    },
    {
        "text": "we're going to ask customers to apply and submit",
        "start": 773.26,
        "duration": 1.86
    },
    {
        "text": "their use cases to ensure that the service in",
        "start": 775.12,
        "duration": 2.4
    },
    {
        "text": "this generative capability isn't being misused or",
        "start": 777.52,
        "duration": 2.94
    },
    {
        "text": "abused for malicious purposes.",
        "start": 780.46,
        "duration": 3.78
    },
    {
        "text": "Then within the product,",
        "start": 784.24,
        "duration": 1.83
    },
    {
        "text": "we're actually starting to develop a set of product tools to help",
        "start": 786.07,
        "duration": 2.535
    },
    {
        "text": "our customers develop the applications responsibly.",
        "start": 788.605,
        "duration": 2.985
    },
    {
        "text": "We're starting with what we",
        "start": 791.59,
        "duration": 1.62
    },
    {
        "text": "call content filtering, content management.",
        "start": 793.21,
        "duration": 1.905
    },
    {
        "text": "What's going to happen is when",
        "start": 795.115,
        "duration": 1.995
    },
    {
        "text": "you or a customer makes an API call,",
        "start": 797.11,
        "duration": 2.415
    },
    {
        "text": "does this generation step,",
        "start": 799.525,
        "duration": 1.455
    },
    {
        "text": "both that prompt and the generated text will",
        "start": 800.98,
        "duration": 2.28
    },
    {
        "text": "get evaluated against a series of classifiers to identify abuse",
        "start": 803.26,
        "duration": 7.02
    },
    {
        "text": "or misuse and so looking for those very high sensitive topics,",
        "start": 810.28,
        "duration": 3.63
    },
    {
        "text": "like is it hate speech or is there violent content being sent.",
        "start": 813.91,
        "duration": 3.87
    },
    {
        "text": "We'll actually start to filter that out and block it to",
        "start": 817.78,
        "duration": 2.625
    },
    {
        "text": "ensure that that kind of content does not go back to the customer.",
        "start": 820.405,
        "duration": 4.005
    },
    {
        "text": ">> I see. We're doing",
        "start": 824.41,
        "duration": 1.53
    },
    {
        "text": "content filtering based upon those actions that you drive on",
        "start": 825.94,
        "duration": 3.3
    },
    {
        "text": "the way in essentially as",
        "start": 829.24,
        "duration": 2.34
    },
    {
        "text": "well as on the way out, am I getting that right?",
        "start": 831.58,
        "duration": 2.82
    },
    {
        "text": ">> Yes. It's actually looking for somebody that is on the way in,",
        "start": 834.4,
        "duration": 3.9
    },
    {
        "text": "trying to misuse it, and that is well stopping",
        "start": 838.3,
        "duration": 1.92
    },
    {
        "text": "any bad generation on the way out.",
        "start": 840.22,
        "duration": 2.265
    },
    {
        "text": ">> Well, that's awesome, because like I said,",
        "start": 842.485,
        "duration": 3.96
    },
    {
        "text": "that's the one concern I have with generative models is,",
        "start": 846.445,
        "duration": 2.64
    },
    {
        "text": "could it make people feel excluded?",
        "start": 849.085,
        "duration": 3.285
    },
    {
        "text": "Could it say things that are untoward?",
        "start": 852.37,
        "duration": 1.875
    },
    {
        "text": "We're doing a lot of work already to",
        "start": 854.245,
        "duration": 1.815
    },
    {
        "text": "make sure that that does not happen.",
        "start": 856.06,
        "duration": 2.68
    },
    {
        "text": ">> I totally agree. I think",
        "start": 859.02,
        "duration": 2.305
    },
    {
        "text": "it's one of many tools we've found that",
        "start": 861.325,
        "duration": 2.175
    },
    {
        "text": "no one single solution is going to",
        "start": 863.5,
        "duration": 2.16
    },
    {
        "text": "solve the whole problem of abuse and misuse.",
        "start": 865.66,
        "duration": 2.805
    },
    {
        "text": "There's the gating, there's these filtering tools,",
        "start": 868.465,
        "duration": 3.135
    },
    {
        "text": "and then the third big one that we do have, and",
        "start": 871.6,
        "duration": 1.8
    },
    {
        "text": "we're working on, is solution design.",
        "start": 873.4,
        "duration": 2.085
    },
    {
        "text": "How do our customers design in a responsible way?",
        "start": 875.485,
        "duration": 3.09
    },
    {
        "text": "That's not unique to the Azure OpenAI Service,",
        "start": 878.575,
        "duration": 2.88
    },
    {
        "text": "it's a AI problem and Microsoft is really investing",
        "start": 881.455,
        "duration": 3.015
    },
    {
        "text": "a lot across the company in providing guidelines,",
        "start": 884.47,
        "duration": 3.225
    },
    {
        "text": "research, and ideas of how customers can do that.",
        "start": 887.695,
        "duration": 3.165
    },
    {
        "text": "We'll start to incorporate that into",
        "start": 890.86,
        "duration": 2.34
    },
    {
        "text": "our documentation in the Azure OpenAI Service as well.",
        "start": 893.2,
        "duration": 2.685
    },
    {
        "text": ">> Well, amazing.",
        "start": 895.885,
        "duration": 1.56
    },
    {
        "text": "Where can people go to find out more or maybe sign up, learn more?",
        "start": 897.445,
        "duration": 3.66
    },
    {
        "text": ">> I think there's a link right",
        "start": 901.105,
        "duration": 2.055
    },
    {
        "text": "below us on where to go and sign up.",
        "start": 903.16,
        "duration": 1.89
    },
    {
        "text": "You have to go apply for access.",
        "start": 905.05,
        "duration": 2.88
    },
    {
        "text": "You can start doing that today.",
        "start": 907.93,
        "duration": 1.44
    },
    {
        "text": "You just have to provide",
        "start": 909.37,
        "duration": 1.29
    },
    {
        "text": "a little information on what you intend to do with the service",
        "start": 910.66,
        "duration": 2.46
    },
    {
        "text": "and so that we can ensure that it matches against",
        "start": 913.12,
        "duration": 1.77
    },
    {
        "text": "our responsible AI policies.",
        "start": 914.89,
        "duration": 2.1
    },
    {
        "text": ">> Which is, again, another axis",
        "start": 916.99,
        "duration": 4.5
    },
    {
        "text": "along the line of how we're doing this responsibly,",
        "start": 921.49,
        "duration": 3.28
    },
    {
        "text": "I think, which is really cool.",
        "start": 924.77,
        "duration": 1.585
    },
    {
        "text": ">> I agree.",
        "start": 926.355,
        "duration": 1.33
    },
    {
        "text": ">> Well, this has been amazing.",
        "start": 927.685,
        "duration": 2.025
    },
    {
        "text": "Chris, thank you so much",
        "start": 929.71,
        "duration": 1.24
    },
    {
        "text": "for spending some time with us, my friend.",
        "start": 930.95,
        "duration": 1.54
    },
    {
        "text": ">> Thanks for having me. I can't wait to be back.",
        "start": 932.49,
        "duration": 1.73
    },
    {
        "text": ">> Awesome. You've been learning all about combining",
        "start": 934.22,
        "duration": 2.91
    },
    {
        "text": "OpenAI models with the power of Azure here on the AI Show.",
        "start": 937.13,
        "duration": 4.095
    },
    {
        "text": "Thank you so much for watching,",
        "start": 941.225,
        "duration": 0.915
    },
    {
        "text": "and hopefully we'll see you next time.",
        "start": 942.14,
        "duration": 1.08
    },
    {
        "text": "Take care.",
        "start": 943.22,
        "duration": 10.625
    }
]