{"speaker": "Mehrnoosh", "title": "Machine Learning Interpretability Toolkit", "videoId": "EIvMxWfsOHo", "description": "We will discuss a little about what it means to develop AI in a transparent way. We will introduce our interpretability toolkit which enables you to use different state-of-the-art interpretability methods to explain your models decisions. By using this toolkit during the training phase of the AI development cycle, you can use interpretability output of a model to verify hypotheses and build trust with stakeholders. You can also use the insights for debugging, validating model behavior, and to check for bias. You can also use this toolkit at inferencing time to explain the predictions of a deployed model to the end users.\n\nLearn More: \nLink to the doc: https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability?ocid=AID2463683&wt.mc_id=ai-c9-sejuare\nLink to the sample notebooks: https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model?ocid=AID2463683&wt.mc_id=ai-c9-sejuare\n \n02:12 \u2013 Responsible AI\n02:34 \u2013 Machine Learning Interpretability\n03:12 \u2013 Interpretability Use Cases\n05:20 - Different Interpretability Techniques\n06:45 - Demo\n\nDon't miss new episodes, subscribe to the AI Show: https://aka.ms/aishowsubscribe  \nCreate a Free account (Azure): https://aka.ms/aishow-seth-azurefree\nAI Blog: https://aka.ms/openaiblog\nFast ML: https://aka.ms/fastml   \nMIT News | AI: https://aka.ms/mitnews\nMedium | Francesca Lazzeri: https://medium.com/@francescalazzeri\nDeep Learning vs. Machine Learning: https://aka.ms/deeplearningmachinelearning\nFollow: https://twitter.com/sethjuarez\nFollow: https://twitter.com/ch9\nFollow: https://twitter.com/Azure\nFollow: https://twitter.com/msdev\n\n#microsoftai #machinelearning #interpretabilitytoolkit"}