[
    {
        "text": ">> Welcome to another episode of the AI show.",
        "start": 1.47,
        "duration": 6.16
    },
    {
        "text": "Today we're going to be talking about",
        "start": 7.63,
        "duration": 1.035
    },
    {
        "text": "the Intelligent Edge with",
        "start": 8.665,
        "duration": 1.265
    },
    {
        "text": "Anusua Trivedi. How are you doing my friend?",
        "start": 9.93,
        "duration": 1.68
    },
    {
        "text": ">> I'm doing good, how are you?",
        "start": 11.61,
        "duration": 1.06
    },
    {
        "text": ">> Awesome. Tell us a little about the project.",
        "start": 12.67,
        "duration": 1.49
    },
    {
        "text": ">> So, today we're going to talk about real-time edge AI.",
        "start": 14.16,
        "duration": 3.695
    },
    {
        "text": "It's important nowadays to not",
        "start": 17.855,
        "duration": 2.36
    },
    {
        "text": "only work on applications but",
        "start": 20.215,
        "duration": 2.635
    },
    {
        "text": "work in a way to actually give it",
        "start": 22.85,
        "duration": 2.46
    },
    {
        "text": "access to every user and empower every developer.",
        "start": 25.31,
        "duration": 2.96
    },
    {
        "text": "So, what we're trying to do here",
        "start": 28.27,
        "duration": 2.13
    },
    {
        "text": "is show you how we build a model on",
        "start": 30.4,
        "duration": 2.98
    },
    {
        "text": "Azure and how can",
        "start": 33.38,
        "duration": 2.43
    },
    {
        "text": "we actually deploy it on the Edge device itself.",
        "start": 35.81,
        "duration": 3.07
    },
    {
        "text": ">> I see. So this is Machine Learning",
        "start": 38.88,
        "duration": 1.86
    },
    {
        "text": "but instead of putting the models in the cloud,",
        "start": 40.74,
        "duration": 2.37
    },
    {
        "text": "it actually works on the devices that you're looking at.",
        "start": 43.11,
        "duration": 2.78
    },
    {
        "text": ">> That's exactly correct. Yes.",
        "start": 45.89,
        "duration": 1.31
    },
    {
        "text": ">> Awesome. Tell us about the project.",
        "start": 47.2,
        "duration": 1.49
    },
    {
        "text": "There's got to be a use case that you have.",
        "start": 48.69,
        "duration": 1.61
    },
    {
        "text": ">> Yes. So when we were thinking about AI applications,",
        "start": 50.3,
        "duration": 3.145
    },
    {
        "text": "the first thing which struck us was healthcare.",
        "start": 53.445,
        "duration": 2.255
    },
    {
        "text": "So, it has a profound effect on everyone's life,",
        "start": 55.7,
        "duration": 4.415
    },
    {
        "text": "and what if we can",
        "start": 60.115,
        "duration": 1.905
    },
    {
        "text": "develop an AI product which affects healthcare?",
        "start": 62.02,
        "duration": 3.135
    },
    {
        "text": "So, we were thinking about the example of skin cancer.",
        "start": 65.155,
        "duration": 4.37
    },
    {
        "text": "What if we can actually develop an app which",
        "start": 69.525,
        "duration": 2.845
    },
    {
        "text": "will enable us to put us in a priority queue?",
        "start": 72.37,
        "duration": 3.285
    },
    {
        "text": "So, take the example of third world countries.",
        "start": 75.655,
        "duration": 2.525
    },
    {
        "text": "I come from India, to take the example of my country,",
        "start": 78.18,
        "duration": 3.215
    },
    {
        "text": "the patient and the doctor ratio",
        "start": 81.395,
        "duration": 2.425
    },
    {
        "text": "is like super weird there.",
        "start": 83.82,
        "duration": 2.055
    },
    {
        "text": "For one doctor you have",
        "start": 85.875,
        "duration": 1.55
    },
    {
        "text": "thousands and thousands of patients.",
        "start": 87.425,
        "duration": 1.88
    },
    {
        "text": "So, how can the doctors prioritize",
        "start": 89.305,
        "duration": 2.505
    },
    {
        "text": "the use cases which are super essential?",
        "start": 91.81,
        "duration": 2.35
    },
    {
        "text": "And cancer is very, very time-sensitive, as you know.",
        "start": 94.16,
        "duration": 2.295
    },
    {
        "text": "So, we thought about,",
        "start": 96.455,
        "duration": 2.175
    },
    {
        "text": "okay, what if we can actually create a suggestion app,",
        "start": 98.63,
        "duration": 2.49
    },
    {
        "text": "kind of an AI assistant for",
        "start": 101.12,
        "duration": 1.94
    },
    {
        "text": "the users or even can be used by the doctors,",
        "start": 103.06,
        "duration": 2.91
    },
    {
        "text": "which will be able to detect the stages",
        "start": 105.97,
        "duration": 3.825
    },
    {
        "text": "of skin cancer and",
        "start": 109.795,
        "duration": 2.265
    },
    {
        "text": "suggest if you need to go and see a doctor.",
        "start": 112.06,
        "duration": 2.33
    },
    {
        "text": ">> I see. So it's not a doctor.",
        "start": 114.39,
        "duration": 2.25
    },
    {
        "text": "We don't want people to be confused",
        "start": 116.64,
        "duration": 1.39
    },
    {
        "text": "about whether this is a doctor.",
        "start": 118.03,
        "duration": 2.08
    },
    {
        "text": ">> Is just a prediction app as AI does.",
        "start": 120.11,
        "duration": 1.56
    },
    {
        "text": ">> It's just predicting. That's right.",
        "start": 121.67,
        "duration": 1.46
    },
    {
        "text": ">> Yeah, so AI does predictions for you.",
        "start": 123.13,
        "duration": 2.455
    },
    {
        "text": "And here, it tries to confidently",
        "start": 125.585,
        "duration": 2.005
    },
    {
        "text": "say if you need to go see a doctor or not.",
        "start": 127.59,
        "duration": 2.25
    },
    {
        "text": ">> Awesome. So, tell us a little about how you went",
        "start": 129.84,
        "duration": 3.0
    },
    {
        "text": "from idea to actual model?",
        "start": 132.84,
        "duration": 3.82
    },
    {
        "text": "Or maybe we should start with a demo,",
        "start": 136.66,
        "duration": 1.29
    },
    {
        "text": "just so people can see what it is.",
        "start": 137.95,
        "duration": 1.34
    },
    {
        "text": "What would you like to do first?",
        "start": 139.29,
        "duration": 0.865
    },
    {
        "text": ">> Yes, let's start with a demo first.",
        "start": 140.155,
        "duration": 2.29
    },
    {
        "text": "As you see here, on my screen,",
        "start": 142.445,
        "duration": 2.53
    },
    {
        "text": "I have two apps.",
        "start": 144.975,
        "duration": 1.175
    },
    {
        "text": "I have photo version of the app and I",
        "start": 146.15,
        "duration": 1.94
    },
    {
        "text": "have a video version of the app.",
        "start": 148.09,
        "duration": 3.13
    },
    {
        "text": "Let me first start with the photo version.",
        "start": 151.22,
        "duration": 1.91
    },
    {
        "text": "What I have done here, I've already",
        "start": 153.13,
        "duration": 2.02
    },
    {
        "text": "preloaded this app with different types of images.",
        "start": 155.15,
        "duration": 3.215
    },
    {
        "text": "Let me choose one of the images and see what it gives me.",
        "start": 158.365,
        "duration": 3.335
    },
    {
        "text": "So as you can see, for this image,",
        "start": 161.7,
        "duration": 2.145
    },
    {
        "text": "it's telling me I'm all clear.",
        "start": 163.845,
        "duration": 1.815
    },
    {
        "text": "So, I can still be confident by not going to a doctor.",
        "start": 165.66,
        "duration": 3.58
    },
    {
        "text": "Let me choose another one, as",
        "start": 169.24,
        "duration": 2.08
    },
    {
        "text": "you can see here it's suggesting me,",
        "start": 171.32,
        "duration": 1.835
    },
    {
        "text": "you should surely go see a doctor.",
        "start": 173.155,
        "duration": 3.765
    },
    {
        "text": "But what if we can make this app real time, right?",
        "start": 176.92,
        "duration": 3.47
    },
    {
        "text": "For example, what if we can feed it",
        "start": 180.39,
        "duration": 2.12
    },
    {
        "text": "a video feed and actually predict something cool.",
        "start": 182.51,
        "duration": 3.915
    },
    {
        "text": "So, if you see here, I have",
        "start": 186.425,
        "duration": 2.215
    },
    {
        "text": "an image on which I",
        "start": 188.64,
        "duration": 2.04
    },
    {
        "text": "am trying to predict and it says that,",
        "start": 190.68,
        "duration": 1.665
    },
    {
        "text": "\"Okay, this is a melanoma,",
        "start": 192.345,
        "duration": 1.225
    },
    {
        "text": "it's undetecting, and you should go see a doctor.\"",
        "start": 193.57,
        "duration": 3.19
    },
    {
        "text": ">> So, let me see if I understand what's",
        "start": 196.76,
        "duration": 1.855
    },
    {
        "text": "going on, as you're doing this,",
        "start": 198.615,
        "duration": 2.0
    },
    {
        "text": "because traditionally I'm used to cognitive services,",
        "start": 200.615,
        "duration": 2.675
    },
    {
        "text": "I would send something to a REST API and it would send",
        "start": 203.29,
        "duration": 2.26
    },
    {
        "text": "back whether you should see",
        "start": 205.55,
        "duration": 1.07
    },
    {
        "text": "your doctor or not, for example.",
        "start": 206.62,
        "duration": 1.505
    },
    {
        "text": "In this case, the video is sending the pictures",
        "start": 208.125,
        "duration": 2.895
    },
    {
        "text": "to a local thing onboard.",
        "start": 211.02,
        "duration": 3.03
    },
    {
        "text": "So, it doesn't even have to be connected to the internet.",
        "start": 214.05,
        "duration": 1.48
    },
    {
        "text": ">> Yes, for example, in my phone here,",
        "start": 215.53,
        "duration": 1.76
    },
    {
        "text": "I don't even have internet working.",
        "start": 217.29,
        "duration": 1.43
    },
    {
        "text": ">> I see.",
        "start": 218.72,
        "duration": 0.71
    },
    {
        "text": ">> So internet is turned off.",
        "start": 219.43,
        "duration": 1.65
    },
    {
        "text": "For example, if I just hold",
        "start": 221.08,
        "duration": 1.33
    },
    {
        "text": "it on my hand and I'm trying to.",
        "start": 222.41,
        "duration": 4.67
    },
    {
        "text": "So, if you see here, it's telling me I'm all clear.",
        "start": 227.08,
        "duration": 5.155
    },
    {
        "text": "It's telling me I'm all clear,",
        "start": 232.235,
        "duration": 2.385
    },
    {
        "text": "I do not need to worry about it.",
        "start": 234.62,
        "duration": 1.38
    },
    {
        "text": "That's my own hand feed, I'm just feeding it.",
        "start": 236.0,
        "duration": 2.51
    },
    {
        "text": "So, it's all live feed.",
        "start": 238.51,
        "duration": 2.065
    },
    {
        "text": "So, what I'm doing is,",
        "start": 240.575,
        "duration": 1.2
    },
    {
        "text": "I'm just giving it all live feed.",
        "start": 241.775,
        "duration": 1.835
    },
    {
        "text": "In the phone I have a trained model deployed",
        "start": 243.61,
        "duration": 3.67
    },
    {
        "text": "and it's just processing in",
        "start": 247.28,
        "duration": 1.255
    },
    {
        "text": "real time and just giving me the information.",
        "start": 248.535,
        "duration": 2.515
    },
    {
        "text": ">> So for those that are new to Machine Learning,",
        "start": 251.05,
        "duration": 1.78
    },
    {
        "text": "why don't you describe what a model",
        "start": 252.83,
        "duration": 1.37
    },
    {
        "text": "is and how you can actually have",
        "start": 254.2,
        "duration": 1.44
    },
    {
        "text": "it fit on a device or work in a device.",
        "start": 255.64,
        "duration": 3.07
    },
    {
        "text": ">> Yes. For that let us go to my screen and see.",
        "start": 258.71,
        "duration": 2.29
    },
    {
        "text": ">> Awesome.",
        "start": 261.0,
        "duration": 1.335
    },
    {
        "text": ">> As you can see my screen here,",
        "start": 262.335,
        "duration": 2.71
    },
    {
        "text": "I have the Azure Machine Learning up and running.",
        "start": 265.045,
        "duration": 2.48
    },
    {
        "text": "You can run it anywhere,",
        "start": 267.525,
        "duration": 1.305
    },
    {
        "text": "but because Azure makes your training processes easier,",
        "start": 268.83,
        "duration": 5.235
    },
    {
        "text": "I'm running my model in Azure Machine Learning.",
        "start": 274.065,
        "duration": 2.26
    },
    {
        "text": "Here, as you can see I have my training code.",
        "start": 276.325,
        "duration": 2.83
    },
    {
        "text": "Here, I have developed,",
        "start": 279.155,
        "duration": 1.585
    },
    {
        "text": "going a little bit deeper into the code,",
        "start": 280.74,
        "duration": 2.18
    },
    {
        "text": "I am showing you here I have a keras model,",
        "start": 282.92,
        "duration": 3.095
    },
    {
        "text": "which has got three layers.",
        "start": 286.015,
        "duration": 2.045
    },
    {
        "text": "I have two convolution layers,",
        "start": 288.06,
        "duration": 2.66
    },
    {
        "text": "as you can see here, two to the convolutions.",
        "start": 290.72,
        "duration": 2.335
    },
    {
        "text": "And I have a max pooling index in there.",
        "start": 293.055,
        "duration": 3.165
    },
    {
        "text": "So, this is exactly what my model is and",
        "start": 296.22,
        "duration": 1.77
    },
    {
        "text": "have trained it on a research data set.",
        "start": 297.99,
        "duration": 2.17
    },
    {
        "text": "The dataset is called is it cancer data set.",
        "start": 300.16,
        "duration": 2.34
    },
    {
        "text": "So, it's an open data set,",
        "start": 302.5,
        "duration": 1.23
    },
    {
        "text": "I've just taken the data set and trained my model on it.",
        "start": 303.73,
        "duration": 2.93
    },
    {
        "text": ">> I see. So just to be clear,",
        "start": 306.66,
        "duration": 1.5
    },
    {
        "text": "this is the part where you actually train the model.",
        "start": 308.16,
        "duration": 2.59
    },
    {
        "text": ">> Right.",
        "start": 310.75,
        "duration": 0.475
    },
    {
        "text": ">> And the data you got, from a data set that has",
        "start": 311.225,
        "duration": 1.95
    },
    {
        "text": "pictures of cancer pictures and non-cancer pictures.",
        "start": 313.175,
        "duration": 2.605
    },
    {
        "text": ">> It's a labeled data set of melanoma and it tells me,",
        "start": 315.78,
        "duration": 4.335
    },
    {
        "text": "\"Okay, this image is",
        "start": 320.115,
        "duration": 1.465
    },
    {
        "text": "a carcinogenic image and",
        "start": 321.58,
        "duration": 1.53
    },
    {
        "text": "this is a non-carcinogenic image. \"",
        "start": 323.11,
        "duration": 1.53
    },
    {
        "text": "And it tells you whether you have melanoma or not.",
        "start": 324.64,
        "duration": 2.85
    },
    {
        "text": "So, I have taken- it is a classification algorithm.",
        "start": 327.49,
        "duration": 2.995
    },
    {
        "text": "So, what I've done is I've taken the images,",
        "start": 330.485,
        "duration": 2.465
    },
    {
        "text": "I have trained a keras classification model",
        "start": 332.95,
        "duration": 2.37
    },
    {
        "text": "on top of it in Azure.",
        "start": 335.32,
        "duration": 1.695
    },
    {
        "text": ">> Okay, so as I'm looking at this, it's,",
        "start": 337.015,
        "duration": 3.37
    },
    {
        "text": "effectively, you're using a convolutional",
        "start": 340.385,
        "duration": 3.355
    },
    {
        "text": "neural network with one layer.",
        "start": 343.74,
        "duration": 1.76
    },
    {
        "text": "Well, one convolutional layer is a max pooling dropout.",
        "start": 345.5,
        "duration": 3.765
    },
    {
        "text": "And then, you run this, I suspect,",
        "start": 349.265,
        "duration": 2.875
    },
    {
        "text": "somewhere on this particular data.",
        "start": 352.14,
        "duration": 1.775
    },
    {
        "text": "So, tell us about that.",
        "start": 353.915,
        "duration": 1.105
    },
    {
        "text": ">> So, what I do is I have actually trained my models.",
        "start": 355.02,
        "duration": 3.845
    },
    {
        "text": "I have uploaded the data.",
        "start": 358.865,
        "duration": 1.66
    },
    {
        "text": "I have uploaded the data just in my local machine,",
        "start": 360.525,
        "duration": 2.79
    },
    {
        "text": "I can upload it in Azure Blob storage, wherever I want.",
        "start": 363.315,
        "duration": 2.99
    },
    {
        "text": "And I have actually preprocessed",
        "start": 366.305,
        "duration": 1.845
    },
    {
        "text": "data and create three tensors.",
        "start": 368.15,
        "duration": 1.94
    },
    {
        "text": "As you can see here, I have a training tensor,",
        "start": 370.09,
        "duration": 1.81
    },
    {
        "text": "a validation tensor, and a test tensor.",
        "start": 371.9,
        "duration": 3.42
    },
    {
        "text": "it's a very, very Machine Learning specific thing.",
        "start": 375.32,
        "duration": 2.475
    },
    {
        "text": "What you do is, you divide your data set",
        "start": 377.795,
        "duration": 2.21
    },
    {
        "text": "into your training test and validation data set.",
        "start": 380.005,
        "duration": 2.565
    },
    {
        "text": "First, you train on your training dataset,",
        "start": 382.57,
        "duration": 2.03
    },
    {
        "text": "then you do your validation on the training code,",
        "start": 384.6,
        "duration": 2.475
    },
    {
        "text": "and then you bring in your test dataset.",
        "start": 387.075,
        "duration": 2.085
    },
    {
        "text": "Something which the model has not seen at all.",
        "start": 389.16,
        "duration": 2.49
    },
    {
        "text": "Then you test it. So, with that,",
        "start": 391.65,
        "duration": 2.16
    },
    {
        "text": "let me go to the scoring",
        "start": 393.81,
        "duration": 1.1
    },
    {
        "text": "and show you how that's doing it.",
        "start": 394.91,
        "duration": 1.05
    },
    {
        "text": ">> Let's do that. So as you're going",
        "start": 395.96,
        "duration": 1.37
    },
    {
        "text": "there, just to be clear,",
        "start": 397.33,
        "duration": 1.52
    },
    {
        "text": "you train it on this bulk of",
        "start": 398.85,
        "duration": 1.46
    },
    {
        "text": "data but you got to know how well it's doing.",
        "start": 400.31,
        "duration": 1.58
    },
    {
        "text": "So, you got to use",
        "start": 401.89,
        "duration": 1.15
    },
    {
        "text": "the validation data to see",
        "start": 403.04,
        "duration": 1.27
    },
    {
        "text": "if everything is going according to plan.",
        "start": 404.31,
        "duration": 1.48
    },
    {
        "text": "Then you use this stuff that it's never seen",
        "start": 405.79,
        "duration": 1.705
    },
    {
        "text": "before and say is this general as well.",
        "start": 407.495,
        "duration": 1.995
    },
    {
        "text": ">> Right. Exactly. So what I'm doing here,",
        "start": 409.49,
        "duration": 2.72
    },
    {
        "text": "is basically I'm loading",
        "start": 412.21,
        "duration": 1.43
    },
    {
        "text": "the model and just what I'm saying here is,",
        "start": 413.64,
        "duration": 2.43
    },
    {
        "text": "take a test image,",
        "start": 416.07,
        "duration": 1.815
    },
    {
        "text": "what you have not seen, and then run that test image.",
        "start": 417.885,
        "duration": 3.675
    },
    {
        "text": "And I'm saying, to load your model,",
        "start": 421.56,
        "duration": 2.39
    },
    {
        "text": "you load it from this schema.json So,",
        "start": 423.95,
        "duration": 2.39
    },
    {
        "text": "I've trained my model,",
        "start": 426.34,
        "duration": 1.09
    },
    {
        "text": "I've saved my model schema and my model weights.",
        "start": 427.43,
        "duration": 2.78
    },
    {
        "text": "I'm loading it up and what I'm doing is I'm",
        "start": 430.21,
        "duration": 2.77
    },
    {
        "text": "actually giving it a test image, trying to test it.",
        "start": 432.98,
        "duration": 2.71
    },
    {
        "text": ">> Awesome. So this now,",
        "start": 435.69,
        "duration": 1.29
    },
    {
        "text": "we've shifted from the actual training,",
        "start": 436.98,
        "duration": 2.15
    },
    {
        "text": "to this is how you predict the thing.",
        "start": 439.13,
        "duration": 1.82
    },
    {
        "text": ">> Right.",
        "start": 440.95,
        "duration": 0.39
    },
    {
        "text": ">> Got it. Okay.",
        "start": 441.34,
        "duration": 0.46
    },
    {
        "text": ">> Where the whole prediction of the app is coming from.",
        "start": 441.8,
        "duration": 2.495
    },
    {
        "text": "So, here what your saying is, my model is saying, okay,",
        "start": 444.295,
        "duration": 3.225
    },
    {
        "text": "using TensorFlow backend for keras,",
        "start": 447.52,
        "duration": 2.085
    },
    {
        "text": "and it giving me, if you can see,",
        "start": 449.605,
        "duration": 1.455
    },
    {
        "text": "two flags, two numbers.",
        "start": 451.06,
        "duration": 1.505
    },
    {
        "text": "If you see the first flag is for the non-malignant,",
        "start": 452.565,
        "duration": 3.68
    },
    {
        "text": "the second flag is for the malignancy.",
        "start": 456.245,
        "duration": 2.33
    },
    {
        "text": "If you see, the coding logic goes in this way,",
        "start": 458.575,
        "duration": 2.39
    },
    {
        "text": "whichever is bigger is the flag, the predicted flag.",
        "start": 460.965,
        "duration": 3.59
    },
    {
        "text": "As you can see here, my malignancy flag",
        "start": 464.555,
        "duration": 2.3
    },
    {
        "text": "is much higher than the non-malignant flag.",
        "start": 466.855,
        "duration": 2.28
    },
    {
        "text": "So, that's why the predicted label",
        "start": 469.135,
        "duration": 2.325
    },
    {
        "text": "comes out to be malignant here.",
        "start": 471.46,
        "duration": 1.43
    },
    {
        "text": ">> I see. So maybe if you're predicting between",
        "start": 472.89,
        "duration": 2.445
    },
    {
        "text": "different kinds of melanoma it",
        "start": 475.335,
        "duration": 1.695
    },
    {
        "text": "might be a larger vector with- okay. Got it.",
        "start": 477.03,
        "duration": 2.03
    },
    {
        "text": ">> Right, this is a binary",
        "start": 479.06,
        "duration": 1.07
    },
    {
        "text": "classification that we are doing.",
        "start": 480.13,
        "duration": 1.305
    },
    {
        "text": "A classification of two types of things.",
        "start": 481.435,
        "duration": 1.93
    },
    {
        "text": "Suppose I had lesions, moles,",
        "start": 483.365,
        "duration": 2.815
    },
    {
        "text": "different types of dermatitis to be categorized,",
        "start": 486.18,
        "duration": 3.295
    },
    {
        "text": "I would have a multi-label classification problem.",
        "start": 489.475,
        "duration": 2.76
    },
    {
        "text": "That is, I have multiple lebels for",
        "start": 492.235,
        "duration": 1.775
    },
    {
        "text": "multiple different types of problems.",
        "start": 494.01,
        "duration": 1.885
    },
    {
        "text": ">> And in this case you will",
        "start": 495.895,
        "duration": 1.255
    },
    {
        "text": "train a different model that would have",
        "start": 497.15,
        "duration": 1.425
    },
    {
        "text": "multiple outputs and you will",
        "start": 498.575,
        "duration": 1.375
    },
    {
        "text": "give it different data from it.",
        "start": 499.95,
        "duration": 1.37
    },
    {
        "text": ">> Right, it just depends",
        "start": 501.32,
        "duration": 1.3
    },
    {
        "text": "on how you're training your model,",
        "start": 502.62,
        "duration": 1.5
    },
    {
        "text": "how you're defining your model.",
        "start": 504.12,
        "duration": 1.3
    },
    {
        "text": "And the model that I'm using could be",
        "start": 505.42,
        "duration": 1.655
    },
    {
        "text": "used for multi-label classification also.",
        "start": 507.075,
        "duration": 1.645
    },
    {
        "text": "All you need to do is just change your number of",
        "start": 508.72,
        "duration": 2.31
    },
    {
        "text": "labels and your label list and it will work seamlessly.",
        "start": 511.03,
        "duration": 3.88
    },
    {
        "text": "For binary as well as for multilevel classification.",
        "start": 514.91,
        "duration": 2.57
    },
    {
        "text": ">> Fantastic. So when you're building this and this",
        "start": 517.48,
        "duration": 2.12
    },
    {
        "text": "is going to get to the heart of what a data scientist is,",
        "start": 519.6,
        "duration": 2.33
    },
    {
        "text": "because I understand Machine Learning models from",
        "start": 521.93,
        "duration": 2.09
    },
    {
        "text": "a mathematical perspective but I've",
        "start": 524.02,
        "duration": 1.56
    },
    {
        "text": "never actually done anything as a data scientist.",
        "start": 525.58,
        "duration": 2.665
    },
    {
        "text": "How did you know to use",
        "start": 528.245,
        "duration": 1.855
    },
    {
        "text": "a convolutional neural network and how",
        "start": 530.1,
        "duration": 1.95
    },
    {
        "text": "did you know to set it up the way you did?",
        "start": 532.05,
        "duration": 2.17
    },
    {
        "text": "Is there just like a trial and error.",
        "start": 534.22,
        "duration": 1.765
    },
    {
        "text": ">> It's mostly a trial and error process,",
        "start": 535.985,
        "duration": 2.015
    },
    {
        "text": "specially if you're training your network from scratch.",
        "start": 538.0,
        "duration": 2.515
    },
    {
        "text": "If I would have been using",
        "start": 540.515,
        "duration": 1.255
    },
    {
        "text": "something called transfer learning,",
        "start": 541.77,
        "duration": 1.35
    },
    {
        "text": "which is very common for image classification,",
        "start": 543.12,
        "duration": 2.52
    },
    {
        "text": "I could have avoided a lot of complexity.",
        "start": 545.64,
        "duration": 2.19
    },
    {
        "text": "But because healthcare images",
        "start": 547.83,
        "duration": 1.545
    },
    {
        "text": "are very, very domain specific,",
        "start": 549.375,
        "duration": 1.72
    },
    {
        "text": "you need to have your hyperparameters",
        "start": 551.095,
        "duration": 2.74
    },
    {
        "text": "adjusted correctly to make your model learn very well.",
        "start": 553.835,
        "duration": 3.765
    },
    {
        "text": "For example, if you see here,",
        "start": 557.6,
        "duration": 1.975
    },
    {
        "text": "in the training code I'm opening,",
        "start": 559.575,
        "duration": 2.735
    },
    {
        "text": "it's giving me an accuracy,",
        "start": 562.31,
        "duration": 2.465
    },
    {
        "text": "a training accuracy of almost 90 percent.",
        "start": 564.775,
        "duration": 4.805
    },
    {
        "text": "So, that is a very high accuracy.",
        "start": 569.58,
        "duration": 2.465
    },
    {
        "text": "If I would have been using transfer learning,",
        "start": 572.045,
        "duration": 1.8
    },
    {
        "text": "where I'm taking a pre-trained generic model.",
        "start": 573.845,
        "duration": 2.33
    },
    {
        "text": "Pre-trained on image net or something like that,",
        "start": 576.175,
        "duration": 2.18
    },
    {
        "text": "and I'm trying to modify it for the healthcare,",
        "start": 578.355,
        "duration": 2.59
    },
    {
        "text": "it would have got me to around 80,",
        "start": 580.945,
        "duration": 2.025
    },
    {
        "text": "85 percent at most.",
        "start": 582.97,
        "duration": 1.055
    },
    {
        "text": ">> And that makes sense, because",
        "start": 584.025,
        "duration": 1.135
    },
    {
        "text": "a transfer learning has been trained on a generic set of",
        "start": 585.16,
        "duration": 2.76
    },
    {
        "text": "images and these images",
        "start": 587.92,
        "duration": 2.485
    },
    {
        "text": "they're generally not at the generic set.",
        "start": 590.405,
        "duration": 2.33
    },
    {
        "text": ">> You're right, it's very, very domain-specific.",
        "start": 592.735,
        "duration": 2.305
    },
    {
        "text": "It has different types of information,",
        "start": 595.04,
        "duration": 2.1
    },
    {
        "text": "color information, age information,",
        "start": 597.14,
        "duration": 1.63
    },
    {
        "text": "dimension information embedded inside.",
        "start": 598.77,
        "duration": 1.9
    },
    {
        "text": "So we want to learn all of those,",
        "start": 600.67,
        "duration": 1.54
    },
    {
        "text": "you need to create things from scratch.",
        "start": 602.21,
        "duration": 1.72
    },
    {
        "text": "Hyperparameter optimization is the hardest thing",
        "start": 603.93,
        "duration": 2.45
    },
    {
        "text": "when you're training a model from scratch.",
        "start": 606.38,
        "duration": 1.33
    },
    {
        "text": "So, I had to spend a lot of time while",
        "start": 607.71,
        "duration": 2.05
    },
    {
        "text": "training on the hyperparameter optimization part.",
        "start": 609.76,
        "duration": 3.455
    },
    {
        "text": ">> Why don't you describe what a hyperparameter is.",
        "start": 613.215,
        "duration": 2.295
    },
    {
        "text": "That way people that are looking,",
        "start": 615.51,
        "duration": 1.38
    },
    {
        "text": "that maybe are new to data science can understand.",
        "start": 616.89,
        "duration": 2.61
    },
    {
        "text": ">> So, hyperparameters, for example,",
        "start": 619.5,
        "duration": 2.495
    },
    {
        "text": "let's take the classification example",
        "start": 621.995,
        "duration": 1.735
    },
    {
        "text": "because hyperparameters can be huge.",
        "start": 623.73,
        "duration": 2.03
    },
    {
        "text": "For classification models,",
        "start": 625.76,
        "duration": 2.105
    },
    {
        "text": "hyperparameters are your learning rate,",
        "start": 627.865,
        "duration": 2.245
    },
    {
        "text": "your initial learning rate.",
        "start": 630.11,
        "duration": 1.41
    },
    {
        "text": "Your learning rate decay.",
        "start": 631.52,
        "duration": 1.635
    },
    {
        "text": "That is because you're doing convolution,",
        "start": 633.155,
        "duration": 2.135
    },
    {
        "text": "you're just learning in the forward pass and then",
        "start": 635.29,
        "duration": 2.21
    },
    {
        "text": "you're decaying it when you're doing the backward pass.",
        "start": 637.5,
        "duration": 2.44
    },
    {
        "text": "So, there is learning rate, there's learning rate decay.",
        "start": 639.94,
        "duration": 2.51
    },
    {
        "text": "And there is also something called the batch size,",
        "start": 642.45,
        "duration": 2.84
    },
    {
        "text": "so you're trying to learn in batches.",
        "start": 645.29,
        "duration": 2.43
    },
    {
        "text": "So, you cannot just give all of your data and",
        "start": 647.72,
        "duration": 1.95
    },
    {
        "text": "your memory would be overflown.",
        "start": 649.67,
        "duration": 2.415
    },
    {
        "text": "So, you would need to identify what is your batch size.",
        "start": 652.085,
        "duration": 3.315
    },
    {
        "text": "Another thing you need is the optimizer.",
        "start": 655.4,
        "duration": 2.185
    },
    {
        "text": "That is actually defined for describing your loss.",
        "start": 657.585,
        "duration": 4.6
    },
    {
        "text": "For me, I spent the most time",
        "start": 662.185,
        "duration": 2.055
    },
    {
        "text": "on the optimization part of the loss.",
        "start": 664.24,
        "duration": 2.45
    },
    {
        "text": "I have been using",
        "start": 666.69,
        "duration": 1.885
    },
    {
        "text": "RMSprop but it was not converging my model.",
        "start": 668.575,
        "duration": 3.785
    },
    {
        "text": "So, I had to change to AdamOptimizer",
        "start": 672.36,
        "duration": 2.145
    },
    {
        "text": "and had to change the values",
        "start": 674.505,
        "duration": 1.765
    },
    {
        "text": "of the AdamOptimizer to make my modern",
        "start": 676.27,
        "duration": 1.87
    },
    {
        "text": "converge and come to a 90 percent accuracy.",
        "start": 678.14,
        "duration": 2.21
    },
    {
        "text": ">> I see. So the hyperparameters are",
        "start": 680.35,
        "duration": 1.79
    },
    {
        "text": "sort of the extra values that you put,",
        "start": 682.14,
        "duration": 2.09
    },
    {
        "text": "the secret ingredients into the actual model.",
        "start": 684.23,
        "duration": 2.49
    },
    {
        "text": ">> Yes, that's a secret sauce of the chef.",
        "start": 686.72,
        "duration": 2.007
    },
    {
        "text": "Usually, there's a lot of trial and error to do that.",
        "start": 688.727,
        "duration": 2.563
    },
    {
        "text": ">> It's a lot of trial and error.",
        "start": 691.29,
        "duration": 1.465
    },
    {
        "text": "To train the model with Azure Machine Learning,",
        "start": 692.755,
        "duration": 2.72
    },
    {
        "text": "it took me very less time.",
        "start": 695.475,
        "duration": 1.225
    },
    {
        "text": "But to build the model,",
        "start": 696.7,
        "duration": 1.415
    },
    {
        "text": "that is where I spent a week to bring",
        "start": 698.115,
        "duration": 2.4
    },
    {
        "text": "my hyperparameters in tune with",
        "start": 700.515,
        "duration": 2.26
    },
    {
        "text": "each other so that they are working in perfect sync.",
        "start": 702.775,
        "duration": 2.645
    },
    {
        "text": ">> That's awesome. So, this makes perfect sense.",
        "start": 705.42,
        "duration": 2.375
    },
    {
        "text": "The thing that maybe is a little",
        "start": 707.795,
        "duration": 1.33
    },
    {
        "text": "bit more unclear to me is,",
        "start": 709.125,
        "duration": 1.815
    },
    {
        "text": "how do you take these models?",
        "start": 710.94,
        "duration": 1.945
    },
    {
        "text": "Because I saw how you did the prediction or",
        "start": 712.885,
        "duration": 1.96
    },
    {
        "text": "the inferencing in here with Python,",
        "start": 714.845,
        "duration": 2.0
    },
    {
        "text": "but how do you actually take this model",
        "start": 716.845,
        "duration": 2.07
    },
    {
        "text": "and put it onto",
        "start": 718.915,
        "duration": 0.82
    },
    {
        "text": "a device that can do Azure Machine Learning?",
        "start": 719.735,
        "duration": 1.42
    },
    {
        "text": ">> Right. For that, let me show",
        "start": 721.155,
        "duration": 1.5
    },
    {
        "text": "you a conversion code that we are doing.",
        "start": 722.655,
        "duration": 2.425
    },
    {
        "text": "So, here, is you've seen the code.",
        "start": 725.08,
        "duration": 1.775
    },
    {
        "text": "What we're trying to do, we",
        "start": 726.855,
        "duration": 1.05
    },
    {
        "text": "develop the keras model, okay?",
        "start": 727.905,
        "duration": 1.705
    },
    {
        "text": "We're loading up the keras model.",
        "start": 729.61,
        "duration": 1.835
    },
    {
        "text": "What we have done is we have",
        "start": 731.445,
        "duration": 1.75
    },
    {
        "text": "installed Core ML in Azure Machine Learning,",
        "start": 733.195,
        "duration": 2.535
    },
    {
        "text": "and I'm trying to convert",
        "start": 735.73,
        "duration": 1.7
    },
    {
        "text": "the keras model to the Core ML format.",
        "start": 737.43,
        "duration": 3.755
    },
    {
        "text": "So, let me define a little bit what Core ML is.",
        "start": 741.185,
        "duration": 2.585
    },
    {
        "text": "Core ML is basically Apple's package of compression.",
        "start": 743.77,
        "duration": 3.415
    },
    {
        "text": "It actually helps you compress",
        "start": 747.185,
        "duration": 2.19
    },
    {
        "text": "your model weights in",
        "start": 749.375,
        "duration": 1.77
    },
    {
        "text": "the format with all the dependent informations,",
        "start": 751.145,
        "duration": 2.935
    },
    {
        "text": "so that you can run it on any device.",
        "start": 754.08,
        "duration": 2.495
    },
    {
        "text": "So, imagine something like sudo apt-get, right?",
        "start": 756.575,
        "duration": 3.465
    },
    {
        "text": "So, imagine you have",
        "start": 760.04,
        "duration": 1.385
    },
    {
        "text": "a deep learning environment, your dependencies,",
        "start": 761.425,
        "duration": 2.67
    },
    {
        "text": "your model, everything inside,",
        "start": 764.095,
        "duration": 1.91
    },
    {
        "text": "your workbench, or your Azure Machine Learning,",
        "start": 766.005,
        "duration": 1.96
    },
    {
        "text": "or wherever in your local machine.",
        "start": 767.965,
        "duration": 1.72
    },
    {
        "text": "And you want to bring all of them together,",
        "start": 769.685,
        "duration": 2.745
    },
    {
        "text": "as well as you want to compress the weights",
        "start": 772.43,
        "duration": 2.135
    },
    {
        "text": "a little bit because when you train",
        "start": 774.565,
        "duration": 1.67
    },
    {
        "text": "a Machine Learning model, it's huge.",
        "start": 776.235,
        "duration": 1.88
    },
    {
        "text": ">> Right.",
        "start": 778.115,
        "duration": 0.31
    },
    {
        "text": ">> Right? So, you would want to compress it so that",
        "start": 778.425,
        "duration": 2.18
    },
    {
        "text": "your phone processor can run it seamlessly.",
        "start": 780.605,
        "duration": 2.475
    },
    {
        "text": ">> Right.",
        "start": 783.08,
        "duration": 0.31
    },
    {
        "text": ">> So, all of these steps are taken care of by Core ML.",
        "start": 783.39,
        "duration": 3.685
    },
    {
        "text": "So, all I needed to do was call Core ML converter,",
        "start": 787.075,
        "duration": 3.55
    },
    {
        "text": "and through a few lines",
        "start": 790.625,
        "duration": 2.0
    },
    {
        "text": "of code like three or four lines of code,",
        "start": 792.625,
        "duration": 1.825
    },
    {
        "text": "it was able to take all my care",
        "start": 794.45,
        "duration": 1.715
    },
    {
        "text": "as dependencies from my environment",
        "start": 796.165,
        "duration": 1.895
    },
    {
        "text": "along with the model and compress it and",
        "start": 798.06,
        "duration": 1.795
    },
    {
        "text": "create a full package, Core ML package.",
        "start": 799.855,
        "duration": 2.405
    },
    {
        "text": "So, when it creates a Core ML package,",
        "start": 802.26,
        "duration": 1.595
    },
    {
        "text": "it's basically the name.mlmodel.",
        "start": 803.855,
        "duration": 3.575
    },
    {
        "text": ">> Right.",
        "start": 807.43,
        "duration": 0.445
    },
    {
        "text": ">> Right? So, anything star.mlmodel is basically,",
        "start": 807.875,
        "duration": 3.04
    },
    {
        "text": "you're getting a Core ML package.",
        "start": 810.915,
        "duration": 1.63
    },
    {
        "text": ">> And that makes sense because",
        "start": 812.545,
        "duration": 1.28
    },
    {
        "text": "when you're doing inferencing,",
        "start": 813.825,
        "duration": 1.64
    },
    {
        "text": "you're not going to need to have",
        "start": 815.465,
        "duration": 1.56
    },
    {
        "text": "anything having to do with the learning rate,",
        "start": 817.025,
        "duration": 2.335
    },
    {
        "text": "or the decay weight,",
        "start": 819.36,
        "duration": 1.195
    },
    {
        "text": "or whatever optimize, you don't need that at all.",
        "start": 820.555,
        "duration": 2.7
    },
    {
        "text": "You just need all the numbers that are getting",
        "start": 823.255,
        "duration": 2.22
    },
    {
        "text": "multiplied together and then how they're",
        "start": 825.475,
        "duration": 1.42
    },
    {
        "text": "going to be output.",
        "start": 826.895,
        "duration": 1.42
    },
    {
        "text": ">> Exactly. You're so right.",
        "start": 828.315,
        "duration": 1.37
    },
    {
        "text": "So now, once we have converted that,",
        "start": 829.685,
        "duration": 3.135
    },
    {
        "text": "the main reason why we are using a Mac today is we put",
        "start": 832.82,
        "duration": 3.325
    },
    {
        "text": "that Core ML model into Xcode and compiled it.",
        "start": 836.145,
        "duration": 3.975
    },
    {
        "text": "That produces a star.mlmodel C file, okay?",
        "start": 840.12,
        "duration": 3.945
    },
    {
        "text": "So, that produced a compiled version of the Core ML.",
        "start": 844.065,
        "duration": 3.195
    },
    {
        "text": "Now, we can drop it on any application and run it.",
        "start": 847.26,
        "duration": 3.145
    },
    {
        "text": "For us, we're using Xamarin.",
        "start": 850.405,
        "duration": 1.67
    },
    {
        "text": "And let me show you the Xamarin.",
        "start": 852.075,
        "duration": 1.27
    },
    {
        "text": ">> Let's do it.",
        "start": 853.345,
        "duration": 0.49
    },
    {
        "text": ">> So, as you can see in my screen here,",
        "start": 853.835,
        "duration": 4.065
    },
    {
        "text": "I have the Xamarin app uploaded.",
        "start": 857.9,
        "duration": 2.395
    },
    {
        "text": "As you can see in the resources,",
        "start": 860.295,
        "duration": 2.005
    },
    {
        "text": "I have dropped the skin cancer_mlmodel.cfile,",
        "start": 862.3,
        "duration": 3.645
    },
    {
        "text": "the compiled one, which I have compiled using Xcode.",
        "start": 865.945,
        "duration": 3.83
    },
    {
        "text": "Here, as you can see after compilation,",
        "start": 869.775,
        "duration": 2.52
    },
    {
        "text": "it has created different versions of it.",
        "start": 872.295,
        "duration": 1.82
    },
    {
        "text": "When I first created the model,",
        "start": 874.115,
        "duration": 2.005
    },
    {
        "text": "it was all just a model, just scareus.HDF5 file.",
        "start": 876.12,
        "duration": 3.545
    },
    {
        "text": "But, when I have compiled it,",
        "start": 879.665,
        "duration": 2.025
    },
    {
        "text": "it has created all the separate files",
        "start": 881.69,
        "duration": 2.085
    },
    {
        "text": "so that my app can effectively read it.",
        "start": 883.775,
        "duration": 2.225
    },
    {
        "text": ">> I see.",
        "start": 886.0,
        "duration": 0.835
    },
    {
        "text": ">> That is what Core ML does.",
        "start": 886.835,
        "duration": 1.635
    },
    {
        "text": "It takes every dependency.",
        "start": 888.47,
        "duration": 1.99
    },
    {
        "text": "So, it takes, okay, what is the shreds?",
        "start": 890.46,
        "duration": 1.605
    },
    {
        "text": "What is the shape?",
        "start": 892.065,
        "duration": 0.78
    },
    {
        "text": "What is the architecture of",
        "start": 892.845,
        "duration": 1.195
    },
    {
        "text": "my model? What are the bindery?",
        "start": 894.04,
        "duration": 1.76
    },
    {
        "text": "What are the DLLs? What are my dependencies?",
        "start": 895.8,
        "duration": 2.295
    },
    {
        "text": "So, everything comes along with the model.",
        "start": 898.095,
        "duration": 2.44
    },
    {
        "text": ">> I see. So, you should think of",
        "start": 900.535,
        "duration": 1.43
    },
    {
        "text": "this as importing an assembly,",
        "start": 901.965,
        "duration": 2.07
    },
    {
        "text": "or a Azure file, or some kind of",
        "start": 904.035,
        "duration": 1.615
    },
    {
        "text": "package that you're going to call to do the inferencing.",
        "start": 905.65,
        "duration": 1.955
    },
    {
        "text": ">> Exactly.",
        "start": 907.605,
        "duration": 0.63
    },
    {
        "text": ">> Okay.",
        "start": 908.235,
        "duration": 0.44
    },
    {
        "text": ">> This is very, very developer-friendly thing,",
        "start": 908.675,
        "duration": 2.27
    },
    {
        "text": "and you can actually take any model convert it to",
        "start": 910.945,
        "duration": 2.98
    },
    {
        "text": "Core ML compiled version and run it on any app. Okay.",
        "start": 913.925,
        "duration": 3.665
    },
    {
        "text": ">> So, if you have a data scientists",
        "start": 917.59,
        "duration": 1.215
    },
    {
        "text": "in-house that has made some amazing model,",
        "start": 918.805,
        "duration": 1.97
    },
    {
        "text": "used keras, or TensorFlow,",
        "start": 920.775,
        "duration": 1.45
    },
    {
        "text": "or something else that's pretty",
        "start": 922.225,
        "duration": 1.54
    },
    {
        "text": "standard, you can ask them,",
        "start": 923.765,
        "duration": 1.54
    },
    {
        "text": "\"Hey can you export this to an ML model,",
        "start": 925.305,
        "duration": 2.445
    },
    {
        "text": "and then use it in your application?\"",
        "start": 927.75,
        "duration": 1.52
    },
    {
        "text": ">> Exactly. In fact, what we have been doing",
        "start": 929.27,
        "duration": 1.915
    },
    {
        "text": "is we are a team of data scientists and developers.",
        "start": 931.185,
        "duration": 2.165
    },
    {
        "text": "So, when I produce a model and give it to my developer,",
        "start": 933.35,
        "duration": 2.325
    },
    {
        "text": "my developer knows to go to the Core ML,",
        "start": 935.675,
        "duration": 1.91
    },
    {
        "text": "just convert it, and just drop it onto the app.",
        "start": 937.585,
        "duration": 1.95
    },
    {
        "text": ">> Yeah. Because it's again,",
        "start": 939.535,
        "duration": 1.62
    },
    {
        "text": "just an assembly like you would,",
        "start": 941.155,
        "duration": 1.295
    },
    {
        "text": "like a PDF library.",
        "start": 942.45,
        "duration": 1.315
    },
    {
        "text": "Nobody knows how PDFs are written",
        "start": 943.765,
        "duration": 1.7
    },
    {
        "text": "but we know that if you call library, it will make one.",
        "start": 945.465,
        "duration": 1.84
    },
    {
        "text": "Same thing here except it will",
        "start": 947.305,
        "duration": 1.6
    },
    {
        "text": "detect whether or not someone may have skin cancer.",
        "start": 948.905,
        "duration": 2.26
    },
    {
        "text": ">> That's right.",
        "start": 951.165,
        "duration": 0.41
    },
    {
        "text": ">> Amazing.",
        "start": 951.575,
        "duration": 0.575
    },
    {
        "text": ">> Yes. So, once you have that,",
        "start": 952.15,
        "duration": 2.35
    },
    {
        "text": "you go to the ViewController.csv file, and there,",
        "start": 954.5,
        "duration": 4.355
    },
    {
        "text": "as you can see, this is a sample app you",
        "start": 958.855,
        "duration": 2.54
    },
    {
        "text": "can find in the Microsoft GitHub.",
        "start": 961.395,
        "duration": 2.8
    },
    {
        "text": ">> We'll make sure to put links. Yeah, we'll put links.",
        "start": 964.195,
        "duration": 1.65
    },
    {
        "text": ">> Yes. So, here,",
        "start": 965.845,
        "duration": 2.64
    },
    {
        "text": "as you can see, I have just",
        "start": 968.485,
        "duration": 1.65
    },
    {
        "text": "pointed it to the model file name,",
        "start": 970.135,
        "duration": 1.96
    },
    {
        "text": "and going down below in the logic,",
        "start": 972.095,
        "duration": 4.28
    },
    {
        "text": "where you're actually doing",
        "start": 976.375,
        "duration": 2.515
    },
    {
        "text": "the calling of the prediction function,",
        "start": 978.89,
        "duration": 2.22
    },
    {
        "text": "you're saying, \"Okay if my ClassName is benign.\"",
        "start": 981.11,
        "duration": 2.085
    },
    {
        "text": "ClassName is basically the label",
        "start": 983.195,
        "duration": 1.38
    },
    {
        "text": "of the predicted label that you are getting.",
        "start": 984.575,
        "duration": 2.505
    },
    {
        "text": "Then suggest that you're all clear.",
        "start": 987.08,
        "duration": 2.255
    },
    {
        "text": "However, if the ClassName is",
        "start": 989.335,
        "duration": 2.72
    },
    {
        "text": "malignant as you were seeing on my Azure ML workbench,",
        "start": 992.055,
        "duration": 3.005
    },
    {
        "text": "it's suggesting to you to go see a doctor.",
        "start": 995.06,
        "duration": 5.545
    },
    {
        "text": ">> So, that's it?",
        "start": 1000.605,
        "duration": 1.67
    },
    {
        "text": "I feel like a lot of times when we talk about",
        "start": 1002.275,
        "duration": 1.8
    },
    {
        "text": "Machine Learning it's like voodoo magic.",
        "start": 1004.075,
        "duration": 2.03
    },
    {
        "text": ">> Right.",
        "start": 1006.105,
        "duration": 0.22
    },
    {
        "text": ">> And when we look at the app initially,",
        "start": 1006.325,
        "duration": 1.8
    },
    {
        "text": "it felt a little voodo magicish.",
        "start": 1008.125,
        "duration": 1.82
    },
    {
        "text": "But now, that we've gone through, we saw the model was",
        "start": 1009.945,
        "duration": 1.98
    },
    {
        "text": "a convolutional neural network built in keras.",
        "start": 1011.925,
        "duration": 2.165
    },
    {
        "text": "A lot of time was spent",
        "start": 1014.09,
        "duration": 1.405
    },
    {
        "text": "on making sure the hyperparameters were okay.",
        "start": 1015.495,
        "duration": 2.5
    },
    {
        "text": "Once they were done, you did",
        "start": 1017.995,
        "duration": 1.24
    },
    {
        "text": "some inferencing in Python to make sure it",
        "start": 1019.235,
        "duration": 1.75
    },
    {
        "text": "was like your internal check then you export to ML model,",
        "start": 1020.985,
        "duration": 3.24
    },
    {
        "text": "and then the ML model then goes",
        "start": 1024.225,
        "duration": 1.35
    },
    {
        "text": "into a Xamarin app, and you're done.",
        "start": 1025.575,
        "duration": 1.93
    },
    {
        "text": ">> Yes. That's as simple as that.",
        "start": 1027.505,
        "duration": 1.755
    },
    {
        "text": "In fact, the Xamarin app part is super easy.",
        "start": 1029.26,
        "duration": 3.17
    },
    {
        "text": "And we'll put links in there.",
        "start": 1032.43,
        "duration": 1.715
    },
    {
        "text": "You take the sample app,",
        "start": 1034.145,
        "duration": 1.185
    },
    {
        "text": "you just changed three or four lines of code and",
        "start": 1035.33,
        "duration": 2.165
    },
    {
        "text": "you have your own specific app running.",
        "start": 1037.495,
        "duration": 2.315
    },
    {
        "text": "So instead of, like a skin cancer,",
        "start": 1039.81,
        "duration": 1.415
    },
    {
        "text": "if you wanted a fruit classification,",
        "start": 1041.225,
        "duration": 1.675
    },
    {
        "text": "you can very easily do that.",
        "start": 1042.9,
        "duration": 0.985
    },
    {
        "text": ">> That's amazing.",
        "start": 1043.885,
        "duration": 1.315
    },
    {
        "text": ">> Yeah.",
        "start": 1045.2,
        "duration": 0.395
    },
    {
        "text": ">> I mean, the thing that I like about this",
        "start": 1045.595,
        "duration": 1.77
    },
    {
        "text": "is that we often,",
        "start": 1047.365,
        "duration": 2.11
    },
    {
        "text": "and I spent a lot of time",
        "start": 1049.475,
        "duration": 1.2
    },
    {
        "text": "studying Machine Learning models,",
        "start": 1050.675,
        "duration": 1.38
    },
    {
        "text": "and when we're talking about optimizing leads,",
        "start": 1052.055,
        "duration": 1.49
    },
    {
        "text": "and she's like, \"I had an optimizer of my own.\"",
        "start": 1053.545,
        "duration": 1.66
    },
    {
        "text": "\"Oh, that's really cool, how",
        "start": 1055.205,
        "duration": 1.29
    },
    {
        "text": "did you figure out your batch size?",
        "start": 1056.495,
        "duration": 1.715
    },
    {
        "text": "How did you know the learning weight was 0.1 instead?\"",
        "start": 1058.21,
        "duration": 2.37
    },
    {
        "text": "That to me is exciting, but then actually seeing from",
        "start": 1060.58,
        "duration": 2.365
    },
    {
        "text": "beginning idea all the",
        "start": 1062.945,
        "duration": 1.61
    },
    {
        "text": "way into an application, it's pretty amazing too.",
        "start": 1064.555,
        "duration": 1.74
    },
    {
        "text": "And in short amount of time, we've only been",
        "start": 1066.295,
        "duration": 1.36
    },
    {
        "text": "spending we're 15 minutes in probably.",
        "start": 1067.655,
        "duration": 1.89
    },
    {
        "text": ">> Yeah.",
        "start": 1069.545,
        "duration": 0.46
    },
    {
        "text": ">> Awesome! So, where can people go to find out more?",
        "start": 1070.005,
        "duration": 2.46
    },
    {
        "text": "Or, is there anything that I'm missing?",
        "start": 1072.465,
        "duration": 1.375
    },
    {
        "text": ">> No. You are not missing anything.",
        "start": 1073.84,
        "duration": 1.935
    },
    {
        "text": "So, we do have Azure GitHub link",
        "start": 1075.775,
        "duration": 2.135
    },
    {
        "text": "and the whole project in the Azure docs.",
        "start": 1077.91,
        "duration": 2.245
    },
    {
        "text": "So, we will give you the link below in the video.",
        "start": 1080.155,
        "duration": 2.92
    },
    {
        "text": ">> Awesome. Well, this has been amazing.",
        "start": 1083.075,
        "duration": 1.51
    },
    {
        "text": "Thanks so much for spending some time",
        "start": 1084.585,
        "duration": 1.27
    },
    {
        "text": "with us. Thanks so much for watching.",
        "start": 1085.855,
        "duration": 1.2
    },
    {
        "text": "You've been learning about computing at the edge,",
        "start": 1087.055,
        "duration": 2.32
    },
    {
        "text": "and we saw an entire example that's pretty",
        "start": 1089.375,
        "duration": 2.08
    },
    {
        "text": "amazing in just a couple of minutes.",
        "start": 1091.455,
        "duration": 2.31
    },
    {
        "text": "Thanks so much for watching. We'll see you next time.",
        "start": 1093.765,
        "duration": 1.545
    },
    {
        "text": "Take care.",
        "start": 1095.31,
        "duration": 3.25
    },
    {
        "text": ">> Thank you. Bye bye.",
        "start": 1098.56,
        "duration": 3.265
    }
]