[
    {
        "text": ">> You're not going to want to miss this episode of",
        "start": 0.0,
        "duration": 1.8
    },
    {
        "text": "the AI Show where we delve",
        "start": 1.8,
        "duration": 1.5
    },
    {
        "text": "into some of the ethical concerns with AI,",
        "start": 3.3,
        "duration": 3.36
    },
    {
        "text": "how to think about it well,",
        "start": 6.66,
        "duration": 1.17
    },
    {
        "text": "and what you can do today to make your software ethical.",
        "start": 7.83,
        "duration": 3.24
    },
    {
        "text": "If it uses AI, make sure you tune in.",
        "start": 11.07,
        "duration": 1.62
    },
    {
        "text": "[MUSIC]",
        "start": 12.69,
        "duration": 8.06
    },
    {
        "text": ">> Hello and welcome to this special edition of the AI Show.",
        "start": 20.75,
        "duration": 2.15
    },
    {
        "text": "We're going to talk a little bit about ethics.",
        "start": 22.9,
        "duration": 3.05
    },
    {
        "text": "I have some special guests with me.",
        "start": 25.95,
        "duration": 1.95
    },
    {
        "text": "Let's start with you, Josh, tell us who you are and what you do.",
        "start": 27.9,
        "duration": 1.86
    },
    {
        "text": ">> Hey, thanks for having me.",
        "start": 29.76,
        "duration": 1.385
    },
    {
        "text": "I'm Josh Lovejoy.",
        "start": 31.145,
        "duration": 1.095
    },
    {
        "text": "I lead design for a team called Ethics and Society at Microsoft.",
        "start": 32.24,
        "duration": 4.29
    },
    {
        "text": "What our team does is a little bit like",
        "start": 36.53,
        "duration": 2.145
    },
    {
        "text": "a design agency meets ethics.",
        "start": 38.675,
        "duration": 3.285
    },
    {
        "text": "We partner with product teams like right in",
        "start": 41.96,
        "duration": 3.0
    },
    {
        "text": "the mix of making actual products,",
        "start": 44.96,
        "duration": 2.76
    },
    {
        "text": "and try to collaboratively answer the question,",
        "start": 47.72,
        "duration": 2.58
    },
    {
        "text": "what does it mean to be responsible?",
        "start": 50.3,
        "duration": 2.54
    },
    {
        "text": ">> Awesome. Sarah.",
        "start": 52.84,
        "duration": 2.255
    },
    {
        "text": ">> Hey, I'm Sarah Bird,",
        "start": 55.095,
        "duration": 1.685
    },
    {
        "text": "and I am one of",
        "start": 56.78,
        "duration": 1.17
    },
    {
        "text": "those product teams that's",
        "start": 57.95,
        "duration": 1.35
    },
    {
        "text": "trying to figure out how to be responsible.",
        "start": 59.3,
        "duration": 2.34
    },
    {
        "text": "I lead responsible AI for the cognitive services,",
        "start": 61.64,
        "duration": 3.24
    },
    {
        "text": "and we work closely with Ethics and Society",
        "start": 64.88,
        "duration": 2.55
    },
    {
        "text": "to try to ensure responsible development and use of our products.",
        "start": 67.43,
        "duration": 4.5
    },
    {
        "text": ">> This is amazing, when it comes to",
        "start": 71.93,
        "duration": 2.1
    },
    {
        "text": "machine learning and AI, they're used interchangeably.",
        "start": 74.03,
        "duration": 2.91
    },
    {
        "text": "When it comes to machine learning specifically,",
        "start": 76.94,
        "duration": 2.145
    },
    {
        "text": "when I think about ethics and machine learning,",
        "start": 79.085,
        "duration": 3.075
    },
    {
        "text": "for some reason and maybe others have said this too,",
        "start": 82.16,
        "duration": 2.52
    },
    {
        "text": "I think it's a lot broader,",
        "start": 84.68,
        "duration": 1.05
    },
    {
        "text": "but I've heard a lot that the ethics of",
        "start": 85.73,
        "duration": 2.73
    },
    {
        "text": "machine learning models lies primarily only in the data,",
        "start": 88.46,
        "duration": 3.09
    },
    {
        "text": "and if your data is unethical or using data wrong.",
        "start": 91.55,
        "duration": 3.495
    },
    {
        "text": "But I've heard ethics centered around data.",
        "start": 95.045,
        "duration": 2.665
    },
    {
        "text": "I feel like that's a little short-sided.",
        "start": 97.71,
        "duration": 1.819
    },
    {
        "text": "Josh, what do you say about that?",
        "start": 99.529,
        "duration": 1.521
    },
    {
        "text": ">> Yeah, I think you're right.",
        "start": 101.05,
        "duration": 2.77
    },
    {
        "text": "Sometimes I relate it to",
        "start": 103.82,
        "duration": 2.715
    },
    {
        "text": "a teaching metaphor, machine learning teaching.",
        "start": 106.535,
        "duration": 5.58
    },
    {
        "text": "Saying that you're designing",
        "start": 112.115,
        "duration": 3.915
    },
    {
        "text": "a system and all you have to do is make",
        "start": 116.03,
        "duration": 1.8
    },
    {
        "text": "sure your data's right, is like saying,",
        "start": 117.83,
        "duration": 2.145
    },
    {
        "text": "you're going to design a course to teach people stuff,",
        "start": 119.975,
        "duration": 3.855
    },
    {
        "text": "and all you have to make sure is",
        "start": 123.83,
        "duration": 1.65
    },
    {
        "text": "that you have all of your references in order.",
        "start": 125.48,
        "duration": 4.59
    },
    {
        "text": "There's a lot more like,",
        "start": 130.07,
        "duration": 2.39
    },
    {
        "text": "who are your students,",
        "start": 132.46,
        "duration": 1.34
    },
    {
        "text": "and what are the subjects,",
        "start": 133.8,
        "duration": 1.65
    },
    {
        "text": "and what do they know already,",
        "start": 135.45,
        "duration": 1.215
    },
    {
        "text": "and where are they going to learn?",
        "start": 136.665,
        "duration": 1.58
    },
    {
        "text": "All these different pieces that come",
        "start": 138.245,
        "duration": 2.055
    },
    {
        "text": "to play a pretty significant role.",
        "start": 140.3,
        "duration": 3.09
    },
    {
        "text": "So data is super important, don't get me wrong,",
        "start": 143.39,
        "duration": 3.225
    },
    {
        "text": "but sometimes it misses a little bit of a bigger picture.",
        "start": 146.615,
        "duration": 4.73
    },
    {
        "text": ">> How do we frame this? Because I know when it comes to ethics,",
        "start": 151.345,
        "duration": 4.9
    },
    {
        "text": "it's all about literally the questions I",
        "start": 156.245,
        "duration": 2.685
    },
    {
        "text": "ask myself before I do something.",
        "start": 158.93,
        "duration": 2.76
    },
    {
        "text": "I feel like there's got to be large set of questions that we",
        "start": 161.69,
        "duration": 4.59
    },
    {
        "text": "ask ourselves when we build AI or machine learning systems. Josh.",
        "start": 166.28,
        "duration": 4.915
    },
    {
        "text": ">> Yeah, there's no shortage of questions.",
        "start": 171.195,
        "duration": 4.01
    },
    {
        "text": "The first one is really just to your point,",
        "start": 175.205,
        "duration": 3.195
    },
    {
        "text": "like when we talk about ethics,",
        "start": 178.4,
        "duration": 1.245
    },
    {
        "text": "the first thing is what are you trying to build and for who?",
        "start": 179.645,
        "duration": 3.745
    },
    {
        "text": "I might flip to some slides that",
        "start": 184.61,
        "duration": 2.59
    },
    {
        "text": "I put together for this talk, would be cool?",
        "start": 187.2,
        "duration": 2.67
    },
    {
        "text": ">> Let's do it.",
        "start": 189.87,
        "duration": 1.23
    },
    {
        "text": ">> Sometimes we start in",
        "start": 191.1,
        "duration": 2.07
    },
    {
        "text": "this really broad strokes picture about building technology.",
        "start": 193.17,
        "duration": 5.59
    },
    {
        "text": "People will show up with a really high level goal,",
        "start": 198.76,
        "duration": 4.765
    },
    {
        "text": "like speech recognition or computer vision or what have you.",
        "start": 203.525,
        "duration": 4.95
    },
    {
        "text": "Speech recognition, fantastic, overarching, technical capability.",
        "start": 208.475,
        "duration": 6.285
    },
    {
        "text": "Literally trying to turn the sound of",
        "start": 214.76,
        "duration": 2.79
    },
    {
        "text": "someone's voice into typed text so you can do stuff with it.",
        "start": 217.55,
        "duration": 3.615
    },
    {
        "text": "But then we end up with this interesting fiction question.",
        "start": 221.165,
        "duration": 6.25
    },
    {
        "text": "Well, how do we make this work for",
        "start": 227.415,
        "duration": 2.945
    },
    {
        "text": "people and who actually needs to use it?",
        "start": 230.36,
        "duration": 4.36
    },
    {
        "text": "This is where, as you said Seth,",
        "start": 234.73,
        "duration": 3.09
    },
    {
        "text": "there are a lot of questions.",
        "start": 237.82,
        "duration": 3.46
    },
    {
        "text": "But also they're the road-map towards getting",
        "start": 241.28,
        "duration": 3.5
    },
    {
        "text": "a stronger connection between",
        "start": 244.78,
        "duration": 1.41
    },
    {
        "text": "the technology and set of capabilities,",
        "start": 246.19,
        "duration": 1.59
    },
    {
        "text": "and a set of people, it's pretty unclear.",
        "start": 247.78,
        "duration": 2.83
    },
    {
        "text": "What are the things that our team tries to do is take on work,",
        "start": 250.61,
        "duration": 4.025
    },
    {
        "text": "working with folks like Sarah,",
        "start": 254.635,
        "duration": 1.71
    },
    {
        "text": "when actually it's really unclear what the steps",
        "start": 256.345,
        "duration": 4.185
    },
    {
        "text": "towards being responsible or",
        "start": 260.53,
        "duration": 2.205
    },
    {
        "text": "building reliable technology looks like.",
        "start": 262.735,
        "duration": 2.085
    },
    {
        "text": "If people have done it before,",
        "start": 264.82,
        "duration": 1.17
    },
    {
        "text": "then it's a known known.",
        "start": 265.99,
        "duration": 1.305
    },
    {
        "text": "But there are some things that are different about",
        "start": 267.295,
        "duration": 2.385
    },
    {
        "text": "this space fundamentally that require",
        "start": 269.68,
        "duration": 2.76
    },
    {
        "text": "us to really take this much more granular at times view of things.",
        "start": 272.44,
        "duration": 8.0
    },
    {
        "text": "It starts by, I think really just respecting",
        "start": 280.44,
        "duration": 2.945
    },
    {
        "text": "the work that we're used to doing and engineering.",
        "start": 283.385,
        "duration": 2.88
    },
    {
        "text": "We've neglected a little bit",
        "start": 286.265,
        "duration": 1.755
    },
    {
        "text": "with machine learning for some reason.",
        "start": 288.02,
        "duration": 1.86
    },
    {
        "text": "One of them is just respecting and",
        "start": 289.88,
        "duration": 1.92
    },
    {
        "text": "appreciating the types of constraints that we're working with.",
        "start": 291.8,
        "duration": 2.655
    },
    {
        "text": "Physicists have all constraints",
        "start": 294.455,
        "duration": 2.365
    },
    {
        "text": "that go into the work that they do,",
        "start": 296.82,
        "duration": 1.5
    },
    {
        "text": "and electrical engineers have",
        "start": 298.32,
        "duration": 1.05
    },
    {
        "text": "all constraints that go into the work that they do.",
        "start": 299.37,
        "duration": 2.24
    },
    {
        "text": "But sometimes when we're building AI and machine learning,",
        "start": 301.61,
        "duration": 2.88
    },
    {
        "text": "we're just like 200,000 cores that I",
        "start": 304.49,
        "duration": 3.98
    },
    {
        "text": "want to run concurrently to build my nine billion layer model,",
        "start": 308.47,
        "duration": 4.0
    },
    {
        "text": "whatever, let me do that.",
        "start": 312.47,
        "duration": 1.98
    },
    {
        "text": "Then when it comes to actually fit into a production system,",
        "start": 314.45,
        "duration": 2.61
    },
    {
        "text": "we have a lot of weird hacks that we have to employ.",
        "start": 317.06,
        "duration": 4.345
    },
    {
        "text": "Well, the other one is accuracy because accuracy is different",
        "start": 321.405,
        "duration": 3.245
    },
    {
        "text": "fundamentally in machine learning than it is in",
        "start": 324.65,
        "duration": 1.92
    },
    {
        "text": "traditional deterministic logic systems.",
        "start": 326.57,
        "duration": 2.115
    },
    {
        "text": "You can't just say, \"Yeah,",
        "start": 328.685,
        "duration": 1.245
    },
    {
        "text": "that's working as I intended it.\"",
        "start": 329.93,
        "duration": 1.665
    },
    {
        "text": "Because actually the fundamental value",
        "start": 331.595,
        "duration": 3.3
    },
    {
        "text": "of machine learning it's fuzzy,",
        "start": 334.895,
        "duration": 3.155
    },
    {
        "text": "it's fuzzy logic, it's a hunch building,",
        "start": 338.05,
        "duration": 2.48
    },
    {
        "text": "it's not truth building.",
        "start": 340.53,
        "duration": 3.01
    },
    {
        "text": "The addition and I'll throw and read off the top is, well,",
        "start": 343.58,
        "duration": 4.05
    },
    {
        "text": "you have to learn how you measure",
        "start": 347.63,
        "duration": 1.62
    },
    {
        "text": "not only accuracy, but also inaccuracy.",
        "start": 349.25,
        "duration": 3.225
    },
    {
        "text": "Another way of saying that from a human-centered perspective is,",
        "start": 352.475,
        "duration": 3.54
    },
    {
        "text": "you have to understand the things about people",
        "start": 356.015,
        "duration": 3.015
    },
    {
        "text": "that should contribute to a more accurate system.",
        "start": 359.03,
        "duration": 4.395
    },
    {
        "text": "You also have to understand the things about people that",
        "start": 363.425,
        "duration": 2.805
    },
    {
        "text": "shouldn't contribute to inaccuracies in the system.",
        "start": 366.23,
        "duration": 5.125
    },
    {
        "text": "Let me get crazy",
        "start": 371.355,
        "duration": 2.495
    },
    {
        "text": "for a second and then put them in a system screen.",
        "start": 373.85,
        "duration": 2.64
    },
    {
        "text": ">> Josh [inaudible]",
        "start": 376.49,
        "duration": 0.18
    },
    {
        "text": ">> Yeah, please, Sarah.",
        "start": 376.67,
        "duration": 1.22
    },
    {
        "text": ">> It seems like that gap can be huge.",
        "start": 377.89,
        "duration": 3.05
    },
    {
        "text": "When you talk to our science teams and",
        "start": 380.94,
        "duration": 2.18
    },
    {
        "text": "they're building a very general purpose model,",
        "start": 383.12,
        "duration": 2.13
    },
    {
        "text": "and some of the beauty is,",
        "start": 385.25,
        "duration": 1.2
    },
    {
        "text": "it can be used in so many different cases,",
        "start": 386.45,
        "duration": 3.03
    },
    {
        "text": "how does the team even start reasoning about the limitations or",
        "start": 389.48,
        "duration": 5.31
    },
    {
        "text": "the people that might use it if",
        "start": 394.79,
        "duration": 1.62
    },
    {
        "text": "the whole goal of AI is to",
        "start": 396.41,
        "duration": 1.8
    },
    {
        "text": "build this very general purpose technology?",
        "start": 398.21,
        "duration": 3.07
    },
    {
        "text": ">> It's a fantastic question.",
        "start": 401.54,
        "duration": 3.68
    },
    {
        "text": "I think I have two answers to help the top.",
        "start": 405.92,
        "duration": 5.02
    },
    {
        "text": "One, the generalizability of machine learning is,",
        "start": 410.94,
        "duration": 4.565
    },
    {
        "text": "I think one of the most overstated things",
        "start": 415.505,
        "duration": 2.985
    },
    {
        "text": "in like the history of technology.",
        "start": 418.49,
        "duration": 2.49
    },
    {
        "text": "It blows my mind when you think",
        "start": 420.98,
        "duration": 3.78
    },
    {
        "text": "about what these systems can do and yet how brittle",
        "start": 424.76,
        "duration": 4.245
    },
    {
        "text": "they end up being because of the nature of the weights and",
        "start": 429.005,
        "duration": 5.295
    },
    {
        "text": "biases that go into multi-dimensional hidden space.",
        "start": 434.3,
        "duration": 6.075
    },
    {
        "text": "There's hidden work and parameters that go into this,",
        "start": 440.375,
        "duration": 2.7
    },
    {
        "text": "and so back to the teaching metaphor,",
        "start": 443.075,
        "duration": 2.295
    },
    {
        "text": "I still go back and I'm like somebody could be an amazing teacher",
        "start": 445.37,
        "duration": 3.48
    },
    {
        "text": "reaching college students and",
        "start": 448.85,
        "duration": 2.28
    },
    {
        "text": "they might suck at reaching preschoolers.",
        "start": 451.13,
        "duration": 2.7
    },
    {
        "text": "The generalizability of machine learning is",
        "start": 453.83,
        "duration": 2.97
    },
    {
        "text": "similar to the generalizability of teaching,",
        "start": 456.8,
        "duration": 3.12
    },
    {
        "text": "which is just the similarities and I think we can get into those.",
        "start": 459.92,
        "duration": 2.925
    },
    {
        "text": "When you want to talk about accuracy and",
        "start": 462.845,
        "duration": 2.925
    },
    {
        "text": "reliability, the specifics matter.",
        "start": 465.77,
        "duration": 2.825
    },
    {
        "text": "I know what your experience has been like that,",
        "start": 468.595,
        "duration": 2.225
    },
    {
        "text": "the types of starting points that teams. Come to you Sarah.",
        "start": 470.82,
        "duration": 3.555
    },
    {
        "text": ">> Yeah, I think going back to Seth's point even about the data,",
        "start": 474.375,
        "duration": 3.815
    },
    {
        "text": "there is a finite set of training data that's in this model.",
        "start": 478.19,
        "duration": 3.81
    },
    {
        "text": "It probably works better for those use cases",
        "start": 482.0,
        "duration": 2.955
    },
    {
        "text": "and not the use cases that you have no data representing it.",
        "start": 484.955,
        "duration": 3.36
    },
    {
        "text": "So I think getting out of",
        "start": 488.315,
        "duration": 2.51
    },
    {
        "text": "this notion that is general is a huge starting point,",
        "start": 490.825,
        "duration": 3.245
    },
    {
        "text": "and I think going further and thinking about how",
        "start": 494.07,
        "duration": 2.93
    },
    {
        "text": "to build this technology for people.",
        "start": 497.0,
        "duration": 3.725
    },
    {
        "text": ">> Here's a question because I love hearing this discussion",
        "start": 500.725,
        "duration": 3.55
    },
    {
        "text": "between the ethical considerations and the product.",
        "start": 504.275,
        "duration": 4.29
    },
    {
        "text": "One of the questions I have is,",
        "start": 508.565,
        "duration": 2.63
    },
    {
        "text": "I'm having a hard time understanding the question,",
        "start": 511.195,
        "duration": 3.684
    },
    {
        "text": "and the question is I should be asking myself,",
        "start": 514.879,
        "duration": 1.861
    },
    {
        "text": "and I think the answer that I'm arriving at",
        "start": 516.74,
        "duration": 1.89
    },
    {
        "text": "from what you're telling us is that it",
        "start": 518.63,
        "duration": 2.1
    },
    {
        "text": "really depends on what you're",
        "start": 520.73,
        "duration": 1.86
    },
    {
        "text": "building and who you're building it for,",
        "start": 522.59,
        "duration": 2.28
    },
    {
        "text": "just like it does whenever you're building any software.",
        "start": 524.87,
        "duration": 3.51
    },
    {
        "text": "Is that an accurate statement?",
        "start": 528.38,
        "duration": 2.64
    },
    {
        "text": "Then you have a lot of other things",
        "start": 531.02,
        "duration": 2.25
    },
    {
        "text": "in there with people that I'd love to understand.",
        "start": 533.27,
        "duration": 2.85
    },
    {
        "text": "I want to get to the point where if I'm",
        "start": 536.12,
        "duration": 1.23
    },
    {
        "text": "building a machine learning system,",
        "start": 537.35,
        "duration": 1.425
    },
    {
        "text": "I'm asking myself the right questions to maybe even consider,",
        "start": 538.775,
        "duration": 4.144
    },
    {
        "text": "this probably isn't the right thing",
        "start": 542.919,
        "duration": 1.571
    },
    {
        "text": "to build using machine learning.",
        "start": 544.49,
        "duration": 2.44
    },
    {
        "text": ">> I think that's spot on. Yeah. The question,",
        "start": 547.17,
        "duration": 5.38
    },
    {
        "text": "you're trying to of build a bridge in a way,",
        "start": 552.55,
        "duration": 5.025
    },
    {
        "text": "between what's at the outermost ring, Seth,",
        "start": 557.575,
        "duration": 3.51
    },
    {
        "text": "that there are a set of capabilities",
        "start": 561.085,
        "duration": 2.4
    },
    {
        "text": "that potentially would feel magical.",
        "start": 563.485,
        "duration": 2.685
    },
    {
        "text": "The ability to actually reason about the visual space,",
        "start": 566.17,
        "duration": 4.53
    },
    {
        "text": "to turn vision into sound,",
        "start": 570.7,
        "duration": 1.77
    },
    {
        "text": "or vision into text,",
        "start": 572.47,
        "duration": 1.44
    },
    {
        "text": "or speech into text,",
        "start": 573.91,
        "duration": 1.62
    },
    {
        "text": "or speech into an understanding about physical space.",
        "start": 575.53,
        "duration": 2.955
    },
    {
        "text": "There's all this incredible stuff that we can do.",
        "start": 578.485,
        "duration": 3.045
    },
    {
        "text": "Then you need to bridge this gap,",
        "start": 581.53,
        "duration": 3.3
    },
    {
        "text": "because you meet people on the other side.",
        "start": 584.83,
        "duration": 3.12
    },
    {
        "text": "Ultimately the value, or the accuracy,",
        "start": 587.95,
        "duration": 2.31
    },
    {
        "text": "or the utility of your technology is",
        "start": 590.26,
        "duration": 2.01
    },
    {
        "text": "measured by whether people like it or not,",
        "start": 592.27,
        "duration": 2.445
    },
    {
        "text": "and whether they find value in it.",
        "start": 594.715,
        "duration": 2.16
    },
    {
        "text": "Starting with some of these first questions,",
        "start": 596.875,
        "duration": 3.375
    },
    {
        "text": "I think there's much more questions baked in,",
        "start": 600.25,
        "duration": 4.23
    },
    {
        "text": "and maybe we can riff through those really quickly.",
        "start": 604.48,
        "duration": 1.89
    },
    {
        "text": "But even this just up front,",
        "start": 606.37,
        "duration": 2.25
    },
    {
        "text": "there's a difference between, for example,",
        "start": 608.62,
        "duration": 2.73
    },
    {
        "text": "when you're building a speech recognition system,",
        "start": 611.35,
        "duration": 1.74
    },
    {
        "text": "to go to that example.",
        "start": 613.09,
        "duration": 1.5
    },
    {
        "text": "What am I trying to accomplish at that point? What are my needs?",
        "start": 614.59,
        "duration": 3.51
    },
    {
        "text": "Do I need to be better understood by a group,",
        "start": 618.1,
        "duration": 1.905
    },
    {
        "text": "or by an individual that I'm trying to relate with?",
        "start": 620.005,
        "duration": 3.24
    },
    {
        "text": "Are we speaking the same language?",
        "start": 623.245,
        "duration": 1.845
    },
    {
        "text": "Am I trying to make a good impression?",
        "start": 625.09,
        "duration": 3.3
    },
    {
        "text": "Are these people I know? Who am I?",
        "start": 628.39,
        "duration": 3.42
    },
    {
        "text": "Do I tend to speak loudly or quietly?",
        "start": 631.81,
        "duration": 3.21
    },
    {
        "text": "Do I perceive myself as having an accent? Not everyone does.",
        "start": 635.02,
        "duration": 3.435
    },
    {
        "text": "This is one of my favorite conversations with my kids,",
        "start": 638.455,
        "duration": 2.565
    },
    {
        "text": "and they're, \"Dad, we don't have",
        "start": 641.02,
        "duration": 1.29
    },
    {
        "text": "accents, do we?\" and I'm, \"No, no.",
        "start": 642.31,
        "duration": 1.545
    },
    {
        "text": "Check your entitlement for a second buddy,",
        "start": 643.855,
        "duration": 1.95
    },
    {
        "text": "you just don't realize it.\"",
        "start": 645.805,
        "duration": 2.575
    },
    {
        "text": "So these types of questions,",
        "start": 648.84,
        "duration": 2.26
    },
    {
        "text": "and then maybe one that might jump out to your viewers,",
        "start": 651.1,
        "duration": 2.55
    },
    {
        "text": "is this social identities question.",
        "start": 653.65,
        "duration": 2.37
    },
    {
        "text": "Stuff's going to fail, technologies failed for us for forever.",
        "start": 656.02,
        "duration": 2.91
    },
    {
        "text": "We have a long relationship with,",
        "start": 658.93,
        "duration": 1.89
    },
    {
        "text": "\"Have you tried turning it on off and on again?\"",
        "start": 660.82,
        "duration": 2.43
    },
    {
        "text": "But when something fails for you because of something",
        "start": 663.25,
        "duration": 2.91
    },
    {
        "text": "that you feel is very personal,",
        "start": 666.16,
        "duration": 2.79
    },
    {
        "text": "like maybe your gender identity,",
        "start": 668.95,
        "duration": 1.95
    },
    {
        "text": "or maybe your race or ethnicity,",
        "start": 670.9,
        "duration": 1.875
    },
    {
        "text": "or maybe a practice or a tradition that you have,",
        "start": 672.775,
        "duration": 2.49
    },
    {
        "text": "maybe a way you dress,",
        "start": 675.265,
        "duration": 2.07
    },
    {
        "text": "those are things where sometimes",
        "start": 677.335,
        "duration": 2.085
    },
    {
        "text": "they can start to cross this line,",
        "start": 679.42,
        "duration": 1.35
    },
    {
        "text": "and you're, \"Wait, is it me?",
        "start": 680.77,
        "duration": 2.04
    },
    {
        "text": "Do they build for me?",
        "start": 682.81,
        "duration": 2.115
    },
    {
        "text": "Do they not build for me?",
        "start": 684.925,
        "duration": 2.19
    },
    {
        "text": "Does the world not believe that this was meant for me?\"",
        "start": 687.115,
        "duration": 4.575
    },
    {
        "text": "Those are hard questions that I,",
        "start": 691.69,
        "duration": 2.385
    },
    {
        "text": "not for a second,",
        "start": 694.075,
        "duration": 1.47
    },
    {
        "text": "I don't believe that's the starting point for any engineer,",
        "start": 695.545,
        "duration": 3.375
    },
    {
        "text": "or data scientist, or research scientist,",
        "start": 698.92,
        "duration": 1.77
    },
    {
        "text": "I don't think anybody sets out to exclude.",
        "start": 700.69,
        "duration": 3.015
    },
    {
        "text": "There's just a lot of stuff that's buried,",
        "start": 703.705,
        "duration": 3.855
    },
    {
        "text": "and maybe we could take a pass",
        "start": 707.56,
        "duration": 1.86
    },
    {
        "text": "through some of these layers and look at some",
        "start": 709.42,
        "duration": 1.98
    },
    {
        "text": "of those issues that maybe end",
        "start": 711.4,
        "duration": 1.5
    },
    {
        "text": "up getting taken for granted too often.",
        "start": 712.9,
        "duration": 1.755
    },
    {
        "text": ">> Yeah, I would love to see the layers,",
        "start": 714.655,
        "duration": 2.145
    },
    {
        "text": "because I love the we're building stuff for people,",
        "start": 716.8,
        "duration": 5.07
    },
    {
        "text": "so maybe we should start with them.",
        "start": 721.87,
        "duration": 1.8
    },
    {
        "text": "But like for example, contexts, environments, apps,",
        "start": 723.67,
        "duration": 3.435
    },
    {
        "text": "I'm not understanding what is that mapping look like to people.",
        "start": 727.105,
        "duration": 4.59
    },
    {
        "text": "Maybe if we can go through some of the layers,",
        "start": 731.695,
        "duration": 1.44
    },
    {
        "text": "so that viewers can get a sense for what you're talking about.",
        "start": 733.135,
        "duration": 1.95
    },
    {
        "text": ">> Yeah. Absolutely. Maybe as we're going through, Sarah,",
        "start": 735.085,
        "duration": 4.395
    },
    {
        "text": "I think you bring so much direct practical experience",
        "start": 739.48,
        "duration": 3.24
    },
    {
        "text": "working with product teams.",
        "start": 742.72,
        "duration": 1.425
    },
    {
        "text": "Maybe adding on, I think some of that",
        "start": 744.145,
        "duration": 2.145
    },
    {
        "text": "specific what you're seeing in practice,",
        "start": 746.29,
        "duration": 3.37
    },
    {
        "text": "could be really great to hear.",
        "start": 749.76,
        "duration": 2.53
    },
    {
        "text": ">> Yeah, I'd be happy to.",
        "start": 752.29,
        "duration": 2.02
    },
    {
        "text": ">> The first thing about this system is each successive ring,",
        "start": 756.06,
        "duration": 7.015
    },
    {
        "text": "has a dependency on the prior ring.",
        "start": 763.075,
        "duration": 6.075
    },
    {
        "text": "Asking about the context,",
        "start": 769.15,
        "duration": 2.325
    },
    {
        "text": "we still carry with us those questions.",
        "start": 771.475,
        "duration": 2.385
    },
    {
        "text": "We haven't built our bridge yet to technology,",
        "start": 773.86,
        "duration": 2.64
    },
    {
        "text": "to know whether or not in the speech recognition case,",
        "start": 776.5,
        "duration": 2.58
    },
    {
        "text": "if we can effectively turn spoken words into typed text.",
        "start": 779.08,
        "duration": 4.5
    },
    {
        "text": "But we started to get a bit closer and we build.",
        "start": 783.58,
        "duration": 3.06
    },
    {
        "text": "If we know the people,",
        "start": 786.64,
        "duration": 1.38
    },
    {
        "text": "and we know some of their characteristics,",
        "start": 788.02,
        "duration": 2.265
    },
    {
        "text": "and we know some of their needs,",
        "start": 790.285,
        "duration": 1.755
    },
    {
        "text": "then it gets into this question of jobs to be done,",
        "start": 792.04,
        "duration": 2.37
    },
    {
        "text": "and things like dynamics.",
        "start": 794.41,
        "duration": 1.23
    },
    {
        "text": "So it's a very different scenario if",
        "start": 795.64,
        "duration": 2.97
    },
    {
        "text": "there's three of us jamming on some stuff going back and forth,",
        "start": 798.61,
        "duration": 3.42
    },
    {
        "text": "and maybe it's good if there's cross talk and some overlap.",
        "start": 802.03,
        "duration": 4.65
    },
    {
        "text": "Whereas if somebody is presenting into a more one-to-many,",
        "start": 806.68,
        "duration": 4.62
    },
    {
        "text": "I think you have some pretty different dynamics in terms of",
        "start": 811.3,
        "duration": 2.1
    },
    {
        "text": "expectations for what you should pick up on,",
        "start": 813.4,
        "duration": 4.35
    },
    {
        "text": "and the types of accuracy or measurement.",
        "start": 817.75,
        "duration": 2.83
    },
    {
        "text": "But also the specific types of transcription or whatever,",
        "start": 820.58,
        "duration": 5.365
    },
    {
        "text": "that'll change if you need to understand whether I'm say a doctor,",
        "start": 825.945,
        "duration": 5.49
    },
    {
        "text": "who is trying to prescribe something,",
        "start": 831.435,
        "duration": 1.995
    },
    {
        "text": "and so the vocabulary or the lexicon I'll",
        "start": 833.43,
        "duration": 1.83
    },
    {
        "text": "use is really different than if I'm a teacher,",
        "start": 835.26,
        "duration": 2.07
    },
    {
        "text": "or if I'm in an environment with",
        "start": 837.33,
        "duration": 2.25
    },
    {
        "text": "other people talking about a technical domain.",
        "start": 839.58,
        "duration": 2.42
    },
    {
        "text": "So much jargon and so many things that go into that,",
        "start": 842.0,
        "duration": 3.215
    },
    {
        "text": "that the jobs to be done has to really be brought to",
        "start": 845.215,
        "duration": 2.475
    },
    {
        "text": "bear to know how you would measure accuracy and usefulness.",
        "start": 847.69,
        "duration": 3.495
    },
    {
        "text": ">> I see and how do we take this into account",
        "start": 851.185,
        "duration": 2.31
    },
    {
        "text": "with our products, Sarah?",
        "start": 853.495,
        "duration": 2.37
    },
    {
        "text": ">> It's very interesting because in essence we",
        "start": 855.865,
        "duration": 3.165
    },
    {
        "text": "are starting a lot more of that technology layer.",
        "start": 859.03,
        "duration": 3.015
    },
    {
        "text": "We have something that",
        "start": 862.045,
        "duration": 1.89
    },
    {
        "text": "powers many different products inside of Microsoft,",
        "start": 863.935,
        "duration": 3.405
    },
    {
        "text": "as well as customer products,",
        "start": 867.34,
        "duration": 1.92
    },
    {
        "text": "and so being that lower outer layer,",
        "start": 869.26,
        "duration": 3.675
    },
    {
        "text": "what we really have to do is work closely",
        "start": 872.935,
        "duration": 2.865
    },
    {
        "text": "with the Microsoft products that are incorporating our models,",
        "start": 875.8,
        "duration": 3.675
    },
    {
        "text": "or with the customers to actually",
        "start": 879.475,
        "duration": 2.355
    },
    {
        "text": "understand some of the answers to these questions,",
        "start": 881.83,
        "duration": 2.37
    },
    {
        "text": "and really understand what are we trying to",
        "start": 884.2,
        "duration": 2.4
    },
    {
        "text": "achieve by deploying this technology.",
        "start": 886.6,
        "duration": 3.52
    },
    {
        "text": ">> Let's move to environments and let's",
        "start": 890.28,
        "duration": 2.41
    },
    {
        "text": "see if we can rifle through these,",
        "start": 892.69,
        "duration": 1.47
    },
    {
        "text": "because I want to get a really good picture",
        "start": 894.16,
        "duration": 1.89
    },
    {
        "text": "of what I should be thinking,",
        "start": 896.05,
        "duration": 1.62
    },
    {
        "text": "except when I see things like environments, apps, etc.",
        "start": 897.67,
        "duration": 2.985
    },
    {
        "text": "You're using these words in ways that I",
        "start": 900.655,
        "duration": 2.655
    },
    {
        "text": "had never considered and so this is super interesting.",
        "start": 903.31,
        "duration": 2.985
    },
    {
        "text": ">> Cool. Yeah, I think the closer to people we get at,",
        "start": 906.295,
        "duration": 4.425
    },
    {
        "text": "the closer it gets to more of social sciences,",
        "start": 910.72,
        "duration": 3.69
    },
    {
        "text": "or user experiencing type of thinking,",
        "start": 914.41,
        "duration": 1.56
    },
    {
        "text": "and you'll see we get a little bit",
        "start": 915.97,
        "duration": 1.35
    },
    {
        "text": "more towards the engineering stack.",
        "start": 917.32,
        "duration": 1.485
    },
    {
        "text": "I think that transition happens here",
        "start": 918.805,
        "duration": 2.445
    },
    {
        "text": "where we'd all agree at an environmental level,",
        "start": 921.25,
        "duration": 2.985
    },
    {
        "text": "if we're in a noisy room versus a quiet room,",
        "start": 924.235,
        "duration": 3.375
    },
    {
        "text": "those parameters should have",
        "start": 927.61,
        "duration": 1.14
    },
    {
        "text": "a fundamental difference on the quality of speech recognition.",
        "start": 928.75,
        "duration": 2.865
    },
    {
        "text": "They also need to be taken into account.",
        "start": 931.615,
        "duration": 1.695
    },
    {
        "text": "If you're building an app or a model that should work in say,",
        "start": 933.31,
        "duration": 4.95
    },
    {
        "text": "helping me get directions while I'm",
        "start": 938.26,
        "duration": 1.68
    },
    {
        "text": "driving and I can speak to my car,",
        "start": 939.94,
        "duration": 2.445
    },
    {
        "text": "you need to have really different ambient audio detection,",
        "start": 942.385,
        "duration": 4.98
    },
    {
        "text": "than in say a transcription note-to-self application environment.",
        "start": 947.365,
        "duration": 5.595
    },
    {
        "text": "So those environments play a significant role.",
        "start": 952.96,
        "duration": 3.12
    },
    {
        "text": "Jumping on a layer again to that app level,",
        "start": 956.08,
        "duration": 2.73
    },
    {
        "text": "and this is where I think the biggest transitions start to happen,",
        "start": 958.81,
        "duration": 3.39
    },
    {
        "text": "because you get stuff like, \"Well,",
        "start": 962.2,
        "duration": 1.62
    },
    {
        "text": "how will people interact with these?",
        "start": 963.82,
        "duration": 1.365
    },
    {
        "text": "Are these swipes?",
        "start": 965.185,
        "duration": 1.725
    },
    {
        "text": "Is it a touch target?",
        "start": 966.91,
        "duration": 1.17
    },
    {
        "text": "Are there are devices that are actually",
        "start": 968.08,
        "duration": 2.07
    },
    {
        "text": "just attached to parts of the room?",
        "start": 970.15,
        "duration": 4.755
    },
    {
        "text": "Is there something that's always listening?",
        "start": 974.905,
        "duration": 3.585
    },
    {
        "text": "Do I have a feedback mechanism that I actually am playing with?",
        "start": 978.49,
        "duration": 4.38
    },
    {
        "text": "Can this thing learn from me?",
        "start": 982.87,
        "duration": 1.905
    },
    {
        "text": "How do I know what it does?\"",
        "start": 984.775,
        "duration": 2.04
    },
    {
        "text": "This is the most central layer in",
        "start": 986.815,
        "duration": 3.27
    },
    {
        "text": "the human and computer interaction part of",
        "start": 990.085,
        "duration": 2.055
    },
    {
        "text": "the system where you'll get these big mismatches,",
        "start": 992.14,
        "duration": 2.7
    },
    {
        "text": "and we know what we think of as the conceptual model of a user,",
        "start": 994.84,
        "duration": 3.42
    },
    {
        "text": "and the conceptual model of the engineer who built it.",
        "start": 998.26,
        "duration": 4.08
    },
    {
        "text": "Think about thermostats. For forever,",
        "start": 1002.34,
        "duration": 3.18
    },
    {
        "text": "you just dialed this thing",
        "start": 1005.52,
        "duration": 1.515
    },
    {
        "text": "further to the right when you wanted it hotter,",
        "start": 1007.035,
        "duration": 2.025
    },
    {
        "text": "and further to the left when you wanted to colder,",
        "start": 1009.06,
        "duration": 1.77
    },
    {
        "text": "but you had no notion of how",
        "start": 1010.83,
        "duration": 1.65
    },
    {
        "text": "quickly it would get hotter or how quickly it would get colder,",
        "start": 1012.48,
        "duration": 2.61
    },
    {
        "text": "and people will do these crazy things",
        "start": 1015.09,
        "duration": 1.41
    },
    {
        "text": "with dragging it to the slider all the way to the side,",
        "start": 1016.5,
        "duration": 2.25
    },
    {
        "text": "because they thought it might",
        "start": 1018.75,
        "duration": 1.845
    },
    {
        "text": "speed up the rate that the room would get hotter or colder,",
        "start": 1020.595,
        "duration": 2.565
    },
    {
        "text": "which is not true.",
        "start": 1023.16,
        "duration": 1.44
    },
    {
        "text": "But again, there's no feedback to the user in that process.",
        "start": 1024.6,
        "duration": 3.78
    },
    {
        "text": "We have to think about that stuff when",
        "start": 1028.38,
        "duration": 1.98
    },
    {
        "text": "we're thinking about optimizing these models.",
        "start": 1030.36,
        "duration": 2.355
    },
    {
        "text": "Because again, if nobody can improve",
        "start": 1032.715,
        "duration": 2.145
    },
    {
        "text": "the quality of the speech recognition over time,",
        "start": 1034.86,
        "duration": 2.505
    },
    {
        "text": "and they just have to receive the results of it,",
        "start": 1037.365,
        "duration": 3.81
    },
    {
        "text": "then how are they going to build confidence?",
        "start": 1041.175,
        "duration": 2.4
    },
    {
        "text": "If they can't build confidence,",
        "start": 1043.575,
        "duration": 1.185
    },
    {
        "text": "are they actually want to keep using it?",
        "start": 1044.76,
        "duration": 1.425
    },
    {
        "text": ">> Yeah, and I think that this layer is probably one of",
        "start": 1046.185,
        "duration": 2.385
    },
    {
        "text": "our biggest opportunities in practice,",
        "start": 1048.57,
        "duration": 3.0
    },
    {
        "text": "where we both can really use it as a way",
        "start": 1051.57,
        "duration": 3.3
    },
    {
        "text": "to better contextualize potential errors that the model is making,",
        "start": 1054.87,
        "duration": 3.525
    },
    {
        "text": "as well as allow us to",
        "start": 1058.395,
        "duration": 1.98
    },
    {
        "text": "learn and understand where the model is failing.",
        "start": 1060.375,
        "duration": 2.235
    },
    {
        "text": "So we really lean into",
        "start": 1062.61,
        "duration": 1.32
    },
    {
        "text": "that app layer and co-design with the model and the technology.",
        "start": 1063.93,
        "duration": 3.9
    },
    {
        "text": "I think there's a lot more we really",
        "start": 1067.83,
        "duration": 2.82
    },
    {
        "text": "could do and in some cases it's been a bit neglected.",
        "start": 1070.65,
        "duration": 4.12
    },
    {
        "text": ">> As a designer,",
        "start": 1075.44,
        "duration": 2.02
    },
    {
        "text": "that's definitely been my experience.",
        "start": 1077.46,
        "duration": 2.01
    },
    {
        "text": "There's a little like, let's assume that everything in the model",
        "start": 1079.47,
        "duration": 3.33
    },
    {
        "text": "is right and then people will just get the results of it.",
        "start": 1082.8,
        "duration": 2.91
    },
    {
        "text": "It's like, what happens when it's",
        "start": 1085.71,
        "duration": 1.38
    },
    {
        "text": "wrong because that's going to happen.",
        "start": 1087.09,
        "duration": 2.595
    },
    {
        "text": "It's guaranteed to happen with machine learning.",
        "start": 1089.685,
        "duration": 2.965
    },
    {
        "text": "Moving to the considerations",
        "start": 1093.77,
        "duration": 3.19
    },
    {
        "text": "for the actual device that you're going to use,",
        "start": 1096.96,
        "duration": 2.865
    },
    {
        "text": "again, brings up more of these questions.",
        "start": 1099.825,
        "duration": 2.355
    },
    {
        "text": "Speech recognition is another one just to continue riffing on,",
        "start": 1102.18,
        "duration": 4.005
    },
    {
        "text": "the difference between an omnidirectional mic",
        "start": 1106.185,
        "duration": 2.235
    },
    {
        "text": "versus a cardioid mic",
        "start": 1108.42,
        "duration": 1.98
    },
    {
        "text": "versus a shotgun mic or whatever types of of",
        "start": 1110.4,
        "duration": 4.26
    },
    {
        "text": "detection range and proximity that you would need in order to",
        "start": 1114.66,
        "duration": 3.06
    },
    {
        "text": "sustain a consistent volume for good transcription recording.",
        "start": 1117.72,
        "duration": 8.17
    },
    {
        "text": "The types of processing capabilities that you might have.",
        "start": 1127.37,
        "duration": 4.27
    },
    {
        "text": "Do you have on-device models that",
        "start": 1131.64,
        "duration": 1.545
    },
    {
        "text": "you're able to run to do things like",
        "start": 1133.185,
        "duration": 1.965
    },
    {
        "text": "attenuation of background noise or",
        "start": 1135.15,
        "duration": 2.07
    },
    {
        "text": "preprocessing to determine, say,",
        "start": 1137.22,
        "duration": 2.28
    },
    {
        "text": "if there's certain weak words",
        "start": 1139.5,
        "duration": 2.49
    },
    {
        "text": "or something that you want to bake into it that actually",
        "start": 1141.99,
        "duration": 2.22
    },
    {
        "text": "should have downstream effects",
        "start": 1144.21,
        "duration": 2.18
    },
    {
        "text": "for which model might get kicked off into.",
        "start": 1146.39,
        "duration": 4.66
    },
    {
        "text": "Lots of stuff to ask at",
        "start": 1151.05,
        "duration": 2.07
    },
    {
        "text": "this layer against getting closer and closer",
        "start": 1153.12,
        "duration": 3.69
    },
    {
        "text": "into things that typically we talk",
        "start": 1156.81,
        "duration": 3.18
    },
    {
        "text": "about through the lens of engineering reliability.",
        "start": 1159.99,
        "duration": 3.345
    },
    {
        "text": "But they're still critical when you're trying to say,",
        "start": 1163.335,
        "duration": 3.0
    },
    {
        "text": "how do I train my model?",
        "start": 1166.335,
        "duration": 1.785
    },
    {
        "text": "What is acceptable loss given, all these considerations.",
        "start": 1168.12,
        "duration": 5.97
    },
    {
        "text": ">> As you go to stack, I'm sorry,",
        "start": 1174.09,
        "duration": 3.42
    },
    {
        "text": "one of the orthogonal concerns that I'm not seeing is for example,",
        "start": 1177.51,
        "duration": 5.88
    },
    {
        "text": "if I'm thinking about jobs to be done,",
        "start": 1183.39,
        "duration": 2.67
    },
    {
        "text": "I might be building an application for adults",
        "start": 1186.06,
        "duration": 3.045
    },
    {
        "text": "versus children versus people that are older.",
        "start": 1189.105,
        "duration": 3.225
    },
    {
        "text": "I don't see anything about",
        "start": 1192.33,
        "duration": 1.92
    },
    {
        "text": "the questions of because this is going to sound wrong,",
        "start": 1194.25,
        "duration": 2.73
    },
    {
        "text": "and I don't want it to sound because I",
        "start": 1196.98,
        "duration": 1.05
    },
    {
        "text": "want to get the right ethics.",
        "start": 1198.03,
        "duration": 1.77
    },
    {
        "text": "Basically once you get to the jobs to be done,",
        "start": 1199.8,
        "duration": 2.64
    },
    {
        "text": "are you not starting to segment",
        "start": 1202.44,
        "duration": 1.47
    },
    {
        "text": "people into who your target audiences,",
        "start": 1203.91,
        "duration": 2.85
    },
    {
        "text": "how do you do that in an ethical way?",
        "start": 1206.76,
        "duration": 3.25
    },
    {
        "text": ">> That's a really,",
        "start": 1210.47,
        "duration": 2.5
    },
    {
        "text": "really, really important question.",
        "start": 1212.97,
        "duration": 1.26
    },
    {
        "text": "I'm glad you asked that. I'm hesitant",
        "start": 1214.23,
        "duration": 3.15
    },
    {
        "text": "to dig in with all these different ideas in this screen.",
        "start": 1217.38,
        "duration": 2.46
    },
    {
        "text": "So audience bear with us.",
        "start": 1219.84,
        "duration": 2.29
    },
    {
        "text": "There's a mistake that we're working our way backwards from,",
        "start": 1223.1,
        "duration": 8.51
    },
    {
        "text": "and some of it has to do",
        "start": 1231.68,
        "duration": 3.31
    },
    {
        "text": "with an over indexing on political correctness.",
        "start": 1234.99,
        "duration": 5.07
    },
    {
        "text": "There was this belief for quite some time and I think we probably,",
        "start": 1240.06,
        "duration": 3.66
    },
    {
        "text": "all three of us might have grown up in this era",
        "start": 1243.72,
        "duration": 1.875
    },
    {
        "text": "where it was easier to just say,",
        "start": 1245.595,
        "duration": 3.6
    },
    {
        "text": "I don't see color or I don't see gender",
        "start": 1249.195,
        "duration": 3.405
    },
    {
        "text": "or world just the same and everyone should be treated equal.",
        "start": 1252.6,
        "duration": 4.9
    },
    {
        "text": "To want to believe in that type of a dynamic is quite alluring.",
        "start": 1258.8,
        "duration": 9.985
    },
    {
        "text": "That's a really nice set",
        "start": 1268.785,
        "duration": 3.195
    },
    {
        "text": "of attributes and ideals to try to encode,",
        "start": 1271.98,
        "duration": 2.265
    },
    {
        "text": "in a generation of kids.",
        "start": 1274.245,
        "duration": 2.365
    },
    {
        "text": "Our parents, they did that",
        "start": 1277.1,
        "duration": 3.19
    },
    {
        "text": "because they wanted to believe that their efforts",
        "start": 1280.29,
        "duration": 2.58
    },
    {
        "text": "and progressive movements and what not",
        "start": 1282.87,
        "duration": 2.67
    },
    {
        "text": "have manifested in a healthier, more equitable future.",
        "start": 1285.54,
        "duration": 3.375
    },
    {
        "text": "It's not wrong, those aren't bad ideals,",
        "start": 1288.915,
        "duration": 2.46
    },
    {
        "text": "but it's not realistic.",
        "start": 1291.375,
        "duration": 1.725
    },
    {
        "text": "Human beings do have differences.",
        "start": 1293.1,
        "duration": 3.06
    },
    {
        "text": "We do judge each other,",
        "start": 1296.16,
        "duration": 1.89
    },
    {
        "text": "we do take into considerations our past experiences and",
        "start": 1298.05,
        "duration": 3.42
    },
    {
        "text": "build stereotypes about the future for right or for wrong.",
        "start": 1301.47,
        "duration": 3.615
    },
    {
        "text": "But those are attributes that it would be wrong for us to",
        "start": 1305.085,
        "duration": 3.645
    },
    {
        "text": "say everyone is exactly",
        "start": 1308.73,
        "duration": 2.37
    },
    {
        "text": "the same and we all have the same background.",
        "start": 1311.1,
        "duration": 1.635
    },
    {
        "text": "Because that itself actually marginalizes",
        "start": 1312.735,
        "duration": 2.595
    },
    {
        "text": "the experiences of people who have been historically forgotten.",
        "start": 1315.33,
        "duration": 4.44
    },
    {
        "text": "There's just too many people who have been historically forgotten",
        "start": 1319.77,
        "duration": 4.38
    },
    {
        "text": "because it's too uncomfortable to start calling out by name,",
        "start": 1324.15,
        "duration": 5.775
    },
    {
        "text": "the fact that we've forgotten them for so long.",
        "start": 1329.925,
        "duration": 3.435
    },
    {
        "text": "So to your points as when you start to build these groups,",
        "start": 1333.36,
        "duration": 4.005
    },
    {
        "text": "it is not a comfortable activity.",
        "start": 1337.365,
        "duration": 2.55
    },
    {
        "text": "I'm sure Sarah can speak to this on the Product Team side.",
        "start": 1339.915,
        "duration": 3.75
    },
    {
        "text": "I'm curious the way that you're helping",
        "start": 1343.665,
        "duration": 4.575
    },
    {
        "text": "groups start to wake up to that need to actually segment,",
        "start": 1348.24,
        "duration": 3.9
    },
    {
        "text": "not for the purposes of judging people,",
        "start": 1352.14,
        "duration": 2.325
    },
    {
        "text": "but from making thing more equitable.",
        "start": 1354.465,
        "duration": 1.695
    },
    {
        "text": ">> Yeah, I think it's as you said,",
        "start": 1356.16,
        "duration": 3.6
    },
    {
        "text": "in an uncomfortable exercise and that",
        "start": 1359.76,
        "duration": 2.58
    },
    {
        "text": "it's very different than what we're used to doing.",
        "start": 1362.34,
        "duration": 3.225
    },
    {
        "text": "A lot of the work is just how do we",
        "start": 1365.565,
        "duration": 2.775
    },
    {
        "text": "even start thinking about this?",
        "start": 1368.34,
        "duration": 2.82
    },
    {
        "text": "How do we think about",
        "start": 1371.16,
        "duration": 2.205
    },
    {
        "text": "the groups of people the technology is designed to work for?",
        "start": 1373.365,
        "duration": 4.185
    },
    {
        "text": "How do we add that next group in that next consideration?",
        "start": 1377.55,
        "duration": 4.65
    },
    {
        "text": "It's definitely a very different way",
        "start": 1382.2,
        "duration": 3.09
    },
    {
        "text": "of thinking than when we're trying",
        "start": 1385.29,
        "duration": 2.34
    },
    {
        "text": "to think about the technology agnostic to the users.",
        "start": 1387.63,
        "duration": 5.4
    },
    {
        "text": ">> One thing just to build on what Sarah's saying.",
        "start": 1393.03,
        "duration": 3.135
    },
    {
        "text": "One of the things that we require as it's like",
        "start": 1396.165,
        "duration": 1.875
    },
    {
        "text": "for our example for ethics and societies,",
        "start": 1398.04,
        "duration": 1.53
    },
    {
        "text": "whenever we partner with the Product Team,",
        "start": 1399.57,
        "duration": 1.68
    },
    {
        "text": "we go through this exercise of trying",
        "start": 1401.25,
        "duration": 2.4
    },
    {
        "text": "to imagine potential futures,",
        "start": 1403.65,
        "duration": 2.25
    },
    {
        "text": "both positive and not so positive",
        "start": 1405.9,
        "duration": 2.19
    },
    {
        "text": "futures and asked ourselves, who's not involved?",
        "start": 1408.09,
        "duration": 5.205
    },
    {
        "text": "Who is not part of the conversation?",
        "start": 1413.295,
        "duration": 3.025
    },
    {
        "text": "Then when we do our research,",
        "start": 1416.66,
        "duration": 3.325
    },
    {
        "text": "we intentionally overrepresent those groups,",
        "start": 1419.985,
        "duration": 3.315
    },
    {
        "text": "and we've been building out this wacky.",
        "start": 1423.3,
        "duration": 3.69
    },
    {
        "text": "It's like of giant spreadsheet of",
        "start": 1426.99,
        "duration": 1.59
    },
    {
        "text": "different ways that people identify.",
        "start": 1428.58,
        "duration": 2.43
    },
    {
        "text": "Because depending on the jobs to be done,",
        "start": 1431.01,
        "duration": 2.94
    },
    {
        "text": "depending on the context,",
        "start": 1433.95,
        "duration": 1.215
    },
    {
        "text": "depending on even the devices,",
        "start": 1435.165,
        "duration": 2.175
    },
    {
        "text": "how expensive the device might be and",
        "start": 1437.34,
        "duration": 2.37
    },
    {
        "text": "who might have access to reliable service,",
        "start": 1439.71,
        "duration": 2.49
    },
    {
        "text": "and where you're going to get the data from.",
        "start": 1442.2,
        "duration": 2.115
    },
    {
        "text": "All these different layers,",
        "start": 1444.315,
        "duration": 1.155
    },
    {
        "text": "they call into question who",
        "start": 1445.47,
        "duration": 2.46
    },
    {
        "text": "was involved and how early were they involved?",
        "start": 1447.93,
        "duration": 4.15
    },
    {
        "text": "We do have to expand",
        "start": 1453.71,
        "duration": 2.59
    },
    {
        "text": "our default settings of being able to categorize people.",
        "start": 1456.3,
        "duration": 6.94
    },
    {
        "text": "Because counter-intuitively, we want to develop for",
        "start": 1465.02,
        "duration": 4.57
    },
    {
        "text": "a future where nobody should have to",
        "start": 1469.59,
        "duration": 2.13
    },
    {
        "text": "change something about themselves",
        "start": 1471.72,
        "duration": 1.56
    },
    {
        "text": "to make a technology work for them.",
        "start": 1473.28,
        "duration": 1.905
    },
    {
        "text": ">> There's this notion that I've been developing in",
        "start": 1475.185,
        "duration": 2.355
    },
    {
        "text": "my head because I grew up the same way, everyone's the same.",
        "start": 1477.54,
        "duration": 3.33
    },
    {
        "text": "But there's a huge difference between",
        "start": 1480.87,
        "duration": 2.415
    },
    {
        "text": "equality and equity, they're different things.",
        "start": 1483.285,
        "duration": 4.08
    },
    {
        "text": "One has to do with outcomes and",
        "start": 1487.365,
        "duration": 2.175
    },
    {
        "text": "one who has to do with opportunities,",
        "start": 1489.54,
        "duration": 1.619
    },
    {
        "text": "and I understand that.",
        "start": 1491.159,
        "duration": 3.341
    },
    {
        "text": "For example, when I'm thinking about",
        "start": 1495.05,
        "duration": 3.01
    },
    {
        "text": "the technology that I want to build and the jobs to be done,",
        "start": 1498.06,
        "duration": 3.27
    },
    {
        "text": "how do I ethically look at both of",
        "start": 1501.33,
        "duration": 2.46
    },
    {
        "text": "those orthogonal concerns that",
        "start": 1503.79,
        "duration": 2.85
    },
    {
        "text": "I think are with the people and the jobs to be done?",
        "start": 1506.64,
        "duration": 2.925
    },
    {
        "text": "How do I do that?",
        "start": 1509.565,
        "duration": 1.29
    },
    {
        "text": "Because obviously, for example,",
        "start": 1510.855,
        "duration": 1.74
    },
    {
        "text": "if you want to cater",
        "start": 1512.595,
        "duration": 2.175
    },
    {
        "text": "to a segment of the population, programmers that are blind,",
        "start": 1514.77,
        "duration": 2.79
    },
    {
        "text": "I've met them, they're amazing,",
        "start": 1517.56,
        "duration": 1.365
    },
    {
        "text": "but you have to consciously think about",
        "start": 1518.925,
        "duration": 2.4
    },
    {
        "text": "that segment of the population when you're building a product.",
        "start": 1521.325,
        "duration": 3.285
    },
    {
        "text": "How are you inclusive when you build these things,",
        "start": 1524.61,
        "duration": 4.995
    },
    {
        "text": "and at the same time,",
        "start": 1529.605,
        "duration": 1.845
    },
    {
        "text": "have the time to build something.",
        "start": 1531.45,
        "duration": 2.01
    },
    {
        "text": "Because you can't build something for everybody,",
        "start": 1533.46,
        "duration": 1.77
    },
    {
        "text": "there's just not enough time.",
        "start": 1535.23,
        "duration": 1.38
    },
    {
        "text": "How do you ethically build for what you",
        "start": 1536.61,
        "duration": 1.98
    },
    {
        "text": "can to start? Do you see what I'm getting at?",
        "start": 1538.59,
        "duration": 3.21
    },
    {
        "text": ">> Absolutely.",
        "start": 1541.8,
        "duration": 2.01
    },
    {
        "text": "Absolutely. I think the different ways that you're",
        "start": 1543.81,
        "duration": 3.39
    },
    {
        "text": "trying to maneuver through even those words,",
        "start": 1547.2,
        "duration": 3.225
    },
    {
        "text": "highlights just how weird this conversation ends up being,",
        "start": 1550.425,
        "duration": 3.195
    },
    {
        "text": "so the first thing is",
        "start": 1553.62,
        "duration": 1.71
    },
    {
        "text": "to not judge each other for the stuff we don't know.",
        "start": 1555.33,
        "duration": 5.085
    },
    {
        "text": "The reality is if you want to paint",
        "start": 1560.415,
        "duration": 3.405
    },
    {
        "text": "this full image of all these different rings of considerations,",
        "start": 1563.82,
        "duration": 4.695
    },
    {
        "text": "building a product is messy.",
        "start": 1568.515,
        "duration": 2.7
    },
    {
        "text": "There's never enough time.",
        "start": 1571.215,
        "duration": 1.92
    },
    {
        "text": "You're always bound by certain constraints.",
        "start": 1573.135,
        "duration": 4.405
    },
    {
        "text": "The dilemma you should be faced with",
        "start": 1578.27,
        "duration": 3.895
    },
    {
        "text": "isn't let's get this",
        "start": 1582.165,
        "duration": 3.795
    },
    {
        "text": "perfect so that it's going to work for all people,",
        "start": 1585.96,
        "duration": 2.715
    },
    {
        "text": "it's let's go through the activity of",
        "start": 1588.675,
        "duration": 5.594
    },
    {
        "text": "describing the things that should",
        "start": 1594.269,
        "duration": 2.821
    },
    {
        "text": "contribute to accuracy and shouldn't contribute to inaccuracy,",
        "start": 1597.09,
        "duration": 3.735
    },
    {
        "text": "so we at least can say,",
        "start": 1600.825,
        "duration": 2.16
    },
    {
        "text": "here are the limitations of the technology we're building.",
        "start": 1602.985,
        "duration": 3.975
    },
    {
        "text": "There's no perfect and this is part of the issue going back to AI",
        "start": 1606.96,
        "duration": 3.33
    },
    {
        "text": "being a little over-hyped in terms of its generalizability.",
        "start": 1610.29,
        "duration": 4.005
    },
    {
        "text": "We might get to a point where metal learning and transfer learning",
        "start": 1614.295,
        "duration": 3.975
    },
    {
        "text": "and we'll have like multi-headed adversarial networks",
        "start": 1618.27,
        "duration": 4.59
    },
    {
        "text": "that will somehow build in resource management systems",
        "start": 1622.86,
        "duration": 2.97
    },
    {
        "text": "and reinforcement learning can somehow take us to a spot where",
        "start": 1625.83,
        "duration": 3.135
    },
    {
        "text": "you've got entropy built into",
        "start": 1628.965,
        "duration": 1.425
    },
    {
        "text": "systems such that they need resources",
        "start": 1630.39,
        "duration": 1.74
    },
    {
        "text": "to keep on going and robots essentially mimic humans.",
        "start": 1632.13,
        "duration": 3.585
    },
    {
        "text": "We could get there. Also, we have what we have right now,",
        "start": 1635.715,
        "duration": 3.765
    },
    {
        "text": "which are systems that are often quite brittle,",
        "start": 1639.48,
        "duration": 3.255
    },
    {
        "text": "and a series of layers and questions that we just",
        "start": 1642.735,
        "duration": 5.055
    },
    {
        "text": "haven't built up a good set of",
        "start": 1647.79,
        "duration": 1.5
    },
    {
        "text": "musculature as teams for how to talk about.",
        "start": 1649.29,
        "duration": 3.72
    },
    {
        "text": "What I'm saying to your question, Seth,",
        "start": 1653.01,
        "duration": 1.545
    },
    {
        "text": "and it's a fantastic question isn't actually that we can solve it.",
        "start": 1654.555,
        "duration": 5.775
    },
    {
        "text": "It's we have to embrace how imperfect what we're going to make is,",
        "start": 1660.33,
        "duration": 4.02
    },
    {
        "text": "and then be specific",
        "start": 1664.35,
        "duration": 3.075
    },
    {
        "text": "about who we're going to make sure it does work well for.",
        "start": 1667.425,
        "duration": 2.85
    },
    {
        "text": ">> I see. Sarah, a quick question.",
        "start": 1670.275,
        "duration": 2.22
    },
    {
        "text": "Well, not a quick question because it's",
        "start": 1672.495,
        "duration": 1.125
    },
    {
        "text": "going to be a long answer I think.",
        "start": 1673.62,
        "duration": 1.5
    },
    {
        "text": "How does the product group engage with these kinds of questions?",
        "start": 1675.12,
        "duration": 5.79
    },
    {
        "text": "How do you work together with our ethics division?",
        "start": 1680.91,
        "duration": 4.62
    },
    {
        "text": "I'm saying ethics division like it's a baseball team or something.",
        "start": 1685.53,
        "duration": 4.32
    },
    {
        "text": ">> [inaudible].",
        "start": 1689.85,
        "duration": 0.24
    },
    {
        "text": ">> What does that look like?",
        "start": 1690.09,
        "duration": 2.19
    },
    {
        "text": ">> Yeah.",
        "start": 1692.28,
        "duration": 3.39
    },
    {
        "text": "Thinking about what Josh was just saying as well,",
        "start": 1695.67,
        "duration": 2.7
    },
    {
        "text": "probably the most significant thing that we've been doing is",
        "start": 1698.37,
        "duration": 4.62
    },
    {
        "text": "just changing how we think about",
        "start": 1702.99,
        "duration": 2.55
    },
    {
        "text": "the products we're building and how we think about the technology.",
        "start": 1705.54,
        "duration": 3.6
    },
    {
        "text": "For something that's a more general platform technology",
        "start": 1709.14,
        "duration": 3.42
    },
    {
        "text": "like the Cognitive Services,",
        "start": 1712.56,
        "duration": 1.994
    },
    {
        "text": "the most freeing thing has been really leaning into,",
        "start": 1714.554,
        "duration": 4.126
    },
    {
        "text": "why are we building this tech?",
        "start": 1718.68,
        "duration": 1.83
    },
    {
        "text": "What is it for?",
        "start": 1720.51,
        "duration": 1.5
    },
    {
        "text": "What do we want to achieve?",
        "start": 1722.01,
        "duration": 1.47
    },
    {
        "text": "Then being much more",
        "start": 1723.48,
        "duration": 2.55
    },
    {
        "text": "comfortable and open about the limitations of it.",
        "start": 1726.03,
        "duration": 2.52
    },
    {
        "text": "I think in fact, that in a lot of cases led to",
        "start": 1728.55,
        "duration": 4.83
    },
    {
        "text": "actually just a lot of clarity in the product groups",
        "start": 1733.38,
        "duration": 2.37
    },
    {
        "text": "thinking and led to more innovation because they're like,",
        "start": 1735.75,
        "duration": 3.36
    },
    {
        "text": "''Oh, that's what we're doing?",
        "start": 1739.11,
        "duration": 1.605
    },
    {
        "text": "If we're doing that, we could add this feature and",
        "start": 1740.715,
        "duration": 1.845
    },
    {
        "text": "that feature and that would be really",
        "start": 1742.56,
        "duration": 1.2
    },
    {
        "text": "cool and customers would love it.''",
        "start": 1743.76,
        "duration": 1.77
    },
    {
        "text": "A lot of it has just really been focusing on,",
        "start": 1745.53,
        "duration": 3.315
    },
    {
        "text": "why does this product exist?",
        "start": 1748.845,
        "duration": 1.635
    },
    {
        "text": "Why did we build it? Who's it for?",
        "start": 1750.48,
        "duration": 2.07
    },
    {
        "text": "Then after that, there certainly",
        "start": 1752.55,
        "duration": 2.01
    },
    {
        "text": "are uncomfortable conversations around, well, who is it for?",
        "start": 1754.56,
        "duration": 2.685
    },
    {
        "text": "Who is it not working well but it should be for?",
        "start": 1757.245,
        "duration": 2.52
    },
    {
        "text": "With that kind of framing at",
        "start": 1759.765,
        "duration": 1.935
    },
    {
        "text": "least we're on a path to starting to say,",
        "start": 1761.7,
        "duration": 1.995
    },
    {
        "text": "''It should work well for this group of people.'",
        "start": 1763.695,
        "duration": 2.445
    },
    {
        "text": "We haven't checked that,",
        "start": 1766.14,
        "duration": 1.71
    },
    {
        "text": "so we might have some very unexpected errors.\"",
        "start": 1767.85,
        "duration": 2.82
    },
    {
        "text": "Which is, I think basically the norm is what we found is.",
        "start": 1770.67,
        "duration": 3.165
    },
    {
        "text": "The errors manifest in very surprising ways.",
        "start": 1773.835,
        "duration": 3.75
    },
    {
        "text": "First with that more principled thinking,",
        "start": 1777.585,
        "duration": 3.285
    },
    {
        "text": "then we can start going and doing our normal products work,",
        "start": 1780.87,
        "duration": 3.03
    },
    {
        "text": "designing tests to line up with that goal,",
        "start": 1783.9,
        "duration": 2.7
    },
    {
        "text": "designing release criteria that",
        "start": 1786.6,
        "duration": 3.3
    },
    {
        "text": "lines up with the reason the technology exists and who is it for.",
        "start": 1789.9,
        "duration": 4.3
    },
    {
        "text": "It's been a great journey together with the ethics team,",
        "start": 1795.02,
        "duration": 4.66
    },
    {
        "text": "think more about how we build that thinking around impact",
        "start": 1799.68,
        "duration": 3.314
    },
    {
        "text": "and the purpose of",
        "start": 1802.994,
        "duration": 1.186
    },
    {
        "text": "a technology into our thinking from the beginning.",
        "start": 1804.18,
        "duration": 2.895
    },
    {
        "text": "There's lots of open questions,",
        "start": 1807.075,
        "duration": 1.875
    },
    {
        "text": "but it's definitely helped create",
        "start": 1808.95,
        "duration": 1.68
    },
    {
        "text": "clarity for us and how we move forward?",
        "start": 1810.63,
        "duration": 2.82
    },
    {
        "text": ">> I've never had an ethics person on the show.",
        "start": 1813.45,
        "duration": 4.665
    },
    {
        "text": "Here's a pet peeve that I've always had with machinery and I",
        "start": 1818.115,
        "duration": 4.035
    },
    {
        "text": "want to run it by you-all and tell me if my thinking is right.",
        "start": 1822.15,
        "duration": 3.915
    },
    {
        "text": "I personally can't stand when people",
        "start": 1826.065,
        "duration": 3.345
    },
    {
        "text": "anthropomorphize machine-learning models because",
        "start": 1829.41,
        "duration": 3.78
    },
    {
        "text": "my inner self is telling",
        "start": 1833.19,
        "duration": 1.5
    },
    {
        "text": "me that it allows people to disclaim liability or to",
        "start": 1834.69,
        "duration": 5.115
    },
    {
        "text": "move the ethics needle over to the machine when",
        "start": 1839.805,
        "duration": 2.805
    },
    {
        "text": "it really is a human who controls those things.",
        "start": 1842.61,
        "duration": 3.54
    },
    {
        "text": "Am I wrong here? Should I not be upset about that?",
        "start": 1846.15,
        "duration": 3.48
    },
    {
        "text": ">> It's actually one of the biggest complaints about the term",
        "start": 1849.63,
        "duration": 2.97
    },
    {
        "text": "responsible AI because it's not that",
        "start": 1852.6,
        "duration": 1.77
    },
    {
        "text": "we want the AI to be responsible,",
        "start": 1854.37,
        "duration": 1.905
    },
    {
        "text": "we want to build it responsibly and",
        "start": 1856.275,
        "duration": 1.665
    },
    {
        "text": "we want to use it responsively.",
        "start": 1857.94,
        "duration": 2.37
    },
    {
        "text": "It's really developing AI responsibly not responsible AI.",
        "start": 1860.31,
        "duration": 6.42
    },
    {
        "text": "The term is what it is.",
        "start": 1866.73,
        "duration": 1.59
    },
    {
        "text": "[inaudible] real questions.",
        "start": 1868.32,
        "duration": 4.62
    },
    {
        "text": ">> It's a really good question.",
        "start": 1872.94,
        "duration": 2.58
    },
    {
        "text": "We've tried to look at that issue, Seth,",
        "start": 1875.52,
        "duration": 2.34
    },
    {
        "text": "from a couple of different perspectives.",
        "start": 1877.86,
        "duration": 2.7
    },
    {
        "text": "One, we just have this little axiom on the team.",
        "start": 1880.56,
        "duration": 5.01
    },
    {
        "text": "We say the fidelity of the AI should not",
        "start": 1885.57,
        "duration": 3.96
    },
    {
        "text": "exceed the fidelity of the capabilities.",
        "start": 1889.53,
        "duration": 4.8
    },
    {
        "text": "When you anthropomorphize something,",
        "start": 1894.33,
        "duration": 3.045
    },
    {
        "text": "you are for all intents and purposes,",
        "start": 1897.375,
        "duration": 2.94
    },
    {
        "text": "representing its capabilities as being",
        "start": 1900.315,
        "duration": 2.49
    },
    {
        "text": "human level and they're not.",
        "start": 1902.805,
        "duration": 3.475
    },
    {
        "text": "That's one issue that we would call it",
        "start": 1906.35,
        "duration": 3.025
    },
    {
        "text": "maybe an ethics issue because you are",
        "start": 1909.375,
        "duration": 2.535
    },
    {
        "text": "implicitly over-representing the capabilities of this thing.",
        "start": 1911.91,
        "duration": 5.25
    },
    {
        "text": "There's other stuff about like relationship formation,",
        "start": 1917.16,
        "duration": 3.675
    },
    {
        "text": "which I think goes to both the points that each of you were",
        "start": 1920.835,
        "duration": 3.465
    },
    {
        "text": "making about who is the responsible party?",
        "start": 1924.3,
        "duration": 4.81
    },
    {
        "text": "It could be a goal, but is our goal to help",
        "start": 1929.6,
        "duration": 2.83
    },
    {
        "text": "people build better relationships with robots?",
        "start": 1932.43,
        "duration": 3.015
    },
    {
        "text": "That could be the goal, but let's be explicit about that goal.",
        "start": 1935.445,
        "duration": 3.495
    },
    {
        "text": "I'm saying that's got to be a difference between in this ring,",
        "start": 1938.94,
        "duration": 3.48
    },
    {
        "text": "so you talk about the data sources",
        "start": 1942.42,
        "duration": 2.37
    },
    {
        "text": "in the optimization goals in the architecture",
        "start": 1944.79,
        "duration": 1.995
    },
    {
        "text": "when we're at that layer",
        "start": 1946.785,
        "duration": 3.36
    },
    {
        "text": "and we're talking about building a synthetic voice,",
        "start": 1950.145,
        "duration": 3.075
    },
    {
        "text": "the optimization goals for that synthetic voice,",
        "start": 1953.22,
        "duration": 4.065
    },
    {
        "text": "is how well they",
        "start": 1957.285,
        "duration": 3.225
    },
    {
        "text": "deceive people into believing that it's a real person.",
        "start": 1960.51,
        "duration": 3.54
    },
    {
        "text": "That's the measure. It's a subjective measure",
        "start": 1964.05,
        "duration": 2.46
    },
    {
        "text": "where people listen to a voice",
        "start": 1966.51,
        "duration": 1.875
    },
    {
        "text": "and they rate it from a 1-5 scale of realisticness.",
        "start": 1968.385,
        "duration": 4.83
    },
    {
        "text": "When it hits that top scale and they basically are like,",
        "start": 1973.215,
        "duration": 2.865
    },
    {
        "text": "''Yeah, I'm listening to a person.''",
        "start": 1976.08,
        "duration": 1.05
    },
    {
        "text": "You've won, you've succeeded,",
        "start": 1977.13,
        "duration": 1.38
    },
    {
        "text": "you've optimized your model to",
        "start": 1978.51,
        "duration": 1.44
    },
    {
        "text": "a point where your loss is sufficient.",
        "start": 1979.95,
        "duration": 2.53
    },
    {
        "text": "That's something we have to unpack.",
        "start": 1983.03,
        "duration": 2.545
    },
    {
        "text": "If that's your intent, is to deceive people and to",
        "start": 1985.575,
        "duration": 3.795
    },
    {
        "text": "potentially over-represent their functionality",
        "start": 1989.37,
        "duration": 2.28
    },
    {
        "text": "so that they can build a relationship with a robot,",
        "start": 1991.65,
        "duration": 2.92
    },
    {
        "text": "that's different than if it's right or wrong.",
        "start": 1994.85,
        "duration": 2.605
    },
    {
        "text": "It's just isn't your intent?",
        "start": 1997.455,
        "duration": 1.635
    },
    {
        "text": ">> I see. We've had",
        "start": 1999.09,
        "duration": 2.34
    },
    {
        "text": "a long discussion and we should definitely do this again",
        "start": 2001.43,
        "duration": 2.34
    },
    {
        "text": "because there's a lot more stuff and I want to make",
        "start": 2003.77,
        "duration": 1.89
    },
    {
        "text": "sure that we keep this tight.",
        "start": 2005.66,
        "duration": 2.895
    },
    {
        "text": "Sarah, could you maybe really quickly go through",
        "start": 2008.555,
        "duration": 4.485
    },
    {
        "text": "this anion with maybe a product that",
        "start": 2013.04,
        "duration": 2.58
    },
    {
        "text": "we built and how you've considered all these things.",
        "start": 2015.62,
        "duration": 2.79
    },
    {
        "text": "I know I'm putting you on the spot.",
        "start": 2018.41,
        "duration": 2.28
    },
    {
        "text": ">> I'm actually hoping you'll let us come back and do that with",
        "start": 2020.69,
        "duration": 3.45
    },
    {
        "text": "some of our upcoming releases that we can't talk about this week.",
        "start": 2024.14,
        "duration": 3.39
    },
    {
        "text": ">> Cool.",
        "start": 2027.53,
        "duration": 0.645
    },
    {
        "text": ">> Can I take a rain check on it?",
        "start": 2028.175,
        "duration": 1.875
    },
    {
        "text": ">> Absolutely. Because for me,",
        "start": 2030.05,
        "duration": 3.57
    },
    {
        "text": "this is all super highly conceptual,",
        "start": 2033.62,
        "duration": 2.595
    },
    {
        "text": "but I'd love to see and maybe we'll do this down",
        "start": 2036.215,
        "duration": 2.655
    },
    {
        "text": "the road where we go through a product and you can see like,",
        "start": 2038.87,
        "duration": 3.69
    },
    {
        "text": "oh, I see how this maps here,",
        "start": 2042.56,
        "duration": 1.26
    },
    {
        "text": "I see how this maps here,",
        "start": 2043.82,
        "duration": 1.02
    },
    {
        "text": "I see how this maps here,",
        "start": 2044.84,
        "duration": 1.08
    },
    {
        "text": "and I see how now we're being",
        "start": 2045.92,
        "duration": 1.47
    },
    {
        "text": "holistically ethical while at the same time being imperfect,",
        "start": 2047.39,
        "duration": 3.06
    },
    {
        "text": "which is what we only can do.",
        "start": 2050.45,
        "duration": 2.565
    },
    {
        "text": "We can strive for perfection,",
        "start": 2053.015,
        "duration": 1.575
    },
    {
        "text": "we will probably never reach it,",
        "start": 2054.59,
        "duration": 1.785
    },
    {
        "text": "but I love to see how this actually maps.",
        "start": 2056.375,
        "duration": 3.105
    },
    {
        "text": "To finish up because I want to make sure",
        "start": 2059.48,
        "duration": 1.53
    },
    {
        "text": "that people can digest this episode.",
        "start": 2061.01,
        "duration": 3.21
    },
    {
        "text": "Josh, what's the key takeaway here from an ethical perspective?",
        "start": 2064.22,
        "duration": 5.07
    },
    {
        "text": "Then Sarah, last word,",
        "start": 2069.29,
        "duration": 1.59
    },
    {
        "text": "how are we taking this to heart when we build AI products? Josh.",
        "start": 2070.88,
        "duration": 3.96
    },
    {
        "text": ">> Yeah, thanks. It's a great question.",
        "start": 2074.84,
        "duration": 2.52
    },
    {
        "text": "Thanks again for having us.",
        "start": 2077.36,
        "duration": 1.98
    },
    {
        "text": "I think the top thing I would hope folks",
        "start": 2079.34,
        "duration": 3.39
    },
    {
        "text": "takeaway is to be ethical or the",
        "start": 2082.73,
        "duration": 3.09
    },
    {
        "text": "start of a journey of being ethical is not even really about",
        "start": 2085.82,
        "duration": 3.15
    },
    {
        "text": "right or wrong or the moral values or what have you.",
        "start": 2088.97,
        "duration": 3.45
    },
    {
        "text": "There's lots of time and consideration needs to go into that.",
        "start": 2092.42,
        "duration": 5.175
    },
    {
        "text": "It's about saying what you're going to",
        "start": 2097.595,
        "duration": 1.725
    },
    {
        "text": "do and then doing what you're going to say.",
        "start": 2099.32,
        "duration": 2.175
    },
    {
        "text": "The ability to be intentional,",
        "start": 2101.495,
        "duration": 3.39
    },
    {
        "text": "to start with a purpose,",
        "start": 2104.885,
        "duration": 2.714
    },
    {
        "text": "and then follow that purpose",
        "start": 2107.599,
        "duration": 1.951
    },
    {
        "text": "through and be clear about where there are",
        "start": 2109.55,
        "duration": 3.69
    },
    {
        "text": "limitations about how accurately or",
        "start": 2113.24,
        "duration": 3.24
    },
    {
        "text": "reliably you are able to deliver on that purpose,",
        "start": 2116.48,
        "duration": 3.975
    },
    {
        "text": "that's really the bedrock.",
        "start": 2120.455,
        "duration": 2.49
    },
    {
        "text": "I think that's really within the capability of",
        "start": 2122.945,
        "duration": 2.655
    },
    {
        "text": "any developer or engineer",
        "start": 2125.6,
        "duration": 2.01
    },
    {
        "text": "or scientist that's working in this domain.",
        "start": 2127.61,
        "duration": 2.655
    },
    {
        "text": "I would ask you to go seek out",
        "start": 2130.265,
        "duration": 1.965
    },
    {
        "text": "your neighborhood UX designer or UX researcher,",
        "start": 2132.23,
        "duration": 3.23
    },
    {
        "text": "and engage in a conversation on that subject because we've",
        "start": 2135.46,
        "duration": 2.94
    },
    {
        "text": "got a lot of ideas and hopefully we can",
        "start": 2138.4,
        "duration": 1.59
    },
    {
        "text": "help you meet people where they're at.",
        "start": 2139.99,
        "duration": 2.375
    },
    {
        "text": ">> Sarah.",
        "start": 2142.365,
        "duration": 1.49
    },
    {
        "text": ">> Yeah, I think for us the biggest,",
        "start": 2143.855,
        "duration": 2.925
    },
    {
        "text": "most concrete change has just been starting with a purpose.",
        "start": 2146.78,
        "duration": 3.66
    },
    {
        "text": "Why are we building this technology?",
        "start": 2150.44,
        "duration": 2.13
    },
    {
        "text": "Then everything else in the diagram falls out from there.",
        "start": 2152.57,
        "duration": 4.59
    },
    {
        "text": "When you have a clear purpose,",
        "start": 2157.16,
        "duration": 1.86
    },
    {
        "text": "it's easier to even ask some of",
        "start": 2159.02,
        "duration": 2.31
    },
    {
        "text": "the hard questions and to make some",
        "start": 2161.33,
        "duration": 1.71
    },
    {
        "text": "of the more challenging trade-offs.",
        "start": 2163.04,
        "duration": 3.01
    },
    {
        "text": "I'm going to come back on the show",
        "start": 2166.39,
        "duration": 4.48
    },
    {
        "text": "and talk more about all of the specific technologies that",
        "start": 2170.87,
        "duration": 3.945
    },
    {
        "text": "we'll be announcing next week at Ignite and",
        "start": 2174.815,
        "duration": 4.335
    },
    {
        "text": "also moving forward in",
        "start": 2179.15,
        "duration": 3.54
    },
    {
        "text": "general so we can get really specific about for each technology,",
        "start": 2182.69,
        "duration": 3.3
    },
    {
        "text": "what we've done and what does it look",
        "start": 2185.99,
        "duration": 1.17
    },
    {
        "text": "like to put this into practice?",
        "start": 2187.16,
        "duration": 2.024
    },
    {
        "text": ">> Well, this has been amazing.",
        "start": 2189.184,
        "duration": 1.936
    },
    {
        "text": "Thank you so much for spending",
        "start": 2191.12,
        "duration": 1.17
    },
    {
        "text": "some time with us, and again, dear viewer,",
        "start": 2192.29,
        "duration": 1.35
    },
    {
        "text": "thank you so much for watching,",
        "start": 2193.64,
        "duration": 1.71
    },
    {
        "text": "we've been learning all about the ethics of AI.",
        "start": 2195.35,
        "duration": 2.64
    },
    {
        "text": "I think it boils down to what are we going to do?",
        "start": 2197.99,
        "duration": 3.195
    },
    {
        "text": "Are we going to stick to it?",
        "start": 2201.185,
        "duration": 1.47
    },
    {
        "text": "It's pretty cool.",
        "start": 2202.655,
        "duration": 1.23
    },
    {
        "text": "Thank you for watching and we'll see you next time. Take care.",
        "start": 2203.885,
        "duration": 2.205
    },
    {
        "text": "[MUSIC]",
        "start": 2206.09,
        "duration": 14.91
    }
]