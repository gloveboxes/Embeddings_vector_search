{"speaker": "", "title": "Faster and Lighter Model Inference with ONNX Runtime from Cloud to Client", "videoId": "WDww8ce12Mc", "description": "ONNX Runtime is a high-performance inferencing and training engine for machine learning models. This show focuses on ONNX Runtime for model inference. ONNX Runtime has been widely adopted by a variety of Microsoft products including Bing, Office 365 and Azure Cognitive Services, achieving an average of 2.9x inference speedup. Now we are glad to introduce ONNX Runtime quantization and ONNX Runtime mobile for further accelerating model inference with even smaller model size and runtime size. ONNX Runtime keeps evolving not only for cloud-based inference but also for on-device inference.\n\nJump To:  \n[00:00] Livestream begins\n[01:02] ONNX and ONNX Runtime overview https://aka.ms/AIShow/ONNXRuntimeGH\n[02:26] model operationalization with ONNX Runtime\n[04:04] ONNX Runtime adoption\n[05:07] ONNX Runtime INT8 quantization for model size reduction and inference speedup\n[09:46] Demo of ONNX Runtime INT8 quantization\n[16:00] ONNX Runtime mobile for runtime size reduction\n\nLearn More: \nONNX Runtime https://aka.ms/AIShow/ONNXRuntimeGH\nFaster and smaller quantized NLP with Hugging Face and ONNX Runtime https://aka.ms/AIShow/QuantizedNLP\nONNX Runtime for Mobile Platforms  https://aka.ms/AIShow/RuntimeforMobilePlatforms \nONNX Runtime Inference on Azure Machine Learning   https://aka.ms/AIShow/RuntimeInferenceonAML\n\nFollow ONNX AI https://twitter.com/onnxai\nFollow ONNX Runtime https://twitter.com/onnxruntime\n\nCreate a Free account (Azure) https://aka.ms/aishow-seth-azurefree\nDeep Learning vs. Machine Learning   https://aka.ms/AIShow/DLvML\nGet Started with Machine Learning  https://aka.ms/AIShow/StartML\nFollow Seth https://twitter.com/sethjuarez\n\nDon't miss new episodes, subscribe to the AI Show   https://aka.ms/aishowsubscribe"}