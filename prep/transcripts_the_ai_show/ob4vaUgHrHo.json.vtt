[
    {
        "text": ">> You're not going to want to miss this episode of",
        "start": 0.0,
        "duration": 1.5
    },
    {
        "text": "The AI Show where we dive into what",
        "start": 1.5,
        "duration": 2.355
    },
    {
        "text": "exactly is PyTorch both technical",
        "start": 3.855,
        "duration": 2.685
    },
    {
        "text": "and community with our good friend Dmytro.",
        "start": 6.54,
        "duration": 2.16
    },
    {
        "text": "Make sure you tune in.",
        "start": 8.7,
        "duration": 1.11
    },
    {
        "text": "[MUSIC]",
        "start": 9.81,
        "duration": 7.98
    },
    {
        "text": "Hello and welcome to this special edition of",
        "start": 17.79,
        "duration": 1.77
    },
    {
        "text": "the AI Show where I've got a special guest,",
        "start": 19.56,
        "duration": 2.19
    },
    {
        "text": "Dmytro Dzhulgakov who is going to talk a little bit about PyTorch.",
        "start": 21.75,
        "duration": 5.07
    },
    {
        "text": "Why don't you introduce yourself, my friend?",
        "start": 26.82,
        "duration": 1.89
    },
    {
        "text": ">> Hi, Seth. Thanks a lot for having me. My name is Dmytro.",
        "start": 28.71,
        "duration": 4.335
    },
    {
        "text": "I'm one of the core developers of PyTorch deep learning framework.",
        "start": 33.045,
        "duration": 3.51
    },
    {
        "text": "That's basically what we're going to talk about today.",
        "start": 36.555,
        "duration": 2.625
    },
    {
        "text": "A little bit about myself. I work at Facebook,",
        "start": 39.18,
        "duration": 4.245
    },
    {
        "text": "I have been in the space",
        "start": 43.425,
        "duration": 1.365
    },
    {
        "text": "of Machine Learning infrastructure for the last maybe six,",
        "start": 44.79,
        "duration": 3.8
    },
    {
        "text": "seven years, and particularly deployment",
        "start": 48.59,
        "duration": 2.415
    },
    {
        "text": "frameworks and PyTorch specifically",
        "start": 51.005,
        "duration": 1.905
    },
    {
        "text": "for the past three, four years.",
        "start": 52.91,
        "duration": 1.815
    },
    {
        "text": "I'm currently working on a lot of core abstractions",
        "start": 54.725,
        "duration": 4.285
    },
    {
        "text": "in the PyTorch development framework,",
        "start": 59.01,
        "duration": 1.59
    },
    {
        "text": "making sure that all the new features in",
        "start": 60.6,
        "duration": 2.19
    },
    {
        "text": "which community or [inaudible] fit together.",
        "start": 62.79,
        "duration": 3.12
    },
    {
        "text": "It's a nice and coherent product experience.",
        "start": 65.91,
        "duration": 3.7
    },
    {
        "text": ">> Well, I'm super excited because I started",
        "start": 69.61,
        "duration": 3.18
    },
    {
        "text": "using PyTorch maybe nine, 10 months ago",
        "start": 72.79,
        "duration": 4.18
    },
    {
        "text": "and I was super fascinated with how it was built.",
        "start": 76.97,
        "duration": 3.27
    },
    {
        "text": "From your perspective as a core contributor to PyTorch,",
        "start": 80.24,
        "duration": 3.72
    },
    {
        "text": "what makes PyTorch different?",
        "start": 83.96,
        "duration": 2.77
    },
    {
        "text": ">> The way I like to formulate the fusion is that PyTorch",
        "start": 87.2,
        "duration": 4.81
    },
    {
        "text": "tries to really embrace",
        "start": 92.01,
        "duration": 2.49
    },
    {
        "text": "the principles that Machine Learning is programming.",
        "start": 94.5,
        "duration": 2.295
    },
    {
        "text": "It doesn't try to take control",
        "start": 96.795,
        "duration": 3.275
    },
    {
        "text": "from you and business huge boxes system.",
        "start": 100.07,
        "duration": 3.31
    },
    {
        "text": "Instead, it tries to be just a set of libraries.",
        "start": 103.38,
        "duration": 3.195
    },
    {
        "text": "By that, it host programming language.",
        "start": 106.575,
        "duration": 2.225
    },
    {
        "text": "Well, as name suggest, it's Python.",
        "start": 108.8,
        "duration": 2.71
    },
    {
        "text": "Because of this approach,",
        "start": 111.51,
        "duration": 2.02
    },
    {
        "text": "it's really easy to get started.",
        "start": 113.53,
        "duration": 1.96
    },
    {
        "text": "It's really easy to use parts of PyTorch with unions",
        "start": 115.49,
        "duration": 2.85
    },
    {
        "text": "but maybe not use some libraries which we don't need.",
        "start": 118.34,
        "duration": 3.15
    },
    {
        "text": "So far, it's very easy to",
        "start": 121.49,
        "duration": 2.205
    },
    {
        "text": "debug and just connect",
        "start": 123.695,
        "duration": 2.715
    },
    {
        "text": "with whatever machine or industry we are going,",
        "start": 126.41,
        "duration": 1.78
    },
    {
        "text": "so pretty much any other part of your workflow or data science,",
        "start": 128.19,
        "duration": 4.325
    },
    {
        "text": "or even a general programming ecosystem.",
        "start": 132.515,
        "duration": 1.685
    },
    {
        "text": "That's because Python ecosystem is really rich and",
        "start": 134.2,
        "duration": 2.68
    },
    {
        "text": "powerful and probably has a library for anything you can imagine.",
        "start": 136.88,
        "duration": 2.96
    },
    {
        "text": ">> Because some people think that",
        "start": 139.84,
        "duration": 5.3
    },
    {
        "text": "PyTorch is just for deep learning, is that right?",
        "start": 145.14,
        "duration": 2.715
    },
    {
        "text": "How would you describe PyTorch just in general?",
        "start": 147.855,
        "duration": 3.685
    },
    {
        "text": ">> I will say core focus of PyTorch is to be",
        "start": 151.54,
        "duration": 5.14
    },
    {
        "text": "like developer environment,",
        "start": 156.68,
        "duration": 1.86
    },
    {
        "text": "like [inaudible] developer platform primarily",
        "start": 158.54,
        "duration": 2.31
    },
    {
        "text": "for deep learning but also a little bit",
        "start": 160.85,
        "duration": 3.12
    },
    {
        "text": "broader for numerical computation.",
        "start": 163.97,
        "duration": 2.64
    },
    {
        "text": "You can think at its core,",
        "start": 166.61,
        "duration": 1.95
    },
    {
        "text": "it's basically like NumPy with different devices support.",
        "start": 168.56,
        "duration": 4.02
    },
    {
        "text": "First of all, GPS and automatic differentiation",
        "start": 172.58,
        "duration": 4.289
    },
    {
        "text": "and all that rest is basic libraries on top.",
        "start": 176.869,
        "duration": 2.911
    },
    {
        "text": "Because of that, obviously it's very popular in",
        "start": 179.78,
        "duration": 2.49
    },
    {
        "text": "deep learning community and related applications.",
        "start": 182.27,
        "duration": 3.945
    },
    {
        "text": "But we actually have users also from traditional,",
        "start": 186.215,
        "duration": 3.984
    },
    {
        "text": "scientific configuration community in physics,",
        "start": 190.199,
        "duration": 3.186
    },
    {
        "text": "biology and stuff like that.",
        "start": 193.385,
        "duration": 1.755
    },
    {
        "text": "There are folks like it's certain,",
        "start": 195.14,
        "duration": 1.47
    },
    {
        "text": "for example, is in PyTorching stuff.",
        "start": 196.61,
        "duration": 1.98
    },
    {
        "text": "So pretty much you can think of it users",
        "start": 198.59,
        "duration": 2.19
    },
    {
        "text": "who would use NumPy and numerical computing",
        "start": 200.78,
        "duration": 6.189
    },
    {
        "text": "in Python has pretty much the same audience.",
        "start": 206.969,
        "duration": 3.346
    },
    {
        "text": ">> It's pretty cool because here's the thing that",
        "start": 210.315,
        "duration": 2.645
    },
    {
        "text": "I started to discover as I use PyTorch.",
        "start": 212.96,
        "duration": 2.55
    },
    {
        "text": "Just the way that it stores computation",
        "start": 215.51,
        "duration": 3.18
    },
    {
        "text": "and allows you to do things for example,",
        "start": 218.69,
        "duration": 2.76
    },
    {
        "text": "automatic differentiation, makes it a general purpose",
        "start": 221.45,
        "duration": 3.75
    },
    {
        "text": "library for doing complex mathematical things.",
        "start": 225.2,
        "duration": 4.2
    },
    {
        "text": "Would you say that that is the case or am I stretching?",
        "start": 229.4,
        "duration": 2.93
    },
    {
        "text": ">> Yeah, I think you're actually right.",
        "start": 232.33,
        "duration": 2.69
    },
    {
        "text": "At its core, it's basically linear algebra tensor.",
        "start": 235.02,
        "duration": 4.035
    },
    {
        "text": "A tensor library which allows you to",
        "start": 239.055,
        "duration": 3.09
    },
    {
        "text": "run those operations efficiently across a number of devices.",
        "start": 242.145,
        "duration": 3.74
    },
    {
        "text": "The CPUs, GPUs, or",
        "start": 245.885,
        "duration": 2.4
    },
    {
        "text": "maybe even more obscure",
        "start": 248.285,
        "duration": 1.845
    },
    {
        "text": "mobile embedded devices and stuff like that.",
        "start": 250.13,
        "duration": 2.64
    },
    {
        "text": "You can think of everything else",
        "start": 252.77,
        "duration": 1.845
    },
    {
        "text": "basically being layers on top of it.",
        "start": 254.615,
        "duration": 1.985
    },
    {
        "text": "I have tensor computations,",
        "start": 256.6,
        "duration": 1.75
    },
    {
        "text": "I want to compute gradients,",
        "start": 258.35,
        "duration": 1.16
    },
    {
        "text": "so that's auto gradient layer on top.",
        "start": 259.51,
        "duration": 2.085
    },
    {
        "text": "Now I want to build my neural network.",
        "start": 261.595,
        "duration": 2.055
    },
    {
        "text": "So I probably want to have a standard library of layers",
        "start": 263.65,
        "duration": 2.345
    },
    {
        "text": "that's Torching in and all the libraries related to that.",
        "start": 265.995,
        "duration": 3.2
    },
    {
        "text": "I want to run the optimization loop,",
        "start": 269.195,
        "duration": 2.27
    },
    {
        "text": "read my data, deploy the models,",
        "start": 271.465,
        "duration": 2.295
    },
    {
        "text": "and like all the further questions in the usual workflow",
        "start": 273.76,
        "duration": 3.48
    },
    {
        "text": "of Machine Learning deep learning practitioner.",
        "start": 277.24,
        "duration": 3.539
    },
    {
        "text": "Basically, whatever the step is,",
        "start": 280.779,
        "duration": 2.281
    },
    {
        "text": "there is usually an independent sub library",
        "start": 283.06,
        "duration": 4.125
    },
    {
        "text": "within PyTorch package which allows you to do that.",
        "start": 287.185,
        "duration": 2.79
    },
    {
        "text": "But what's cool is basically",
        "start": 289.975,
        "duration": 1.935
    },
    {
        "text": "those are structured as like libraries in an SDK.",
        "start": 291.91,
        "duration": 3.41
    },
    {
        "text": "They work well with each other but they are modular and pluggable.",
        "start": 295.32,
        "duration": 3.105
    },
    {
        "text": "If you want to have your own data reader or",
        "start": 298.425,
        "duration": 3.125
    },
    {
        "text": "you don't need something which is PyTorch related,",
        "start": 301.55,
        "duration": 3.42
    },
    {
        "text": "you don't have to use it and you don't have to",
        "start": 304.97,
        "duration": 1.74
    },
    {
        "text": "pay abstraction price for that.",
        "start": 306.71,
        "duration": 2.295
    },
    {
        "text": "You can bring your own or build",
        "start": 309.005,
        "duration": 1.725
    },
    {
        "text": "your own project on top, doing something with it.",
        "start": 310.73,
        "duration": 1.985
    },
    {
        "text": ">> This is cool. At the center then",
        "start": 312.715,
        "duration": 2.86
    },
    {
        "text": "is this tensor computational graph.",
        "start": 315.575,
        "duration": 3.11
    },
    {
        "text": "Then you wrap layers around it is what you're saying.",
        "start": 318.685,
        "duration": 2.745
    },
    {
        "text": "Let's do this if you don't mind.",
        "start": 321.43,
        "duration": 1.9
    },
    {
        "text": "Can we take a look at it as a Machine Learning",
        "start": 323.33,
        "duration": 2.19
    },
    {
        "text": "computational framework type thing",
        "start": 325.52,
        "duration": 2.67
    },
    {
        "text": "and start with the whole onion?",
        "start": 328.19,
        "duration": 3.12
    },
    {
        "text": "Let's start with the tensors and let's talk about the data",
        "start": 331.31,
        "duration": 3.225
    },
    {
        "text": "around it and how you actually move data in there.",
        "start": 334.535,
        "duration": 3.105
    },
    {
        "text": "Then let's move to the models,",
        "start": 337.64,
        "duration": 1.245
    },
    {
        "text": "the optimization and the deployment",
        "start": 338.885,
        "duration": 1.785
    },
    {
        "text": "because you mentioned each of those things.",
        "start": 340.67,
        "duration": 1.63
    },
    {
        "text": "Let's start with the data at its foundation,",
        "start": 342.3,
        "duration": 2.91
    },
    {
        "text": "tell me how PyTorch deals with data.",
        "start": 345.21,
        "duration": 2.88
    },
    {
        "text": ">> Yes. I mean, Machine Learning is usually",
        "start": 348.09,
        "duration": 3.11
    },
    {
        "text": "be inside 90 percent of data preparation",
        "start": 351.2,
        "duration": 3.15
    },
    {
        "text": "and 10 percent maybe of actual deep learning.",
        "start": 354.35,
        "duration": 3.225
    },
    {
        "text": "Obviously it's very important.",
        "start": 357.575,
        "duration": 2.28
    },
    {
        "text": "Again, as I said, PyTorch tries to embrace",
        "start": 359.855,
        "duration": 3.295
    },
    {
        "text": "Python ecosystem to its maximum.",
        "start": 363.15,
        "duration": 2.91
    },
    {
        "text": "If you basically get started, you can think of",
        "start": 366.06,
        "duration": 3.33
    },
    {
        "text": "how would you write your workflow in Python.",
        "start": 369.39,
        "duration": 2.88
    },
    {
        "text": "You'll probably have some iterator over your data",
        "start": 372.27,
        "duration": 3.26
    },
    {
        "text": "which may be reading from",
        "start": 375.53,
        "duration": 1.905
    },
    {
        "text": "local CSV file or from local folder with images.",
        "start": 377.435,
        "duration": 2.955
    },
    {
        "text": "Or maybe more production seconds like streaming from",
        "start": 380.39,
        "duration": 2.67
    },
    {
        "text": "some bucket and storage in the Cloud",
        "start": 383.06,
        "duration": 2.08
    },
    {
        "text": "or some database or whatever.",
        "start": 385.14,
        "duration": 2.055
    },
    {
        "text": "Basically you wrap it in the form of tensors and you do",
        "start": 387.195,
        "duration": 3.395
    },
    {
        "text": "your neural Networkish computation or computation on those.",
        "start": 390.59,
        "duration": 5.74
    },
    {
        "text": "Basically, PyTorch the core package comes with",
        "start": 396.33,
        "duration": 3.975
    },
    {
        "text": "big and simple abstractions to make it easy for",
        "start": 400.305,
        "duration": 2.795
    },
    {
        "text": "you to get started and handle typical tasks.",
        "start": 403.1,
        "duration": 2.96
    },
    {
        "text": "That's what's called Torch Data Loader.",
        "start": 406.06,
        "duration": 2.855
    },
    {
        "text": "If you have typical data sets,",
        "start": 408.915,
        "duration": 3.195
    },
    {
        "text": "set of images somewhere, something like this,",
        "start": 412.11,
        "duration": 2.03
    },
    {
        "text": "you can probably just grab existing class",
        "start": 414.14,
        "duration": 2.13
    },
    {
        "text": "and be able to read it efficiently in",
        "start": 416.27,
        "duration": 2.73
    },
    {
        "text": "a multithreaded fashion and have a library of",
        "start": 419.0,
        "duration": 2.88
    },
    {
        "text": "preprocessing tool so you can delete data augmentation,",
        "start": 421.88,
        "duration": 3.21
    },
    {
        "text": "and some professional re-scale images like",
        "start": 425.09,
        "duration": 3.78
    },
    {
        "text": "cropping the text preprocessing depending on your domain,",
        "start": 428.87,
        "duration": 4.02
    },
    {
        "text": "and basically produce an iterator of tensors which is",
        "start": 432.89,
        "duration": 4.65
    },
    {
        "text": "your mini batches of data going into",
        "start": 437.54,
        "duration": 1.77
    },
    {
        "text": "training and pretty much take it from there.",
        "start": 439.31,
        "duration": 2.67
    },
    {
        "text": "PyTorch is not like opinionated on how exactly it's structured.",
        "start": 441.98,
        "duration": 3.66
    },
    {
        "text": "There is libraries within",
        "start": 445.64,
        "duration": 2.05
    },
    {
        "text": "the Torch Data Loader within",
        "start": 447.69,
        "duration": 1.23
    },
    {
        "text": "PyTorch's shelves which makes it easier",
        "start": 448.92,
        "duration": 1.71
    },
    {
        "text": "to get started and use it for common cases.",
        "start": 450.63,
        "duration": 5.19
    },
    {
        "text": "But also if you're operating in",
        "start": 455.82,
        "duration": 1.77
    },
    {
        "text": "some very specialized workflow",
        "start": 457.59,
        "duration": 2.64
    },
    {
        "text": "like your data is in some proprietary,",
        "start": 460.23,
        "duration": 2.33
    },
    {
        "text": "crazy database from which you want the stream.",
        "start": 462.56,
        "duration": 2.28
    },
    {
        "text": "Again, because it's all just Python",
        "start": 464.84,
        "duration": 2.585
    },
    {
        "text": "and a general programming ecosystem, it's pretty easy to link.",
        "start": 467.425,
        "duration": 3.125
    },
    {
        "text": "We have extension mechanism,",
        "start": 470.55,
        "duration": 1.8
    },
    {
        "text": "you can just basically build your function in whatever language.",
        "start": 472.35,
        "duration": 3.81
    },
    {
        "text": "One thread like return data in a multidimensional array,",
        "start": 476.16,
        "duration": 3.87
    },
    {
        "text": "wrap it in PyTorch tensor and pretty much that will work.",
        "start": 480.03,
        "duration": 4.125
    },
    {
        "text": "It mirrors the philosophy of PyTorch,",
        "start": 484.155,
        "duration": 2.275
    },
    {
        "text": "basically to give you as a user and as",
        "start": 486.43,
        "duration": 2.5
    },
    {
        "text": "a developer the right tools to do",
        "start": 488.93,
        "duration": 2.46
    },
    {
        "text": "your workflow but not be extremely",
        "start": 491.39,
        "duration": 2.28
    },
    {
        "text": "opinionated that you have to do it exactly one way.",
        "start": 493.67,
        "duration": 3.315
    },
    {
        "text": "Because workflows differ and especially",
        "start": 496.985,
        "duration": 2.895
    },
    {
        "text": "in research or bigger production settings,",
        "start": 499.88,
        "duration": 3.045
    },
    {
        "text": "there's a lot of variability between these cases so",
        "start": 502.925,
        "duration": 1.935
    },
    {
        "text": "the customizability and modularity are important.",
        "start": 504.86,
        "duration": 3.37
    },
    {
        "text": ">> Look the first time I learned how to do this,",
        "start": 509.18,
        "duration": 3.66
    },
    {
        "text": "obviously when you're doing machine learning type of stuff,",
        "start": 512.84,
        "duration": 2.82
    },
    {
        "text": "your goal is to get any piece of data",
        "start": 515.66,
        "duration": 1.77
    },
    {
        "text": "into some multidimensional array,",
        "start": 517.43,
        "duration": 2.9
    },
    {
        "text": "I guess a Tensor in this case.",
        "start": 520.33,
        "duration": 2.05
    },
    {
        "text": "But then the first abstraction that I looked at was the data set.",
        "start": 522.38,
        "duration": 3.81
    },
    {
        "text": "I was like, \"Okay, so now I need to inherit from data set.\"",
        "start": 526.19,
        "duration": 2.37
    },
    {
        "text": "I went to look at the base data",
        "start": 528.56,
        "duration": 2.37
    },
    {
        "text": "set class and it was basically like,",
        "start": 530.93,
        "duration": 2.01
    },
    {
        "text": "\"Well, you just need to tell us how many are",
        "start": 532.94,
        "duration": 2.25
    },
    {
        "text": "there and how to get one from your set?\"",
        "start": 535.19,
        "duration": 2.52
    },
    {
        "text": ">> Exactly.",
        "start": 537.71,
        "duration": 0.585
    },
    {
        "text": ">> I was like, \"Whoa,",
        "start": 538.295,
        "duration": 1.38
    },
    {
        "text": "this is literally nothing.\"",
        "start": 539.675,
        "duration": 1.875
    },
    {
        "text": "I can literally do whatever I want,",
        "start": 541.55,
        "duration": 2.94
    },
    {
        "text": "which was really cool to me.",
        "start": 544.49,
        "duration": 2.635
    },
    {
        "text": "Then once I had that base abstraction,",
        "start": 547.125,
        "duration": 2.225
    },
    {
        "text": "I could literally just piggyback onto a data loader",
        "start": 549.35,
        "duration": 2.37
    },
    {
        "text": "to get me the batch size that I want.",
        "start": 551.72,
        "duration": 2.267
    },
    {
        "text": ">> Yeah.",
        "start": 553.987,
        "duration": 0.268
    },
    {
        "text": ">> I could put transformers in there to",
        "start": 554.255,
        "duration": 2.325
    },
    {
        "text": "transform the data as it came out if I want,",
        "start": 556.58,
        "duration": 2.865
    },
    {
        "text": "or I could have put that in the data set.",
        "start": 559.445,
        "duration": 1.95
    },
    {
        "text": "To me the whole data to Tensor to data set to data loader",
        "start": 561.395,
        "duration": 5.22
    },
    {
        "text": "was super freeing in that",
        "start": 566.615,
        "duration": 2.115
    },
    {
        "text": "the structure was there if I wanted",
        "start": 568.73,
        "duration": 1.56
    },
    {
        "text": "it but I didn't necessarily need it.",
        "start": 570.29,
        "duration": 1.98
    },
    {
        "text": ">> Exactly.",
        "start": 572.27,
        "duration": 0.745
    },
    {
        "text": ">> That's what important for me.",
        "start": 573.015,
        "duration": 2.245
    },
    {
        "text": ">> Yeah. To extend your example,",
        "start": 575.26,
        "duration": 3.075
    },
    {
        "text": "you have image folder dataset which",
        "start": 578.335,
        "duration": 2.085
    },
    {
        "text": "would implement what are there.",
        "start": 580.42,
        "duration": 3.29
    },
    {
        "text": "Whatever file size in the directory,",
        "start": 583.71,
        "duration": 1.695
    },
    {
        "text": "how to get one, just read that file.",
        "start": 585.405,
        "duration": 2.235
    },
    {
        "text": "That works for small and medium size use cases.",
        "start": 587.64,
        "duration": 3.165
    },
    {
        "text": "If it's probably something like super large scale,",
        "start": 590.805,
        "duration": 2.295
    },
    {
        "text": "there are libraries out there connecting PyTorch data loader,",
        "start": 593.1,
        "duration": 4.435
    },
    {
        "text": "dataset implementations to Cloud providers,",
        "start": 597.535,
        "duration": 3.045
    },
    {
        "text": "like Azure or AWS or whatever,",
        "start": 600.58,
        "duration": 2.73
    },
    {
        "text": "to just stream data from some storage bucket somewhere.",
        "start": 603.31,
        "duration": 3.57
    },
    {
        "text": "That's also like you have to pay for what you've actually used.",
        "start": 606.88,
        "duration": 5.025
    },
    {
        "text": "If something doesn't suit here, as you said,",
        "start": 611.905,
        "duration": 1.86
    },
    {
        "text": "you can always inherit from that class,",
        "start": 613.765,
        "duration": 2.835
    },
    {
        "text": "or inherit from different extension point and basically",
        "start": 616.6,
        "duration": 2.88
    },
    {
        "text": "implement whatever logic you",
        "start": 619.48,
        "duration": 2.16
    },
    {
        "text": "might need and optimize it for your settings.",
        "start": 621.64,
        "duration": 2.34
    },
    {
        "text": ">> Let's move over now. Now that we've",
        "start": 623.98,
        "duration": 1.5
    },
    {
        "text": "talked about data, because again,",
        "start": 625.48,
        "duration": 1.575
    },
    {
        "text": "to me I just had this notion of tensor computational graph,",
        "start": 627.055,
        "duration": 4.875
    },
    {
        "text": "the data set, the data loader.",
        "start": 631.93,
        "duration": 1.695
    },
    {
        "text": "Let's talk about building actual models.",
        "start": 633.625,
        "duration": 2.729
    },
    {
        "text": "Is it the same philosophy in PyTorch?",
        "start": 636.354,
        "duration": 2.881
    },
    {
        "text": "The same approach to build your own models?",
        "start": 639.235,
        "duration": 3.195
    },
    {
        "text": ">> Yes. The general idea is the same",
        "start": 642.43,
        "duration": 1.95
    },
    {
        "text": "as decomposed like necessary building blocks to",
        "start": 644.38,
        "duration": 3.84
    },
    {
        "text": "their minimal useful components",
        "start": 648.22,
        "duration": 2.34
    },
    {
        "text": "and give you some control",
        "start": 650.56,
        "duration": 2.04
    },
    {
        "text": "to each of those they want to use or not.",
        "start": 652.6,
        "duration": 2.37
    },
    {
        "text": "For example, if you are following",
        "start": 654.97,
        "duration": 2.235
    },
    {
        "text": "a tutorial how to build a neural network,",
        "start": 657.205,
        "duration": 3.27
    },
    {
        "text": "you usually learn about how to do linear algebra operations.",
        "start": 660.475,
        "duration": 3.195
    },
    {
        "text": "Then you learn about gradients and how to do",
        "start": 663.67,
        "duration": 3.9
    },
    {
        "text": "automatic differentiation with",
        "start": 667.57,
        "duration": 1.275
    },
    {
        "text": "usually reverse mode Autograd and chain rule.",
        "start": 668.845,
        "duration": 4.215
    },
    {
        "text": "Then you learned about different layers",
        "start": 673.06,
        "duration": 2.04
    },
    {
        "text": "and how to construct actual training loop,",
        "start": 675.1,
        "duration": 4.275
    },
    {
        "text": "which reads data in like pass forward,",
        "start": 679.375,
        "duration": 2.04
    },
    {
        "text": "pass backward, pass and calls optimizer.",
        "start": 681.415,
        "duration": 1.995
    },
    {
        "text": "This is exactly a structure which abstractions in PyTorch follow.",
        "start": 683.41,
        "duration": 3.09
    },
    {
        "text": "you have general computations which we talked about.",
        "start": 686.5,
        "duration": 2.61
    },
    {
        "text": "You have confined called Autograd,",
        "start": 689.11,
        "duration": 1.845
    },
    {
        "text": "which is basically actually similar to",
        "start": 690.955,
        "duration": 2.97
    },
    {
        "text": "original Autograd projects coming from 1980s.",
        "start": 693.925,
        "duration": 4.41
    },
    {
        "text": "The idea is you can flip a flag on",
        "start": 698.335,
        "duration": 3.42
    },
    {
        "text": "a tensor saying I'm interested in",
        "start": 701.755,
        "duration": 2.235
    },
    {
        "text": "computing gradients for this one and now,",
        "start": 703.99,
        "duration": 1.95
    },
    {
        "text": "like whatever operation you're going to do with these tensors.",
        "start": 705.94,
        "duration": 2.52
    },
    {
        "text": "But you should be remembering which steps were taken,",
        "start": 708.46,
        "duration": 2.595
    },
    {
        "text": "software that you can take some result",
        "start": 711.055,
        "duration": 2.235
    },
    {
        "text": "which is usually lost in different cases,",
        "start": 713.29,
        "duration": 2.31
    },
    {
        "text": "say the backwards, and",
        "start": 715.6,
        "duration": 1.635
    },
    {
        "text": "automatically get gradients for the original tensors,",
        "start": 717.235,
        "duration": 4.635
    },
    {
        "text": "which in this case usually will be weights of the neural network.",
        "start": 721.87,
        "duration": 3.135
    },
    {
        "text": "Again, Autograd is completely pluggable.",
        "start": 725.005,
        "duration": 2.055
    },
    {
        "text": "It doesn't try to enforce what the structure of your program is.",
        "start": 727.06,
        "duration": 4.695
    },
    {
        "text": "Like your program might have been arbitrary Python control flow,",
        "start": 731.755,
        "duration": 3.465
    },
    {
        "text": "it can have recursive function in the middle.",
        "start": 735.22,
        "duration": 2.355
    },
    {
        "text": "It can have a bunch of lambdas and whatever.",
        "start": 737.575,
        "duration": 3.045
    },
    {
        "text": "It doesn't really matter like you can use whatever.",
        "start": 740.62,
        "duration": 1.995
    },
    {
        "text": "Python has available Autograd,",
        "start": 742.615,
        "duration": 3.525
    },
    {
        "text": "just like basically trace whatever tensor operations",
        "start": 746.14,
        "duration": 2.55
    },
    {
        "text": "were happening in your program and let you to compute gradients.",
        "start": 748.69,
        "duration": 3.255
    },
    {
        "text": "That makes it very pluggable",
        "start": 751.945,
        "duration": 2.415
    },
    {
        "text": "and flexible, especially for research.",
        "start": 754.36,
        "duration": 1.875
    },
    {
        "text": "If you look at usually for vision models,",
        "start": 756.235,
        "duration": 4.695
    },
    {
        "text": "it's usually pretty simple.",
        "start": 760.93,
        "duration": 1.395
    },
    {
        "text": "Everybody likes to look at [inaudible].",
        "start": 762.325,
        "duration": 3.675
    },
    {
        "text": "But if you go in especially crazier NLP research,",
        "start": 766.0,
        "duration": 3.36
    },
    {
        "text": "people do fun stuff like tree-structured to STM, whatever.",
        "start": 769.36,
        "duration": 4.35
    },
    {
        "text": "The way your model looks like,",
        "start": 773.71,
        "duration": 1.53
    },
    {
        "text": "it's literally recursive function trying",
        "start": 775.24,
        "duration": 1.77
    },
    {
        "text": "to parse the sentence and stuff like this.",
        "start": 777.01,
        "duration": 2.97
    },
    {
        "text": "Because Autograd doesn't try",
        "start": 779.98,
        "duration": 2.25
    },
    {
        "text": "to import the structure of your program,",
        "start": 782.23,
        "duration": 1.29
    },
    {
        "text": "it just records operations,",
        "start": 783.52,
        "duration": 1.29
    },
    {
        "text": "this tensor that happened, that works.",
        "start": 784.81,
        "duration": 1.965
    },
    {
        "text": "You can still differentiate tensors",
        "start": 786.775,
        "duration": 3.765
    },
    {
        "text": "flowing through your program.",
        "start": 790.54,
        "duration": 1.305
    },
    {
        "text": "We talked about Autograd.",
        "start": 791.845,
        "duration": 1.935
    },
    {
        "text": "Usually next layer is grabbing",
        "start": 793.78,
        "duration": 3.69
    },
    {
        "text": "actual standard neural network layers,",
        "start": 797.47,
        "duration": 2.985
    },
    {
        "text": "some convolutional layer",
        "start": 800.455,
        "duration": 2.445
    },
    {
        "text": "or different activation functions.",
        "start": 802.9,
        "duration": 2.805
    },
    {
        "text": "PyTorch, from the beginning comes to",
        "start": 805.705,
        "duration": 3.075
    },
    {
        "text": "this standard library of neural networks layers,",
        "start": 808.78,
        "duration": 3.735
    },
    {
        "text": "torch and N. It's directly in the core package,",
        "start": 812.515,
        "duration": 3.09
    },
    {
        "text": "and that's pretty much all you need to",
        "start": 815.605,
        "duration": 2.325
    },
    {
        "text": "construct your regular neural network",
        "start": 817.93,
        "duration": 2.28
    },
    {
        "text": "and what those layers really combine.",
        "start": 820.21,
        "duration": 2.1
    },
    {
        "text": "They basically tie in state.",
        "start": 822.31,
        "duration": 2.355
    },
    {
        "text": "You can think of neural network layer",
        "start": 824.665,
        "duration": 2.745
    },
    {
        "text": "presented as a module in PyTorch,",
        "start": 827.41,
        "duration": 2.01
    },
    {
        "text": "it's basically as an object through it,",
        "start": 829.42,
        "duration": 1.725
    },
    {
        "text": "and its weight is a state which you're going to train.",
        "start": 831.145,
        "duration": 3.81
    },
    {
        "text": "It has a forward function",
        "start": 834.955,
        "duration": 1.605
    },
    {
        "text": "which when I'm going to call this module,",
        "start": 836.56,
        "duration": 1.95
    },
    {
        "text": "basically apply this layer to some inputs to produce some output.",
        "start": 838.51,
        "duration": 2.7
    },
    {
        "text": "Again, it's pretty much embracing like",
        "start": 841.21,
        "duration": 2.29
    },
    {
        "text": "traditional software engineering wisdom",
        "start": 843.5,
        "duration": 2.62
    },
    {
        "text": "of object-oriented programming.",
        "start": 846.12,
        "duration": 1.77
    },
    {
        "text": "So very powerful abstraction for constructing your networks.",
        "start": 847.89,
        "duration": 3.51
    },
    {
        "text": "If you're trying to do something",
        "start": 851.4,
        "duration": 1.17
    },
    {
        "text": "like weight sharing where you have",
        "start": 852.57,
        "duration": 1.965
    },
    {
        "text": "the same module applied into different places in your network,",
        "start": 854.535,
        "duration": 3.77
    },
    {
        "text": "it's very convenient to express",
        "start": 858.305,
        "duration": 2.525
    },
    {
        "text": "for programming experience perspective",
        "start": 860.83,
        "duration": 3.21
    },
    {
        "text": "because you just create an object and you call it twice.",
        "start": 864.04,
        "duration": 2.28
    },
    {
        "text": "Just how you would do it in regular Python or C++ program.",
        "start": 866.32,
        "duration": 4.155
    },
    {
        "text": "With neural network library,",
        "start": 870.475,
        "duration": 2.07
    },
    {
        "text": "you can basically construct your typical neural network structure.",
        "start": 872.545,
        "duration": 4.41
    },
    {
        "text": "Again, because it's just Python,",
        "start": 876.955,
        "duration": 1.935
    },
    {
        "text": "you can have like arbitrary control flow,",
        "start": 878.89,
        "duration": 2.04
    },
    {
        "text": "you can have loops and",
        "start": 880.93,
        "duration": 1.56
    },
    {
        "text": "branching and whatever you want in the middle.",
        "start": 882.49,
        "duration": 2.325
    },
    {
        "text": "You can still compute the derivatives of parameters",
        "start": 884.815,
        "duration": 3.21
    },
    {
        "text": "by turning on Autograd,",
        "start": 888.025,
        "duration": 3.135
    },
    {
        "text": "like passing your inputs through the network.",
        "start": 891.16,
        "duration": 2.835
    },
    {
        "text": "That's pretty much what you need for forward and backward",
        "start": 893.995,
        "duration": 2.625
    },
    {
        "text": "to pass a typical neural networks training loop.",
        "start": 896.62,
        "duration": 4.275
    },
    {
        "text": "That pretty much gives you necessary components to construct",
        "start": 900.895,
        "duration": 3.915
    },
    {
        "text": "the entire training loop and train your first neural network.",
        "start": 904.81,
        "duration": 5.34
    },
    {
        "text": "You would take data loader, which you talked about,",
        "start": 910.15,
        "duration": 2.445
    },
    {
        "text": "which grabs data from somewhere and packages to the tensors.",
        "start": 912.595,
        "duration": 3.63
    },
    {
        "text": "You would construct your model,",
        "start": 916.225,
        "duration": 1.86
    },
    {
        "text": "their hierarchy of modules.",
        "start": 918.085,
        "duration": 1.98
    },
    {
        "text": "You would call it on the inputs.",
        "start": 920.065,
        "duration": 2.295
    },
    {
        "text": "It will produce your loss,",
        "start": 922.36,
        "duration": 2.4
    },
    {
        "text": "you would call Autograd backwards",
        "start": 924.76,
        "duration": 2.16
    },
    {
        "text": "computing their derivatives of parameters.",
        "start": 926.92,
        "duration": 3.45
    },
    {
        "text": "You would call optimizer layers from the standard libraries.",
        "start": 930.37,
        "duration": 2.61
    },
    {
        "text": "So you want to play SGD,",
        "start": 932.98,
        "duration": 1.53
    },
    {
        "text": "Adam, or Autograd or whatever,",
        "start": 934.51,
        "duration": 1.77
    },
    {
        "text": "your favorite optimization method",
        "start": 936.28,
        "duration": 1.56
    },
    {
        "text": "is and pretty much do all of that in the loop.",
        "start": 937.84,
        "duration": 2.94
    },
    {
        "text": "Keep reading your data and updating parameters and that would be",
        "start": 940.78,
        "duration": 3.0
    },
    {
        "text": "your classical training loop for a neural network.",
        "start": 943.78,
        "duration": 4.44
    },
    {
        "text": "But again, because it's like very explicit",
        "start": 948.22,
        "duration": 1.92
    },
    {
        "text": "and following the programming abstractions,",
        "start": 950.14,
        "duration": 3.719
    },
    {
        "text": "it's very easy to recreate.",
        "start": 953.859,
        "duration": 1.411
    },
    {
        "text": "Now if you are trying to do against, for example,",
        "start": 955.27,
        "duration": 4.005
    },
    {
        "text": "generative adversarial neural networks where",
        "start": 959.275,
        "duration": 3.225
    },
    {
        "text": "training loop looks like two separate parts of the loop,",
        "start": 962.5,
        "duration": 2.7
    },
    {
        "text": "it's very easy to understand where to plug it in.",
        "start": 965.2,
        "duration": 3.075
    },
    {
        "text": "Now you have two modules, just call one or another.",
        "start": 968.275,
        "duration": 2.565
    },
    {
        "text": "If you have something more complicated",
        "start": 970.84,
        "duration": 3.0
    },
    {
        "text": "like lists of tree structures, again,",
        "start": 973.84,
        "duration": 3.21
    },
    {
        "text": "it's very easy to express it in this program and really this",
        "start": 977.05,
        "duration": 3.195
    },
    {
        "text": "simplicity and decompose it",
        "start": 980.245,
        "duration": 1.575
    },
    {
        "text": "into individual components which work well together.",
        "start": 981.82,
        "duration": 3.645
    },
    {
        "text": "Gives both good user experience,",
        "start": 985.465,
        "duration": 3.135
    },
    {
        "text": "but also a good flexibility for more advanced use cases.",
        "start": 988.6,
        "duration": 4.29
    },
    {
        "text": "I guess it's one of the reasons why PyTorch was",
        "start": 992.89,
        "duration": 2.76
    },
    {
        "text": "even from the initial versions very popular and these",
        "start": 995.65,
        "duration": 3.45
    },
    {
        "text": "days very popular especially in research environment",
        "start": 999.1,
        "duration": 3.6
    },
    {
        "text": "where people need a lot of this flexibility,",
        "start": 1002.7,
        "duration": 2.43
    },
    {
        "text": "how to tune, how to trick their model structure in their training,",
        "start": 1005.13,
        "duration": 4.56
    },
    {
        "text": "their regime and stuff like that.",
        "start": 1009.69,
        "duration": 1.47
    },
    {
        "text": ">> This is cool because basically I asked you about models,",
        "start": 1011.16,
        "duration": 4.305
    },
    {
        "text": "but you actually started with computational graphs,",
        "start": 1015.465,
        "duration": 2.955
    },
    {
        "text": "which when you think about it,",
        "start": 1018.42,
        "duration": 1.29
    },
    {
        "text": "it's like, yeah, that's basically what a model is.",
        "start": 1019.71,
        "duration": 2.235
    },
    {
        "text": "It's a forward computation that you pass forward,",
        "start": 1021.945,
        "duration": 3.81
    },
    {
        "text": "but when you're trying to optimize it,",
        "start": 1025.755,
        "duration": 2.145
    },
    {
        "text": "you're just basically going",
        "start": 1027.9,
        "duration": 1.53
    },
    {
        "text": "backwards and that's all just built in.",
        "start": 1029.43,
        "duration": 2.565
    },
    {
        "text": "You mentioned something about,",
        "start": 1031.995,
        "duration": 1.455
    },
    {
        "text": "and I think it's NN.modules.",
        "start": 1033.45,
        "duration": 1.71
    },
    {
        "text": "Is that the right place where these things are?",
        "start": 1035.16,
        "duration": 2.25
    },
    {
        "text": ">> Yeah. The actual library with standard layers is like torch.nn.",
        "start": 1037.41,
        "duration": 4.08
    },
    {
        "text": "So you will have a torch.nn linear,",
        "start": 1041.49,
        "duration": 1.95
    },
    {
        "text": "or torch.nn comms2-D or something like this.",
        "start": 1043.44,
        "duration": 2.745
    },
    {
        "text": "Also like you mentioned about conditional graphs.",
        "start": 1046.185,
        "duration": 2.205
    },
    {
        "text": "Again, PyTorch, if you wanted to do backward pass,",
        "start": 1048.39,
        "duration": 3.36
    },
    {
        "text": "you actually record like the graphic table stuff which happens.",
        "start": 1051.75,
        "duration": 3.165
    },
    {
        "text": "But in forward you actually don't",
        "start": 1054.915,
        "duration": 1.485
    },
    {
        "text": "have to construct the graph like so.",
        "start": 1056.4,
        "duration": 1.83
    },
    {
        "text": "Which is just a Python program.",
        "start": 1058.23,
        "duration": 2.1
    },
    {
        "text": "You can call arbitrary Python",
        "start": 1060.33,
        "duration": 2.04
    },
    {
        "text": "function in the middle if you wanted to,",
        "start": 1062.37,
        "duration": 2.565
    },
    {
        "text": "which basically means your model structure",
        "start": 1064.935,
        "duration": 4.305
    },
    {
        "text": "is not constrained to what a graph should be like.",
        "start": 1069.24,
        "duration": 2.82
    },
    {
        "text": "You don't need to learn your fancy constructs of PyTorch itself.",
        "start": 1072.06,
        "duration": 4.875
    },
    {
        "text": "If you want to write an IF statement,",
        "start": 1076.935,
        "duration": 1.98
    },
    {
        "text": "you basically write Python IF statement. That's it.",
        "start": 1078.915,
        "duration": 2.835
    },
    {
        "text": ">> That's really cool. Because like I said,",
        "start": 1081.75,
        "duration": 2.1
    },
    {
        "text": "when you're trying to optimize these things,",
        "start": 1083.85,
        "duration": 2.04
    },
    {
        "text": "which is what you weigh in to,",
        "start": 1085.89,
        "duration": 1.815
    },
    {
        "text": "the cool bit about this is in",
        "start": 1087.705,
        "duration": 2.04
    },
    {
        "text": "other frameworks you have to be very",
        "start": 1089.745,
        "duration": 1.185
    },
    {
        "text": "cognizant of what's happening.",
        "start": 1090.93,
        "duration": 1.829
    },
    {
        "text": "But in this framework,",
        "start": 1092.759,
        "duration": 1.261
    },
    {
        "text": "because of the way the things flow forward,",
        "start": 1094.02,
        "duration": 2.76
    },
    {
        "text": "if you do an IF statement,",
        "start": 1096.78,
        "duration": 1.32
    },
    {
        "text": "it just doesn't go through that part of the graph and it",
        "start": 1098.1,
        "duration": 1.98
    },
    {
        "text": "doesn't compute the gradients if you don't need them.",
        "start": 1100.08,
        "duration": 2.37
    },
    {
        "text": "If you don't need the gradients at all,",
        "start": 1102.45,
        "duration": 1.11
    },
    {
        "text": "you just say don't compute the gradients,",
        "start": 1103.56,
        "duration": 1.83
    },
    {
        "text": "which to me it was really cool as well.",
        "start": 1105.39,
        "duration": 2.73
    },
    {
        "text": ">> Then the other thing that was",
        "start": 1108.12,
        "duration": 2.07
    },
    {
        "text": "interesting is everyone knows about",
        "start": 1110.19,
        "duration": 1.5
    },
    {
        "text": "AECOM 2D rider or linear layer.",
        "start": 1111.69,
        "duration": 3.705
    },
    {
        "text": "You can actually make your own layers",
        "start": 1115.395,
        "duration": 1.695
    },
    {
        "text": "however you like by just sub-classing",
        "start": 1117.09,
        "duration": 2.19
    },
    {
        "text": "the right thing and then putting it",
        "start": 1119.28,
        "duration": 1.17
    },
    {
        "text": "in the other structures, correct?",
        "start": 1120.45,
        "duration": 2.685
    },
    {
        "text": ">> Yeah, exactly. PyTorch and library,",
        "start": 1123.135,
        "duration": 5.28
    },
    {
        "text": "is probably the most common and the most standard layers,",
        "start": 1128.415,
        "duration": 3.105
    },
    {
        "text": "which pretty much everybody doing",
        "start": 1131.52,
        "duration": 2.79
    },
    {
        "text": "deploying usually agrees upon.",
        "start": 1134.31,
        "duration": 2.19
    },
    {
        "text": "But if you look at more domain-specific libraries,",
        "start": 1136.5,
        "duration": 3.045
    },
    {
        "text": "either part of PyTorch,",
        "start": 1139.545,
        "duration": 1.935
    },
    {
        "text": "media tele-systems such as Torchvision or Torchtext,",
        "start": 1141.48,
        "duration": 2.775
    },
    {
        "text": "Torchaudio for respectfully like different domains.",
        "start": 1144.255,
        "duration": 3.345
    },
    {
        "text": "They will have more specialized modules.",
        "start": 1147.6,
        "duration": 3.015
    },
    {
        "text": "For example vision, the detection style of",
        "start": 1150.615,
        "duration": 4.065
    },
    {
        "text": "red layers like non-medium suppression.",
        "start": 1154.68,
        "duration": 3.795
    },
    {
        "text": "If you're trying to do detection,",
        "start": 1158.475,
        "duration": 0.975
    },
    {
        "text": "classical stuff like that,",
        "start": 1159.45,
        "duration": 1.26
    },
    {
        "text": "if you look at some projects doing 3D detection,",
        "start": 1160.71,
        "duration": 5.625
    },
    {
        "text": "they would have their own layers implemented",
        "start": 1166.335,
        "duration": 2.385
    },
    {
        "text": "and imagery of cases like very easy to",
        "start": 1168.72,
        "duration": 2.37
    },
    {
        "text": "understand how it's done because it just",
        "start": 1171.09,
        "duration": 2.43
    },
    {
        "text": "inherit from a Python class or maybe right there.",
        "start": 1173.52,
        "duration": 3.345
    },
    {
        "text": "Call somebody and ultimately, again,",
        "start": 1176.865,
        "duration": 2.325
    },
    {
        "text": "if you want to extend that Autograd,",
        "start": 1179.19,
        "duration": 1.26
    },
    {
        "text": "you just inherit from Autograd function specify,",
        "start": 1180.45,
        "duration": 2.895
    },
    {
        "text": "what is my forward part,",
        "start": 1183.345,
        "duration": 1.275
    },
    {
        "text": "what is my backward part.",
        "start": 1184.62,
        "duration": 1.479
    },
    {
        "text": "That kind of places, that rest of the program.",
        "start": 1186.099,
        "duration": 2.411
    },
    {
        "text": ">> As I said, to me it was really cool because",
        "start": 1188.51,
        "duration": 2.76
    },
    {
        "text": "now I could literally make any structure",
        "start": 1191.27,
        "duration": 3.209
    },
    {
        "text": "that I wanted and optimize it.",
        "start": 1194.479,
        "duration": 1.546
    },
    {
        "text": "That's why it doesn't have to be reserved",
        "start": 1196.025,
        "duration": 2.575
    },
    {
        "text": "or just deep learning.",
        "start": 1198.6,
        "duration": 0.81
    },
    {
        "text": "It could be any mathematical optimization",
        "start": 1199.41,
        "duration": 2.16
    },
    {
        "text": "[inaudible] in theory.",
        "start": 1201.57,
        "duration": 1.245
    },
    {
        "text": "You could totally do this.",
        "start": 1202.815,
        "duration": 1.5
    },
    {
        "text": "The optimization loop to me was",
        "start": 1204.315,
        "duration": 1.62
    },
    {
        "text": "super cool because basically, you got the batch,",
        "start": 1205.935,
        "duration": 2.79
    },
    {
        "text": "you put it in, you measure the loss,",
        "start": 1208.725,
        "duration": 2.445
    },
    {
        "text": "you take the gradients,",
        "start": 1211.17,
        "duration": 1.364
    },
    {
        "text": "and then you update the weights,",
        "start": 1212.534,
        "duration": 1.486
    },
    {
        "text": "and then you keep doing that until you're done.",
        "start": 1214.02,
        "duration": 2.025
    },
    {
        "text": "Which is basically the whole optimization problem.",
        "start": 1216.045,
        "duration": 3.105
    },
    {
        "text": ">> Yeah. I mean,",
        "start": 1219.15,
        "duration": 1.785
    },
    {
        "text": "if you want to take it further,",
        "start": 1220.935,
        "duration": 2.085
    },
    {
        "text": "another example of molarity, the support, for example,",
        "start": 1223.02,
        "duration": 3.18
    },
    {
        "text": "higher-order gradients, when you're doing your backwards,",
        "start": 1226.2,
        "duration": 2.76
    },
    {
        "text": "you can turn on recording gradients on the [inaudible].",
        "start": 1228.96,
        "duration": 5.58
    },
    {
        "text": "You will basically compute gradients of gradients.",
        "start": 1234.54,
        "duration": 1.77
    },
    {
        "text": "It's second partial derivative,",
        "start": 1236.31,
        "duration": 2.64
    },
    {
        "text": "which is how you compute the Hessian,",
        "start": 1238.95,
        "duration": 3.315
    },
    {
        "text": "the whole Jacobian matrix,",
        "start": 1242.265,
        "duration": 3.41
    },
    {
        "text": "people doing research on second-order optimization methods",
        "start": 1245.675,
        "duration": 3.855
    },
    {
        "text": "or we can make many problems perform better",
        "start": 1249.53,
        "duration": 3.91
    },
    {
        "text": "than first-order optimization method on.",
        "start": 1253.44,
        "duration": 2.04
    },
    {
        "text": "There's very natural way of doing that.",
        "start": 1255.48,
        "duration": 2.64
    },
    {
        "text": "There are examples of how",
        "start": 1258.12,
        "duration": 2.49
    },
    {
        "text": "modularity bring stoke natural user experience.",
        "start": 1260.61,
        "duration": 4.395
    },
    {
        "text": ">> That's really cool and I'll tell you,",
        "start": 1265.005,
        "duration": 2.235
    },
    {
        "text": "it does my heart good and it",
        "start": 1267.24,
        "duration": 2.16
    },
    {
        "text": "pains me because when I was in grad school,",
        "start": 1269.4,
        "duration": 1.77
    },
    {
        "text": "I actually had to calculate the Hessian for",
        "start": 1271.17,
        "duration": 1.95
    },
    {
        "text": "the problem I was doing and I couldn't do it.",
        "start": 1273.12,
        "duration": 3.675
    },
    {
        "text": "I had to do some numerical approximation thing",
        "start": 1276.795,
        "duration": 3.675
    },
    {
        "text": "and I wrote it in five different languages or whatever.",
        "start": 1280.47,
        "duration": 2.79
    },
    {
        "text": "It was just not very cool.",
        "start": 1283.26,
        "duration": 1.8
    },
    {
        "text": "I didn't know that you could actually",
        "start": 1285.06,
        "duration": 1.56
    },
    {
        "text": "do higher-order derivatives in there,",
        "start": 1286.62,
        "duration": 1.68
    },
    {
        "text": "which is super good,",
        "start": 1288.3,
        "duration": 1.5
    },
    {
        "text": "especially if you're trying to do different kinds of",
        "start": 1289.8,
        "duration": 2.4
    },
    {
        "text": "optimizations that actually tells you",
        "start": 1292.2,
        "duration": 1.8
    },
    {
        "text": "which direction to go on, which is cool.",
        "start": 1294.0,
        "duration": 2.295
    },
    {
        "text": ">> Yeah, exactly. Also, you can do it directly this pieces a",
        "start": 1296.295,
        "duration": 4.005
    },
    {
        "text": "little bit and actually edit higher level Autograd APIs.",
        "start": 1300.3,
        "duration": 4.035
    },
    {
        "text": "More optimized implementations for computing locations.",
        "start": 1304.335,
        "duration": 3.24
    },
    {
        "text": "Secondly, because I mean, they're jointly or",
        "start": 1307.575,
        "duration": 2.79
    },
    {
        "text": "how some of the tricks how you can optimize computations there.",
        "start": 1310.365,
        "duration": 4.905
    },
    {
        "text": "You can check out some recent releases.",
        "start": 1315.27,
        "duration": 3.15
    },
    {
        "text": "There is extra stuff being edited on",
        "start": 1318.42,
        "duration": 1.68
    },
    {
        "text": "the Spark as well as one of the many directions.",
        "start": 1320.1,
        "duration": 2.76
    },
    {
        "text": ">> That's something I didn't know about. Thank you for that.",
        "start": 1322.86,
        "duration": 3.45
    },
    {
        "text": "Let's get into now.",
        "start": 1326.31,
        "duration": 1.215
    },
    {
        "text": "Just a couple more things to finish up.",
        "start": 1327.525,
        "duration": 3.18
    },
    {
        "text": "It's great that we can do all these cool mathematical things.",
        "start": 1330.705,
        "duration": 4.455
    },
    {
        "text": "But when it comes to time to actually take what we've",
        "start": 1335.16,
        "duration": 2.04
    },
    {
        "text": "learned and deploy and put these things out into production.",
        "start": 1337.2,
        "duration": 3.48
    },
    {
        "text": "What does PyTorch offer?",
        "start": 1340.68,
        "duration": 1.68
    },
    {
        "text": ">> Yeah, I'm actually focused on bringing",
        "start": 1342.36,
        "duration": 2.94
    },
    {
        "text": "this research to production is a core part of the project.",
        "start": 1345.3,
        "duration": 4.005
    },
    {
        "text": "In a sense because Machine Learning field",
        "start": 1349.305,
        "duration": 2.025
    },
    {
        "text": "is evolving really quickly,",
        "start": 1351.33,
        "duration": 2.31
    },
    {
        "text": "and deep learning in particular.",
        "start": 1353.64,
        "duration": 1.74
    },
    {
        "text": "Even from used cases,",
        "start": 1355.38,
        "duration": 3.84
    },
    {
        "text": "which we see at Facebook or other companies,",
        "start": 1359.22,
        "duration": 2.13
    },
    {
        "text": "frequently you take the model which I cannot",
        "start": 1361.35,
        "duration": 3.12
    },
    {
        "text": "just got published and",
        "start": 1364.47,
        "duration": 1.5
    },
    {
        "text": "people want to put it in production application",
        "start": 1365.97,
        "duration": 1.725
    },
    {
        "text": "in weeks or months later.",
        "start": 1367.695,
        "duration": 3.57
    },
    {
        "text": "How electric supply chain is,",
        "start": 1371.265,
        "duration": 1.875
    },
    {
        "text": "simply avoiding rewriting the whole thing",
        "start": 1373.14,
        "duration": 2.7
    },
    {
        "text": "from scratch is really important.",
        "start": 1375.84,
        "duration": 1.755
    },
    {
        "text": "The developers in the past few years focus on,",
        "start": 1377.595,
        "duration": 2.295
    },
    {
        "text": "in a similar philosophy,",
        "start": 1379.89,
        "duration": 1.635
    },
    {
        "text": "how to give you tools for packaging and deploy new models in",
        "start": 1381.525,
        "duration": 3.645
    },
    {
        "text": "optimizing for high-performance in",
        "start": 1385.17,
        "duration": 1.74
    },
    {
        "text": "France became a core part of the project.",
        "start": 1386.91,
        "duration": 2.805
    },
    {
        "text": "Some other version is make sense of",
        "start": 1389.715,
        "duration": 1.995
    },
    {
        "text": "the decompose into several pieces.",
        "start": 1391.71,
        "duration": 2.955
    },
    {
        "text": "What does Deploy Machine Learning Models mean?",
        "start": 1394.665,
        "duration": 2.535
    },
    {
        "text": "They're different. It's pretty wide field.",
        "start": 1397.2,
        "duration": 3.45
    },
    {
        "text": "There are different kinds of settings",
        "start": 1400.65,
        "duration": 1.23
    },
    {
        "text": "and regimes where you might deploy.",
        "start": 1401.88,
        "duration": 1.83
    },
    {
        "text": "If you're trying to deploy something on server-side in the Cloud,",
        "start": 1403.71,
        "duration": 4.275
    },
    {
        "text": "then frequently even just running",
        "start": 1407.985,
        "duration": 2.52
    },
    {
        "text": "your Python program that directly taking your model class and say,",
        "start": 1410.505,
        "duration": 4.86
    },
    {
        "text": "hey, this is my part",
        "start": 1415.365,
        "duration": 3.525
    },
    {
        "text": "of the model we share onto the playful inference.",
        "start": 1418.89,
        "duration": 1.575
    },
    {
        "text": "This is my train checkpoint.",
        "start": 1420.465,
        "duration": 1.485
    },
    {
        "text": "This, whatever we are going to just take and",
        "start": 1421.95,
        "duration": 1.485
    },
    {
        "text": "put in some server somewhere.",
        "start": 1423.435,
        "duration": 3.15
    },
    {
        "text": "There is Project Management Ecosystem called the third CRF,",
        "start": 1426.585,
        "duration": 3.105
    },
    {
        "text": "which would actually build with partners",
        "start": 1429.69,
        "duration": 2.37
    },
    {
        "text": "from Amazon and Microsoft,",
        "start": 1432.06,
        "duration": 3.36
    },
    {
        "text": "MIT gives you the ability to",
        "start": 1435.42,
        "duration": 3.555
    },
    {
        "text": "easily package the model and attach rest API to it.",
        "start": 1438.975,
        "duration": 3.78
    },
    {
        "text": "You can just deploy",
        "start": 1442.755,
        "duration": 1.095
    },
    {
        "text": "it with few commands and start sending requests.",
        "start": 1443.85,
        "duration": 4.035
    },
    {
        "text": "You can do it. By default you can do",
        "start": 1447.885,
        "duration": 2.115
    },
    {
        "text": "it basically just like Python.",
        "start": 1450.0,
        "duration": 1.77
    },
    {
        "text": "Implementation as a model.",
        "start": 1451.77,
        "duration": 2.01
    },
    {
        "text": "In many cases, it might be undesirable to",
        "start": 1453.78,
        "duration": 3.66
    },
    {
        "text": "actually run Python directly.",
        "start": 1457.44,
        "duration": 4.15
    },
    {
        "text": "I don't server-side because high load",
        "start": 1461.63,
        "duration": 2.95
    },
    {
        "text": "and polarization problems of Python, global interpreter lock.",
        "start": 1464.58,
        "duration": 4.14
    },
    {
        "text": "Or especially if you're trying to deploy for",
        "start": 1468.72,
        "duration": 1.92
    },
    {
        "text": "embedded devices for like on mobile phones for example.",
        "start": 1470.64,
        "duration": 3.705
    },
    {
        "text": "Writing Python interpreter is not good.",
        "start": 1474.345,
        "duration": 2.88
    },
    {
        "text": "They actually have some component of",
        "start": 1477.225,
        "duration": 2.625
    },
    {
        "text": "the PyTorch ecosystem called Torchscript, which is,",
        "start": 1479.85,
        "duration": 2.58
    },
    {
        "text": "you can think of it as an implementation",
        "start": 1482.43,
        "duration": 2.46
    },
    {
        "text": "of subset of Python language,",
        "start": 1484.89,
        "duration": 1.68
    },
    {
        "text": "which is really like tailored to",
        "start": 1486.57,
        "duration": 2.04
    },
    {
        "text": "the experience a writing models in PyTorch.",
        "start": 1488.61,
        "duration": 4.26
    },
    {
        "text": "Select your typical model.",
        "start": 1492.87,
        "duration": 1.35
    },
    {
        "text": "You can basically train it in",
        "start": 1494.22,
        "duration": 1.83
    },
    {
        "text": "full Python this PyTorch how you would do.",
        "start": 1496.05,
        "duration": 2.73
    },
    {
        "text": "Then take just the model,",
        "start": 1498.78,
        "duration": 2.355
    },
    {
        "text": "say Torchscript on it and what it will do it",
        "start": 1501.135,
        "duration": 3.435
    },
    {
        "text": "basically go look at your NN model.",
        "start": 1504.57,
        "duration": 3.03
    },
    {
        "text": "Go find all the pieces of code which is necessary to run it.",
        "start": 1507.6,
        "duration": 4.575
    },
    {
        "text": "Basically, package it in one file,",
        "start": 1512.175,
        "duration": 2.67
    },
    {
        "text": "which we can run without the Python as our own interpreter.",
        "start": 1514.845,
        "duration": 3.915
    },
    {
        "text": "But you'll literally go and parch the Python source code.",
        "start": 1518.76,
        "duration": 3.285
    },
    {
        "text": "Like in for some reasonable subset of it,",
        "start": 1522.045,
        "duration": 3.33
    },
    {
        "text": "very dynamic stuff will not work,",
        "start": 1525.375,
        "duration": 1.605
    },
    {
        "text": "but majority of stuff which people have in their models will work.",
        "start": 1526.98,
        "duration": 2.76
    },
    {
        "text": "Basically that piece of [inaudible] deploy it now,",
        "start": 1529.74,
        "duration": 4.065
    },
    {
        "text": "embedded in some SQL specification without Python being in it.",
        "start": 1533.805,
        "duration": 4.26
    },
    {
        "text": "Or it can be brought to mobile device and whatever.",
        "start": 1538.065,
        "duration": 3.27
    },
    {
        "text": "That's the packaging and deployment part of story.",
        "start": 1541.335,
        "duration": 4.41
    },
    {
        "text": "I guess second big ingredient for",
        "start": 1545.745,
        "duration": 4.545
    },
    {
        "text": "efficient deployment of production is",
        "start": 1550.29,
        "duration": 2.79
    },
    {
        "text": "basically how to optimize performance of models.",
        "start": 1553.08,
        "duration": 4.155
    },
    {
        "text": "If they're running, usually, an inference on a huge scale,",
        "start": 1557.235,
        "duration": 3.69
    },
    {
        "text": "you'd probably don't want to waste.",
        "start": 1560.925,
        "duration": 1.83
    },
    {
        "text": "They're usually pretty strict",
        "start": 1562.755,
        "duration": 2.46
    },
    {
        "text": "on those capacity requirements or",
        "start": 1565.215,
        "duration": 1.455
    },
    {
        "text": "latency requirements, stuff like that.",
        "start": 1566.67,
        "duration": 1.425
    },
    {
        "text": "That's where set of techniques utilized and been up,",
        "start": 1568.095,
        "duration": 2.895
    },
    {
        "text": "but the neural networks, something",
        "start": 1570.99,
        "duration": 1.14
    },
    {
        "text": "like quantization, pruning, specification.",
        "start": 1572.13,
        "duration": 3.165
    },
    {
        "text": "Maybe just like combination like go to low precision or",
        "start": 1575.295,
        "duration": 4.365
    },
    {
        "text": "actually doing some things",
        "start": 1579.66,
        "duration": 2.34
    },
    {
        "text": "like techniques that preserves the accuracy,",
        "start": 1582.0,
        "duration": 2.325
    },
    {
        "text": "or maybe just doing",
        "start": 1584.325,
        "duration": 2.145
    },
    {
        "text": "some local optimization on some models so",
        "start": 1586.47,
        "duration": 1.86
    },
    {
        "text": "you can replace some models or modules,",
        "start": 1588.33,
        "duration": 2.07
    },
    {
        "text": "like more efficient implementations.",
        "start": 1590.4,
        "duration": 1.89
    },
    {
        "text": "PyTorch by itself, actually gives you this set",
        "start": 1592.29,
        "duration": 3.33
    },
    {
        "text": "of ingredients to optimize the models for efficient inference.",
        "start": 1595.62,
        "duration": 4.53
    },
    {
        "text": "You have combination libraries",
        "start": 1600.15,
        "duration": 3.48
    },
    {
        "text": "and workloads as a part of core PyTorch.",
        "start": 1603.63,
        "duration": 2.82
    },
    {
        "text": "Basically, there are some optimized",
        "start": 1606.45,
        "duration": 2.7
    },
    {
        "text": "implementations operators,",
        "start": 1609.15,
        "duration": 2.22
    },
    {
        "text": "you can build your own.",
        "start": 1611.37,
        "duration": 2.7
    },
    {
        "text": "That's why they also deploy into",
        "start": 1614.07,
        "duration": 2.775
    },
    {
        "text": "wide range of devices who comes in because you'll",
        "start": 1616.845,
        "duration": 1.935
    },
    {
        "text": "usually train on GPUs or maybe some Cloud accelerators.",
        "start": 1618.78,
        "duration": 6.15
    },
    {
        "text": "But in terms of deployment,",
        "start": 1624.93,
        "duration": 1.2
    },
    {
        "text": "there's a wide range of devices you might deploy.",
        "start": 1626.13,
        "duration": 2.79
    },
    {
        "text": "There's server-side CPUs and GPUs.",
        "start": 1628.92,
        "duration": 2.37
    },
    {
        "text": "There is all kinds of server-side accelerators these people have.",
        "start": 1631.29,
        "duration": 3.63
    },
    {
        "text": "There are all the kind of Zoof,",
        "start": 1634.92,
        "duration": 2.73
    },
    {
        "text": "different mobile devices and DSPs and stuff like that.",
        "start": 1637.65,
        "duration": 2.55
    },
    {
        "text": "Our approach there is that,",
        "start": 1640.2,
        "duration": 1.755
    },
    {
        "text": "for the most popular platforms,",
        "start": 1641.955,
        "duration": 1.545
    },
    {
        "text": "we try to provide implementations to",
        "start": 1643.5,
        "duration": 1.815
    },
    {
        "text": "all these tensor operations on them in the core PyTorch,",
        "start": 1645.315,
        "duration": 3.15
    },
    {
        "text": "like how you do the CPU, GPU.",
        "start": 1648.465,
        "duration": 1.605
    },
    {
        "text": "We're now working on bigger coverage for mobile CPU like",
        "start": 1650.07,
        "duration": 4.23
    },
    {
        "text": "ARM CPUs and something like mobile GPUs,",
        "start": 1654.3,
        "duration": 5.01
    },
    {
        "text": "like open jailbreak-ins. But then there's",
        "start": 1659.31,
        "duration": 4.83
    },
    {
        "text": "this long tail of different devices.",
        "start": 1664.14,
        "duration": 5.37
    },
    {
        "text": "It's also important to think how",
        "start": 1669.51,
        "duration": 1.86
    },
    {
        "text": "you can take your model deployed to that.",
        "start": 1671.37,
        "duration": 2.175
    },
    {
        "text": "Again, depending on how restricted your devices",
        "start": 1673.545,
        "duration": 2.745
    },
    {
        "text": "computationally, it might not be very automatic process,",
        "start": 1676.29,
        "duration": 3.63
    },
    {
        "text": "but it's still important to give you,",
        "start": 1679.92,
        "duration": 1.875
    },
    {
        "text": "as a developer, tools to make it smoother,",
        "start": 1681.795,
        "duration": 2.085
    },
    {
        "text": "so there is a number of technologies in better",
        "start": 1683.88,
        "duration": 1.95
    },
    {
        "text": "to support that tool, ranging from,",
        "start": 1685.83,
        "duration": 3.135
    },
    {
        "text": "for example, ONNX export,",
        "start": 1688.965,
        "duration": 2.265
    },
    {
        "text": "so you can export it to the format which",
        "start": 1691.23,
        "duration": 2.37
    },
    {
        "text": "a lot of these runtimes can import for inference.",
        "start": 1693.6,
        "duration": 2.91
    },
    {
        "text": "There is also a direct integration points in",
        "start": 1696.51,
        "duration": 2.46
    },
    {
        "text": "PyTorch for integrating different backends.",
        "start": 1698.97,
        "duration": 2.49
    },
    {
        "text": "If you want to run optimization on some part of your module,",
        "start": 1701.46,
        "duration": 4.11
    },
    {
        "text": "delegate to some runtime something like NVIDIA",
        "start": 1705.57,
        "duration": 2.43
    },
    {
        "text": "[inaudible] or like ONNX from Microsoft,",
        "start": 1708.0,
        "duration": 2.94
    },
    {
        "text": "you can even basically can,",
        "start": 1710.94,
        "duration": 2.115
    },
    {
        "text": "by taking part of your model and exporting it to that form.",
        "start": 1713.055,
        "duration": 4.38
    },
    {
        "text": ">> That's a cool bit. It seems",
        "start": 1717.435,
        "duration": 4.545
    },
    {
        "text": "you-all have thought a lot about the framework itself,",
        "start": 1721.98,
        "duration": 2.58
    },
    {
        "text": "but you've also thought a lot",
        "start": 1724.56,
        "duration": 1.5
    },
    {
        "text": "about how do we get these things out in an efficient way.",
        "start": 1726.06,
        "duration": 3.615
    },
    {
        "text": "You can just do it or you can optimize it with",
        "start": 1729.675,
        "duration": 2.865
    },
    {
        "text": "Torch script and embed it into C++ style application.",
        "start": 1732.54,
        "duration": 3.93
    },
    {
        "text": "I'm a fan of ONNX, the ONNX runtime,",
        "start": 1736.47,
        "duration": 2.94
    },
    {
        "text": "as well as the ONNX format for",
        "start": 1739.41,
        "duration": 1.95
    },
    {
        "text": "getting multiple folks to be able to use it.",
        "start": 1741.36,
        "duration": 2.205
    },
    {
        "text": "So just to finish up,",
        "start": 1743.565,
        "duration": 1.38
    },
    {
        "text": "any last things that make PyTorch special",
        "start": 1744.945,
        "duration": 3.075
    },
    {
        "text": "and then where can people go to find out more?",
        "start": 1748.02,
        "duration": 2.88
    },
    {
        "text": ">> Yeah. We talked a lot about technical merits of PyTorch,",
        "start": 1750.9,
        "duration": 5.4
    },
    {
        "text": "especially on the core framework.",
        "start": 1756.3,
        "duration": 2.355
    },
    {
        "text": "Probably, the most important reason why",
        "start": 1758.655,
        "duration": 3.12
    },
    {
        "text": "PyTorch is a developer platform,",
        "start": 1761.775,
        "duration": 3.33
    },
    {
        "text": "is so popular, it's friendly,",
        "start": 1765.105,
        "duration": 2.475
    },
    {
        "text": "it's actually not technical but more on the community side.",
        "start": 1767.58,
        "duration": 3.57
    },
    {
        "text": "From basic had an amazing community and we can relate,",
        "start": 1771.15,
        "duration": 4.755
    },
    {
        "text": "spend effort to nurture it and support different members.",
        "start": 1775.905,
        "duration": 5.025
    },
    {
        "text": "Core PyTorch contributors basically from Day 1",
        "start": 1780.93,
        "duration": 3.57
    },
    {
        "text": "includes multiple industry companies",
        "start": 1784.5,
        "duration": 3.645
    },
    {
        "text": "like university labs and just",
        "start": 1788.145,
        "duration": 1.395
    },
    {
        "text": "like individual open-source developers.",
        "start": 1789.54,
        "duration": 2.445
    },
    {
        "text": "Those contributions range from",
        "start": 1791.985,
        "duration": 2.415
    },
    {
        "text": "contributing to the core framework to",
        "start": 1794.4,
        "duration": 2.1
    },
    {
        "text": "just doing awesome work",
        "start": 1796.5,
        "duration": 1.56
    },
    {
        "text": "in supporting and answering questions in PyTorch forums,",
        "start": 1798.06,
        "duration": 3.15
    },
    {
        "text": "which a lot of people say",
        "start": 1801.21,
        "duration": 1.65
    },
    {
        "text": "that is one of the main reasons why they love PyTorch.",
        "start": 1802.86,
        "duration": 2.205
    },
    {
        "text": "It's actually very responsive forums,",
        "start": 1805.065,
        "duration": 3.15
    },
    {
        "text": "and obviously, this huge ecosystem",
        "start": 1808.215,
        "duration": 2.265
    },
    {
        "text": "of projects built on top of PyTorch.",
        "start": 1810.48,
        "duration": 2.4
    },
    {
        "text": "If you pretty much look at any subfield of deep learning,",
        "start": 1812.88,
        "duration": 5.22
    },
    {
        "text": "just go search that keyword, PyTorch.",
        "start": 1818.1,
        "duration": 4.185
    },
    {
        "text": "Google it to find a bunch of GitHub repos,",
        "start": 1822.285,
        "duration": 3.195
    },
    {
        "text": "which are all awesome projects,",
        "start": 1825.48,
        "duration": 2.43
    },
    {
        "text": "work we spent time building up on PyTorch,",
        "start": 1827.91,
        "duration": 2.085
    },
    {
        "text": "but implement special layers of functionalities, algorithms,",
        "start": 1829.995,
        "duration": 3.135
    },
    {
        "text": "and models for that particular domain.",
        "start": 1833.13,
        "duration": 3.375
    },
    {
        "text": "Being it something more standard like computer vision or",
        "start": 1836.505,
        "duration": 5.175
    },
    {
        "text": "maybe more obscure applications like biology and stuff like that.",
        "start": 1841.68,
        "duration": 6.93
    },
    {
        "text": "That ranges both on type of domains and type",
        "start": 1848.61,
        "duration": 3.48
    },
    {
        "text": "of emergent research areas being it",
        "start": 1852.09,
        "duration": 3.84
    },
    {
        "text": "like privacy preserving Machine Learning",
        "start": 1855.93,
        "duration": 4.05
    },
    {
        "text": "or being it like different optimization methods",
        "start": 1859.98,
        "duration": 2.64
    },
    {
        "text": "or being applications in different domains.",
        "start": 1862.62,
        "duration": 1.975
    },
    {
        "text": "That's really the most amazing thing to see how",
        "start": 1864.595,
        "duration": 4.855
    },
    {
        "text": "empowering community allow us to basically create",
        "start": 1869.45,
        "duration": 6.42
    },
    {
        "text": "this ecosystem, where you can find",
        "start": 1875.87,
        "duration": 3.475
    },
    {
        "text": "many different projects and",
        "start": 1879.345,
        "duration": 2.25
    },
    {
        "text": "many different tools for pretty much any job.",
        "start": 1881.595,
        "duration": 2.79
    },
    {
        "text": "A lot of ideas also start in this independent projects",
        "start": 1884.385,
        "duration": 4.935
    },
    {
        "text": "and become also like share the cross manual processors",
        "start": 1889.32,
        "duration": 5.52
    },
    {
        "text": "like very intelligent people to contribute those ideas and models,",
        "start": 1894.84,
        "duration": 3.45
    },
    {
        "text": "whatever back to PyTorch core so they",
        "start": 1898.29,
        "duration": 2.22
    },
    {
        "text": "can benefit even broader set of use cases.",
        "start": 1900.51,
        "duration": 2.97
    },
    {
        "text": "That's very organic and natural process.",
        "start": 1903.48,
        "duration": 2.415
    },
    {
        "text": "Yeah, I guess that probably would be the main,",
        "start": 1905.895,
        "duration": 4.095
    },
    {
        "text": "not just technical factor of PyTorch success.",
        "start": 1909.99,
        "duration": 4.455
    },
    {
        "text": "You asked where to learn more about PyTorch,",
        "start": 1914.445,
        "duration": 3.795
    },
    {
        "text": "basically go to pytorch.org.",
        "start": 1918.24,
        "duration": 1.845
    },
    {
        "text": "There are tutorials, there are documentation, there is, basically,",
        "start": 1920.085,
        "duration": 4.995
    },
    {
        "text": "a set of pointers to ecosystem projects,",
        "start": 1925.08,
        "duration": 2.77
    },
    {
        "text": "then instructions on how to download and start play and visit,",
        "start": 1928.64,
        "duration": 4.945
    },
    {
        "text": "or finding the project which",
        "start": 1933.585,
        "duration": 1.89
    },
    {
        "text": "made you fit in domain you are working in.",
        "start": 1935.475,
        "duration": 3.06
    },
    {
        "text": "If you're new to deep learning,",
        "start": 1938.535,
        "duration": 2.025
    },
    {
        "text": "actually, one of your previous speakers,",
        "start": 1940.56,
        "duration": 3.03
    },
    {
        "text": "Jeremy Howard, was like, \"How's the TA?\"",
        "start": 1943.59,
        "duration": 2.865
    },
    {
        "text": "Basically, Jeremy and other folks build",
        "start": 1946.455,
        "duration": 2.655
    },
    {
        "text": "this amazing library as an intro to deep learning as well.",
        "start": 1949.11,
        "duration": 3.225
    },
    {
        "text": "So actually, it was PyTorch and Jeremy is involved.",
        "start": 1952.335,
        "duration": 2.34
    },
    {
        "text": "That's also an example of ecosystem, community projects.",
        "start": 1954.675,
        "duration": 6.645
    },
    {
        "text": "That's probably the stuff,",
        "start": 1961.32,
        "duration": 2.07
    },
    {
        "text": "and as you start using PyTorch you also go to",
        "start": 1963.39,
        "duration": 2.97
    },
    {
        "text": "forums and also Lincoln's website.",
        "start": 1966.36,
        "duration": 3.75
    },
    {
        "text": "Ask your question, and usually people will be happy to",
        "start": 1970.11,
        "duration": 2.685
    },
    {
        "text": "help you out if it gets dark or point you in some direction.",
        "start": 1972.795,
        "duration": 3.285
    },
    {
        "text": "Of course, if you want the contribute to PyTorch itself,",
        "start": 1976.08,
        "duration": 3.72
    },
    {
        "text": "all our development happens on GitHub,",
        "start": 1979.8,
        "duration": 2.1
    },
    {
        "text": "so go grab some simple issue,",
        "start": 1981.9,
        "duration": 2.91
    },
    {
        "text": "maybe send DPR and [inaudible]",
        "start": 1984.81,
        "duration": 5.4
    },
    {
        "text": ">> Well, I'm a huge fan, Dmytro.",
        "start": 1990.21,
        "duration": 1.845
    },
    {
        "text": "Thank you so much for spending some time with us.",
        "start": 1992.055,
        "duration": 2.145
    },
    {
        "text": "Again, if you want to learn more about PyTorch,",
        "start": 1994.2,
        "duration": 2.19
    },
    {
        "text": "go to PyTorch.org,",
        "start": 1996.39,
        "duration": 1.05
    },
    {
        "text": "there's some pretty cool tutorials there,",
        "start": 1997.44,
        "duration": 1.89
    },
    {
        "text": "that I'm excited about.",
        "start": 1999.33,
        "duration": 1.26
    },
    {
        "text": "We've been learning all about PyTorch,",
        "start": 2000.59,
        "duration": 2.31
    },
    {
        "text": "not just from a technical's perspective,",
        "start": 2002.9,
        "duration": 1.86
    },
    {
        "text": "but from a community perspective.",
        "start": 2004.76,
        "duration": 1.305
    },
    {
        "text": "Thanks so much for being with us, Dmytro.",
        "start": 2006.065,
        "duration": 2.055
    },
    {
        "text": ">> Thank you so much for having me. It was fun.",
        "start": 2008.12,
        "duration": 3.075
    },
    {
        "text": ">> Awesome. Again, thank you so much for watching.",
        "start": 2011.195,
        "duration": 2.235
    },
    {
        "text": "This has been another episode of",
        "start": 2013.43,
        "duration": 1.05
    },
    {
        "text": "the AI Show where we learned all about PyTorch.",
        "start": 2014.48,
        "duration": 2.64
    },
    {
        "text": "We're going to have to get you on again, my friend.",
        "start": 2017.12,
        "duration": 2.19
    },
    {
        "text": "So watch for Dmytro, hopefully,",
        "start": 2019.31,
        "duration": 2.73
    },
    {
        "text": "in later episodes to show us some code or some cool sample.",
        "start": 2022.04,
        "duration": 3.54
    },
    {
        "text": "Again, thanks for watching, and",
        "start": 2025.58,
        "duration": 1.2
    },
    {
        "text": "hopefully, we'll see you next time. Take care.",
        "start": 2026.78,
        "duration": 1.29
    },
    {
        "text": "[MUSIC]",
        "start": 2028.07,
        "duration": 14.93
    }
]