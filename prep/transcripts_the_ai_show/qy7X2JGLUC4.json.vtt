[
    {
        "text": ">> Hi. My name is Faith Xu and I'm",
        "start": 0.0,
        "duration": 2.79
    },
    {
        "text": "a Program Manager on the ML Platform Framework's team.",
        "start": 2.79,
        "duration": 3.285
    },
    {
        "text": "It's hard to ignore the growing impact of",
        "start": 6.075,
        "duration": 2.22
    },
    {
        "text": "AI and Machine Learning as it expands",
        "start": 8.295,
        "duration": 2.145
    },
    {
        "text": "into every industry from",
        "start": 10.44,
        "duration": 1.395
    },
    {
        "text": "automobile and healthcare to arts and games.",
        "start": 11.835,
        "duration": 2.91
    },
    {
        "text": "For sustainable and fair growth in this hot spot of innovation,",
        "start": 14.745,
        "duration": 3.615
    },
    {
        "text": "it's critical to have an open ecosystem",
        "start": 18.36,
        "duration": 2.34
    },
    {
        "text": "to support flexibility in development.",
        "start": 20.7,
        "duration": 2.85
    },
    {
        "text": "As champions of Open and Interoperable AI,",
        "start": 23.55,
        "duration": 3.435
    },
    {
        "text": "we at Microsoft have invested in",
        "start": 26.985,
        "duration": 2.19
    },
    {
        "text": "the Open Neural Network Exchange or",
        "start": 29.175,
        "duration": 2.175
    },
    {
        "text": "ONNX initiative and are proud to",
        "start": 31.35,
        "duration": 2.1
    },
    {
        "text": "share that we have open source ONNX Runtime,",
        "start": 33.45,
        "duration": 2.714
    },
    {
        "text": "a scoring engine for ONNX format models.",
        "start": 36.164,
        "duration": 2.836
    },
    {
        "text": "ONNX Runtime is now available on GitHub.",
        "start": 39.0,
        "duration": 3.405
    },
    {
        "text": "ONNX is an open format for representing",
        "start": 42.405,
        "duration": 3.065
    },
    {
        "text": "traditional and deep learning ML models",
        "start": 45.47,
        "duration": 2.295
    },
    {
        "text": "supported by a community of over 20 leading companies.",
        "start": 47.765,
        "duration": 3.57
    },
    {
        "text": "ONNX Runtime is a high-performance engine",
        "start": 51.335,
        "duration": 2.865
    },
    {
        "text": "for running these models,",
        "start": 54.2,
        "duration": 1.47
    },
    {
        "text": "fully compliant with operators",
        "start": 55.67,
        "duration": 1.62
    },
    {
        "text": "defined in the ONNX spec and works with",
        "start": 57.29,
        "duration": 2.37
    },
    {
        "text": "both CPU and GPU across",
        "start": 59.66,
        "duration": 2.16
    },
    {
        "text": "a growing number of platforms including Linux, Windows, and Mac.",
        "start": 61.82,
        "duration": 3.485
    },
    {
        "text": "It is designed with an extensible architecture to support plug-in",
        "start": 65.305,
        "duration": 3.525
    },
    {
        "text": "hardware accelerators allowing it to",
        "start": 68.83,
        "duration": 2.14
    },
    {
        "text": "stay up-to-date with the latest innovations.",
        "start": 70.97,
        "duration": 2.41
    },
    {
        "text": "Companies such as Nvidia and Intel are actively",
        "start": 73.38,
        "duration": 3.05
    },
    {
        "text": "contributing by integrating custom accelerators into ONNX Runtime.",
        "start": 76.43,
        "duration": 3.915
    },
    {
        "text": "You can get a pre-trained model from the ONNX model zoo or",
        "start": 80.345,
        "duration": 3.165
    },
    {
        "text": "training convert it from",
        "start": 83.51,
        "duration": 1.035
    },
    {
        "text": "any popular frameworks such as TensorFlow,",
        "start": 84.545,
        "duration": 2.4
    },
    {
        "text": "Keras, Scikit-Learn, PyTorch, Core ML, and more.",
        "start": 86.945,
        "duration": 4.515
    },
    {
        "text": "ONNX Runtime is simple to use.",
        "start": 91.46,
        "duration": 2.81
    },
    {
        "text": "At a high level, once you have a model,",
        "start": 94.27,
        "duration": 2.2
    },
    {
        "text": "you can create a session,",
        "start": 96.47,
        "duration": 1.275
    },
    {
        "text": "set the input data, and score the model.",
        "start": 97.745,
        "duration": 2.66
    },
    {
        "text": "You can integrate ONNX Runtime into your code",
        "start": 100.405,
        "duration": 2.575
    },
    {
        "text": "directly from source or precompiled binaries",
        "start": 102.98,
        "duration": 2.52
    },
    {
        "text": "but one simple popular way to operationalize this is through",
        "start": 105.5,
        "duration": 3.3
    },
    {
        "text": "Azure ML to deploy a service for your application to call.",
        "start": 108.8,
        "duration": 3.705
    },
    {
        "text": "Let's see this end-to-end in action.",
        "start": 112.505,
        "duration": 3.065
    },
    {
        "text": "Here, I am opening up",
        "start": 115.57,
        "duration": 2.18
    },
    {
        "text": "a Jupyter notebook which we will use to convert,",
        "start": 117.75,
        "duration": 2.36
    },
    {
        "text": "load, and run the tiny yellow model using ONNX Runtime.",
        "start": 120.11,
        "duration": 3.375
    },
    {
        "text": "This particular model is originally published in",
        "start": 123.485,
        "duration": 2.505
    },
    {
        "text": "Core ML and it's used for real-time object detection.",
        "start": 125.99,
        "duration": 3.41
    },
    {
        "text": "First, we will download the model and convert it into ONNX.",
        "start": 129.4,
        "duration": 4.48
    },
    {
        "text": "We're using the open source on XML tools and for ML tools to load,",
        "start": 133.88,
        "duration": 4.74
    },
    {
        "text": "convert, and save the model into ONNX format.",
        "start": 138.62,
        "duration": 3.79
    },
    {
        "text": "To deploy this model as a service in Azure,",
        "start": 142.49,
        "duration": 3.135
    },
    {
        "text": "we'll use the Azure ML SDK.",
        "start": 145.625,
        "duration": 2.965
    },
    {
        "text": "After creating a workspace,",
        "start": 149.5,
        "duration": 2.605
    },
    {
        "text": "we can register the model we just converted for use.",
        "start": 152.105,
        "duration": 3.655
    },
    {
        "text": "Using Azure ML, we will create a scoring file",
        "start": 157.3,
        "duration": 3.715
    },
    {
        "text": "that contains the instructions to execute the runtime.",
        "start": 161.015,
        "duration": 3.225
    },
    {
        "text": "In this case, we will create a session,",
        "start": 164.24,
        "duration": 3.48
    },
    {
        "text": "format the input and output data formats,",
        "start": 167.72,
        "duration": 3.89
    },
    {
        "text": "and finally, we will run this session with the given inputs.",
        "start": 171.61,
        "duration": 5.28
    },
    {
        "text": "Next, we can just build and deploy an image with",
        "start": 177.47,
        "duration": 3.81
    },
    {
        "text": "this model and scoring file using Azure Container Service.",
        "start": 181.28,
        "duration": 4.33
    },
    {
        "text": "It can also be productionized using",
        "start": 186.17,
        "duration": 2.75
    },
    {
        "text": "Azure Kubernetes service for production level traffic.",
        "start": 188.92,
        "duration": 3.31
    },
    {
        "text": "Since this can take a few minutes,",
        "start": 192.23,
        "duration": 2.085
    },
    {
        "text": "we'll just use the service I deployed",
        "start": 194.315,
        "duration": 1.5
    },
    {
        "text": "earlier using the same configuration.",
        "start": 195.815,
        "duration": 2.52
    },
    {
        "text": "Let's put this to use in an application.",
        "start": 198.335,
        "duration": 3.295
    },
    {
        "text": "Here, I'm just pasting in the URL which will run",
        "start": 205.3,
        "duration": 3.82
    },
    {
        "text": "the model and uploading an image I took",
        "start": 209.12,
        "duration": 4.035
    },
    {
        "text": "last weekend and this image you will see",
        "start": 213.155,
        "duration": 3.585
    },
    {
        "text": "that here the car is identified",
        "start": 216.74,
        "duration": 1.635
    },
    {
        "text": "as well as the bike and the person.",
        "start": 218.375,
        "duration": 2.325
    },
    {
        "text": "We can also see this using a live video.",
        "start": 220.7,
        "duration": 3.61
    },
    {
        "text": "Here, you'll see that I'm identified as a person and",
        "start": 224.6,
        "duration": 4.5
    },
    {
        "text": "this bottle of hot sauce is identified as a bottle.",
        "start": 229.1,
        "duration": 5.35
    },
    {
        "text": "Here at Microsoft, we are using ONNX Runtime to improve",
        "start": 235.25,
        "duration": 4.23
    },
    {
        "text": "the prediction latency and efficiency for many of",
        "start": 239.48,
        "duration": 2.64
    },
    {
        "text": "our models using core scenarios within Bing Search,",
        "start": 242.12,
        "duration": 3.03
    },
    {
        "text": "image and multimedia constant recognition,",
        "start": 245.15,
        "duration": 2.43
    },
    {
        "text": "office productivity services and more.",
        "start": 247.58,
        "duration": 2.66
    },
    {
        "text": "In online scenarios, it can decrease user perceived latency for",
        "start": 250.24,
        "duration": 3.61
    },
    {
        "text": "a better and smoother user experience",
        "start": 253.85,
        "duration": 2.07
    },
    {
        "text": "and for offline computations,",
        "start": 255.92,
        "duration": 1.58
    },
    {
        "text": "it can save on machine costs by increasing throughput.",
        "start": 257.5,
        "duration": 3.114
    },
    {
        "text": "Compared to original model scored on various frameworks,",
        "start": 260.614,
        "duration": 3.346
    },
    {
        "text": "we have seen significant latency gains in running",
        "start": 263.96,
        "duration": 2.28
    },
    {
        "text": "these models in the ONNX format using ONNX Runtime.",
        "start": 266.24,
        "duration": 3.195
    },
    {
        "text": "We encourage you to try this out and to contribute to",
        "start": 269.435,
        "duration": 3.225
    },
    {
        "text": "the continuously growing community of ONNX supporters.",
        "start": 272.66,
        "duration": 4.15
    }
]