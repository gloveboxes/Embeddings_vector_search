[
    {
        "text": ">> You're not going want to miss this episode of the AI Show.",
        "start": 0.0,
        "duration": 2.16
    },
    {
        "text": "We look at what's new in image captioning, human parody goodness.",
        "start": 2.16,
        "duration": 4.664
    },
    {
        "text": "We're going to see how it's done,",
        "start": 6.824,
        "duration": 1.546
    },
    {
        "text": "we're going to see how you can do it. Make sure you tune in.",
        "start": 8.37,
        "duration": 2.19
    },
    {
        "text": "[MUSIC]",
        "start": 10.56,
        "duration": 8.279
    },
    {
        "text": ">> Hello and welcome to this episode of the AI Show.",
        "start": 18.839,
        "duration": 2.131
    },
    {
        "text": "We're going to talk about what's new with image captioning.",
        "start": 20.97,
        "duration": 2.64
    },
    {
        "text": "I've got a special guest.",
        "start": 23.61,
        "duration": 0.87
    },
    {
        "text": "Varsha, why don't you introduce yourself,",
        "start": 24.48,
        "duration": 1.41
    },
    {
        "text": "tell us who you are and what you do.",
        "start": 25.89,
        "duration": 1.485
    },
    {
        "text": ">> Yes. I'm Varsha,",
        "start": 27.375,
        "duration": 1.935
    },
    {
        "text": "Program Manager on Azure Cognitive Services team.",
        "start": 29.31,
        "duration": 2.535
    },
    {
        "text": "I work on pre-built computer vision offerings",
        "start": 31.845,
        "duration": 2.415
    },
    {
        "text": "like object detection,",
        "start": 34.26,
        "duration": 1.2
    },
    {
        "text": "image tagging, and image captioning.",
        "start": 35.46,
        "duration": 2.73
    },
    {
        "text": ">> Fantastic. Let's start first with people if they don't",
        "start": 38.19,
        "duration": 2.87
    },
    {
        "text": "know about what Microsoft has to offer with computer vision.",
        "start": 41.06,
        "duration": 3.21
    },
    {
        "text": "Let's start there. What does Microsoft",
        "start": 44.27,
        "duration": 1.47
    },
    {
        "text": "have to offer with computer vision?",
        "start": 45.74,
        "duration": 1.73
    },
    {
        "text": ">> Sure. Let's start with computer vision service.",
        "start": 47.47,
        "duration": 2.93
    },
    {
        "text": "Cognitive services vision offerings gives you access to",
        "start": 50.4,
        "duration": 3.77
    },
    {
        "text": "advanced algorithms that process",
        "start": 54.17,
        "duration": 2.16
    },
    {
        "text": "images and returns information based on visual features,",
        "start": 56.33,
        "duration": 2.865
    },
    {
        "text": "whether it's tagging content images",
        "start": 59.195,
        "duration": 2.445
    },
    {
        "text": "or detecting objects, identifying entities,",
        "start": 61.64,
        "duration": 3.18
    },
    {
        "text": "or even actions between these entities,",
        "start": 64.82,
        "duration": 2.49
    },
    {
        "text": "essentially helping our customers extract insights from images.",
        "start": 67.31,
        "duration": 5.03
    },
    {
        "text": ">> Sorry, you were saying, and what else?",
        "start": 72.95,
        "duration": 3.055
    },
    {
        "text": ">> Image captioning is part of the computer vision service,",
        "start": 76.005,
        "duration": 2.89
    },
    {
        "text": "which is something we'll dive into today.",
        "start": 78.895,
        "duration": 2.8
    },
    {
        "text": "It's designed to let you automatically extract",
        "start": 81.695,
        "duration": 2.415
    },
    {
        "text": "content and generate human readable image captions.",
        "start": 84.11,
        "duration": 3.695
    },
    {
        "text": ">> It's like we're the same person,",
        "start": 87.805,
        "duration": 1.675
    },
    {
        "text": "I was literally going to dive in.",
        "start": 89.48,
        "duration": 1.38
    },
    {
        "text": "We're talking about image captioning specifically.",
        "start": 90.86,
        "duration": 3.375
    },
    {
        "text": "You mentioned a little bit about what it was,",
        "start": 94.235,
        "duration": 2.385
    },
    {
        "text": "but what is image captioning?",
        "start": 96.62,
        "duration": 2.295
    },
    {
        "text": "What is it that we offer? Let's start there.",
        "start": 98.915,
        "duration": 3.005
    },
    {
        "text": ">> Yeah. I'm excited to share",
        "start": 101.92,
        "duration": 2.305
    },
    {
        "text": "the recent milestone we established with image captioning.",
        "start": 104.225,
        "duration": 2.655
    },
    {
        "text": "Our research team developed a new approach to image captioning,",
        "start": 106.88,
        "duration": 3.3
    },
    {
        "text": "and this breakthrough technology has helped us achieve",
        "start": 110.18,
        "duration": 2.73
    },
    {
        "text": "human parody on nocaps benchmark dataset.",
        "start": 112.91,
        "duration": 3.045
    },
    {
        "text": "Nocaps is novel object captioning at scale.",
        "start": 115.955,
        "duration": 3.095
    },
    {
        "text": "This is roughly used as an indicator of",
        "start": 119.05,
        "duration": 1.99
    },
    {
        "text": "advancement in capturing algorithms.",
        "start": 121.04,
        "duration": 2.52
    },
    {
        "text": "The images that you're seeing on the right has been",
        "start": 123.56,
        "duration": 2.235
    },
    {
        "text": "pulled out of the nocaps test dataset,",
        "start": 125.795,
        "duration": 2.415
    },
    {
        "text": "and the caption generated is through our AI model.",
        "start": 128.21,
        "duration": 3.675
    },
    {
        "text": "Seth, to be completely honest,",
        "start": 131.885,
        "duration": 2.025
    },
    {
        "text": "it took me a while to realize that",
        "start": 133.91,
        "duration": 1.785
    },
    {
        "text": "there's a table behind the waffle cone there.",
        "start": 135.695,
        "duration": 2.445
    },
    {
        "text": "It's pretty impressive.",
        "start": 138.14,
        "duration": 1.71
    },
    {
        "text": ">> That is amazing.",
        "start": 139.85,
        "duration": 1.815
    },
    {
        "text": "Look, I'm a computer vision person.",
        "start": 141.665,
        "duration": 2.49
    },
    {
        "text": "The level of classifying images",
        "start": 144.155,
        "duration": 1.755
    },
    {
        "text": "and telling if it's a cow or a horse,",
        "start": 145.91,
        "duration": 2.28
    },
    {
        "text": "I'm pretty good at that thing.",
        "start": 148.19,
        "duration": 2.07
    },
    {
        "text": "But image captioning that's this good,",
        "start": 150.26,
        "duration": 3.295
    },
    {
        "text": "how do we do that?",
        "start": 153.555,
        "duration": 2.12
    },
    {
        "text": "Can you give us a look inside at how we're getting",
        "start": 155.675,
        "duration": 2.925
    },
    {
        "text": "this human level captioning?",
        "start": 158.6,
        "duration": 3.09
    },
    {
        "text": ">> Yeah. Let's start with the process of",
        "start": 161.69,
        "duration": 2.31
    },
    {
        "text": "image captioning and why it's such a complex challenge.",
        "start": 164.0,
        "duration": 3.135
    },
    {
        "text": "The illustration that I'm showing here shows",
        "start": 167.135,
        "duration": 2.955
    },
    {
        "text": "the translating images into",
        "start": 170.09,
        "duration": 1.77
    },
    {
        "text": "words and the complexity associated with it.",
        "start": 171.86,
        "duration": 2.475
    },
    {
        "text": "Consider for a moment what it takes to visually",
        "start": 174.335,
        "duration": 2.955
    },
    {
        "text": "identify and describe something to another person,",
        "start": 177.29,
        "duration": 2.88
    },
    {
        "text": "and what if this person does not have",
        "start": 180.17,
        "duration": 2.145
    },
    {
        "text": "access or can see the object that you're trying to describe.",
        "start": 182.315,
        "duration": 3.735
    },
    {
        "text": "Every detail that you capture through your human eyes matters.",
        "start": 186.05,
        "duration": 4.655
    },
    {
        "text": "How does the machine decide",
        "start": 190.705,
        "duration": 1.885
    },
    {
        "text": "what information is important and what's not?",
        "start": 192.59,
        "duration": 2.16
    },
    {
        "text": "You need to know exactly what everything is,",
        "start": 194.75,
        "duration": 2.64
    },
    {
        "text": "where everything is, and what it's",
        "start": 197.39,
        "duration": 1.95
    },
    {
        "text": "doing in relation to other objects.",
        "start": 199.34,
        "duration": 2.7
    },
    {
        "text": "Well, also understanding what is the foreground",
        "start": 202.04,
        "duration": 2.79
    },
    {
        "text": "and what information is important and what isn't.",
        "start": 204.83,
        "duration": 3.0
    },
    {
        "text": "You really need to understand what's going on,",
        "start": 207.83,
        "duration": 2.79
    },
    {
        "text": "and a machine needs to be able to summarize this and",
        "start": 210.62,
        "duration": 2.745
    },
    {
        "text": "also describe it in natural language description.",
        "start": 213.365,
        "duration": 4.405
    },
    {
        "text": ">> That's really cool. I mean,",
        "start": 219.44,
        "duration": 2.19
    },
    {
        "text": "can you go back one slide",
        "start": 221.63,
        "duration": 1.02
    },
    {
        "text": "because you need to know every little thing in there,",
        "start": 222.65,
        "duration": 5.07
    },
    {
        "text": "and then you didn't know how to arrange",
        "start": 227.72,
        "duration": 1.62
    },
    {
        "text": "the words so that it makes sense.",
        "start": 229.34,
        "duration": 1.98
    },
    {
        "text": "Like if you said dog, cat, table,",
        "start": 231.32,
        "duration": 2.04
    },
    {
        "text": "that's not really a good sentence,",
        "start": 233.36,
        "duration": 1.724
    },
    {
        "text": "but knowing where everything is is actually really cool as well.",
        "start": 235.084,
        "duration": 3.196
    },
    {
        "text": "Sorry, I interrupted you. I just wanted to point that out,",
        "start": 238.28,
        "duration": 1.86
    },
    {
        "text": "that it's actually a hard problem.",
        "start": 240.14,
        "duration": 2.015
    },
    {
        "text": ">> For sure. Even with the fine tuning",
        "start": 242.155,
        "duration": 2.245
    },
    {
        "text": "we do generate multiple instances of",
        "start": 244.4,
        "duration": 2.25
    },
    {
        "text": "what we think as a human natural language sentence",
        "start": 246.65,
        "duration": 2.955
    },
    {
        "text": "and with the confidence score.",
        "start": 249.605,
        "duration": 1.5
    },
    {
        "text": "Customers are able to pick it up in terms",
        "start": 251.105,
        "duration": 2.565
    },
    {
        "text": "of what localization and what tags are more important to them.",
        "start": 253.67,
        "duration": 4.055
    },
    {
        "text": ">> Awesome.",
        "start": 257.725,
        "duration": 2.165
    },
    {
        "text": ">> Moving onto how these image captioning systems are designed.",
        "start": 259.89,
        "duration": 4.715
    },
    {
        "text": "They're typically trained with datasets that contain",
        "start": 264.605,
        "duration": 2.28
    },
    {
        "text": "images paired with sentences that describe these images.",
        "start": 266.885,
        "duration": 3.3
    },
    {
        "text": "It's a set of captioned images.",
        "start": 270.185,
        "duration": 2.94
    },
    {
        "text": "In this training set,",
        "start": 273.125,
        "duration": 1.725
    },
    {
        "text": "there's a combination of COCO dataset which is",
        "start": 274.85,
        "duration": 2.28
    },
    {
        "text": "about 80 object classes with caption images,",
        "start": 277.13,
        "duration": 3.075
    },
    {
        "text": "and there's also Open Image dataset which is",
        "start": 280.205,
        "duration": 2.475
    },
    {
        "text": "about 600 classes with just object tags,",
        "start": 282.68,
        "duration": 2.64
    },
    {
        "text": "there are no captions associated with it.",
        "start": 285.32,
        "duration": 2.19
    },
    {
        "text": "What makes this nocaps challenge even more challenging is,",
        "start": 287.51,
        "duration": 4.065
    },
    {
        "text": "it tests how the model is able to",
        "start": 291.575,
        "duration": 1.815
    },
    {
        "text": "describe so that the novel unique objects,",
        "start": 293.39,
        "duration": 2.6
    },
    {
        "text": "they are present in",
        "start": 295.99,
        "duration": 1.9
    },
    {
        "text": "the testing image but not in the training dataset.",
        "start": 297.89,
        "duration": 2.88
    },
    {
        "text": "In this example here,",
        "start": 300.77,
        "duration": 2.57
    },
    {
        "text": "the COCO object classes, for example,",
        "start": 303.34,
        "duration": 3.22
    },
    {
        "text": "like dogs, bench, or umbrella,",
        "start": 306.56,
        "duration": 2.52
    },
    {
        "text": "they're images in the train dataset",
        "start": 309.08,
        "duration": 1.725
    },
    {
        "text": "with captions and descriptions.",
        "start": 310.805,
        "duration": 1.725
    },
    {
        "text": "Whereas, for Open Images that you're seeing,",
        "start": 312.53,
        "duration": 2.64
    },
    {
        "text": "whether it's goat or dolphin,",
        "start": 315.17,
        "duration": 1.455
    },
    {
        "text": "it's just object tags,",
        "start": 316.625,
        "duration": 1.425
    },
    {
        "text": "there are no captions present.",
        "start": 318.05,
        "duration": 2.29
    },
    {
        "text": ">> Sorry, keep going.",
        "start": 320.87,
        "duration": 2.095
    },
    {
        "text": ">> But in the right, you see how well the model performs",
        "start": 322.965,
        "duration": 3.35
    },
    {
        "text": "on even out of domain object classes like dolphin.",
        "start": 326.315,
        "duration": 4.195
    },
    {
        "text": ">> Every class of thing that it can look at, obviously,",
        "start": 334.55,
        "duration": 3.53
    },
    {
        "text": "it needs to know about that and it's been trained on",
        "start": 338.08,
        "duration": 2.38
    },
    {
        "text": "these Open Image dataset as well as the COCO dataset.",
        "start": 340.46,
        "duration": 3.69
    },
    {
        "text": "Is that how it gets knowledge",
        "start": 344.15,
        "duration": 1.53
    },
    {
        "text": "about the actual things that are there?",
        "start": 345.68,
        "duration": 2.0
    },
    {
        "text": ">> Yeah. With the COCO dataset there are descriptions associated.",
        "start": 347.68,
        "duration": 3.47
    },
    {
        "text": "You're understanding the semantic coupling",
        "start": 351.15,
        "duration": 2.76
    },
    {
        "text": "between the different tags;",
        "start": 353.91,
        "duration": 1.04
    },
    {
        "text": "between the dog and the bench,",
        "start": 354.95,
        "duration": 1.17
    },
    {
        "text": "the dog is sitting on the bench,",
        "start": 356.12,
        "duration": 1.56
    },
    {
        "text": "or the child is sitting on the couch.",
        "start": 357.68,
        "duration": 2.01
    },
    {
        "text": "Whereas, with Open Images where it just tags the present,",
        "start": 359.69,
        "duration": 3.0
    },
    {
        "text": "there's very little information that",
        "start": 362.69,
        "duration": 1.53
    },
    {
        "text": "the model gains from just the tags.",
        "start": 364.22,
        "duration": 1.98
    },
    {
        "text": "There's no semantic coupling,",
        "start": 366.2,
        "duration": 1.14
    },
    {
        "text": "there is no dolphin in the water.",
        "start": 367.34,
        "duration": 1.74
    },
    {
        "text": "But as you can see,",
        "start": 369.08,
        "duration": 1.26
    },
    {
        "text": "we validly test this model",
        "start": 370.34,
        "duration": 1.995
    },
    {
        "text": "that out of domain knowledge is equally good.",
        "start": 372.335,
        "duration": 2.7
    },
    {
        "text": "A dolphin is swimming close to the ocean,",
        "start": 375.035,
        "duration": 2.115
    },
    {
        "text": "so the water coupling is happening with",
        "start": 377.15,
        "duration": 2.19
    },
    {
        "text": "the novel unique object that the model is seeing.",
        "start": 379.34,
        "duration": 2.525
    },
    {
        "text": ">> That's cool. I feel like I want to see some examples",
        "start": 381.865,
        "duration": 2.665
    },
    {
        "text": "of things that it's captured. Do you have anything like that?",
        "start": 384.53,
        "duration": 2.895
    },
    {
        "text": ">> Yeah. For sure, you're reading my mind.",
        "start": 387.425,
        "duration": 2.16
    },
    {
        "text": ">> Yeah, I see. It's almost as if we looked at",
        "start": 389.585,
        "duration": 3.165
    },
    {
        "text": "this and talked about it together before. Almost.",
        "start": 392.75,
        "duration": 3.335
    },
    {
        "text": ">> Almost. Let's take a look at a few examples",
        "start": 396.085,
        "duration": 3.295
    },
    {
        "text": "that we tested both with",
        "start": 399.38,
        "duration": 1.02
    },
    {
        "text": "our previous model that was released in 2015,",
        "start": 400.4,
        "duration": 2.58
    },
    {
        "text": "and with the latest human parody model.",
        "start": 402.98,
        "duration": 2.1
    },
    {
        "text": "As you can see the older model",
        "start": 405.08,
        "duration": 2.88
    },
    {
        "text": "gave us a caption of close up of a plant,",
        "start": 407.96,
        "duration": 2.27
    },
    {
        "text": "and now it's a close up of a wheat",
        "start": 410.23,
        "duration": 1.93
    },
    {
        "text": "in a field which is much more accurate.",
        "start": 412.16,
        "duration": 3.08
    },
    {
        "text": ">> Yeah. I mean, that's pretty amazing.",
        "start": 416.18,
        "duration": 2.935
    },
    {
        "text": ">> Yeah. Even here,",
        "start": 419.115,
        "duration": 2.505
    },
    {
        "text": "the older model gave us an update",
        "start": 421.62,
        "duration": 2.71
    },
    {
        "text": "of a blue shirt or a man wearing a blue shirt,",
        "start": 424.33,
        "duration": 2.275
    },
    {
        "text": "and now we can see its surgical mask.",
        "start": 426.605,
        "duration": 2.295
    },
    {
        "text": "It's a lot more contextual and in specific domain.",
        "start": 428.9,
        "duration": 4.39
    },
    {
        "text": ">> This is really cool because, I mean,",
        "start": 433.31,
        "duration": 3.61
    },
    {
        "text": "back when I was a programmer I would get",
        "start": 436.92,
        "duration": 3.63
    },
    {
        "text": "data that's in a database and with numbers,",
        "start": 440.55,
        "duration": 3.72
    },
    {
        "text": "with even strings, you could do stuff with text.",
        "start": 444.27,
        "duration": 3.17
    },
    {
        "text": "But with images it was hard to actually",
        "start": 447.44,
        "duration": 2.31
    },
    {
        "text": "figure out what's actually going on an image.",
        "start": 449.75,
        "duration": 3.03
    },
    {
        "text": "Now with this, you can totally just do it, which is pretty amazing.",
        "start": 452.78,
        "duration": 4.29
    },
    {
        "text": "Do you have any you can show us?",
        "start": 457.07,
        "duration": 1.19
    },
    {
        "text": "How do people take advantage of this kind of functionality?",
        "start": 458.26,
        "duration": 2.875
    },
    {
        "text": ">> I've few more examples.",
        "start": 461.135,
        "duration": 1.53
    },
    {
        "text": ">> Let's bring it on, let's do it.",
        "start": 462.665,
        "duration": 2.245
    },
    {
        "text": ">> We can jump to this one actually.",
        "start": 465.35,
        "duration": 2.53
    },
    {
        "text": "As you can see, the model is extracting",
        "start": 467.88,
        "duration": 2.33
    },
    {
        "text": "more of these unique objects present in the image,",
        "start": 470.21,
        "duration": 2.459
    },
    {
        "text": "and generating in many cases",
        "start": 472.669,
        "duration": 1.531
    },
    {
        "text": "more accurate description of the images.",
        "start": 474.2,
        "duration": 3.61
    },
    {
        "text": ">> This is cool.",
        "start": 477.86,
        "duration": 2.2
    },
    {
        "text": ">> This is one of my favorite ones where",
        "start": 480.06,
        "duration": 2.06
    },
    {
        "text": "AI model is not just detecting",
        "start": 482.12,
        "duration": 2.055
    },
    {
        "text": "objects present in the image but",
        "start": 484.175,
        "duration": 1.665
    },
    {
        "text": "also conveying the intent behind it.",
        "start": 485.84,
        "duration": 2.31
    },
    {
        "text": "The new caption generated is a person making bread,",
        "start": 488.15,
        "duration": 3.48
    },
    {
        "text": "which clearly the intent",
        "start": 491.63,
        "duration": 1.77
    },
    {
        "text": "and the insight that you want to decipher from the images.",
        "start": 493.4,
        "duration": 3.44
    },
    {
        "text": ">> Yeah. If I squint,",
        "start": 496.84,
        "duration": 2.12
    },
    {
        "text": "it does look like a person cooking hotdogs on a cutting board.",
        "start": 498.96,
        "duration": 3.48
    },
    {
        "text": ">> Yeah.",
        "start": 502.44,
        "duration": 0.335
    },
    {
        "text": ">> But now when you look at it, it's excellent,",
        "start": 502.775,
        "duration": 2.92
    },
    {
        "text": "you can see that there is intent behind a person.",
        "start": 505.695,
        "duration": 3.255
    },
    {
        "text": "It's pretty cool that it's capturing that as well.",
        "start": 508.95,
        "duration": 3.03
    },
    {
        "text": "You show me so many good pictures,",
        "start": 511.98,
        "duration": 1.35
    },
    {
        "text": "now can we take a look at how can I use this today?",
        "start": 513.33,
        "duration": 2.43
    },
    {
        "text": ">> Sure. I'm sorry. Let's start here then.",
        "start": 515.76,
        "duration": 5.365
    },
    {
        "text": "Here I'm showing the quick demo of how",
        "start": 521.125,
        "duration": 2.295
    },
    {
        "text": "the API can be integrated into your application.",
        "start": 523.42,
        "duration": 2.969
    },
    {
        "text": "To access Azure Cognitive Services API,",
        "start": 526.389,
        "duration": 2.821
    },
    {
        "text": "you need the Azure subscription.",
        "start": 529.21,
        "duration": 1.515
    },
    {
        "text": "You can create one for free using a free pricing tier.",
        "start": 530.725,
        "duration": 3.825
    },
    {
        "text": "Review the subscription, and create",
        "start": 534.55,
        "duration": 1.92
    },
    {
        "text": "the resource in specific deployment region.",
        "start": 536.47,
        "duration": 2.85
    },
    {
        "text": "Once you have Azure subscription,",
        "start": 539.32,
        "duration": 2.46
    },
    {
        "text": "create a computer vision resource in the Azure portal,",
        "start": 541.78,
        "duration": 3.18
    },
    {
        "text": "and get access to your key and endpoint.",
        "start": 544.96,
        "duration": 3.165
    },
    {
        "text": "You will need the key and endpoint",
        "start": 548.125,
        "duration": 2.325
    },
    {
        "text": "for the resource you create to connect your application,",
        "start": 550.45,
        "duration": 3.284
    },
    {
        "text": "to the Computer Vision service.",
        "start": 553.734,
        "duration": 1.516
    },
    {
        "text": "You'll be placing your key and endpoint in the application code.",
        "start": 555.25,
        "duration": 3.075
    },
    {
        "text": "Let's copy them over.",
        "start": 558.325,
        "duration": 2.67
    },
    {
        "text": "Now, let's pull up my Jupyter Notebook to see this in action.",
        "start": 560.995,
        "duration": 6.51
    },
    {
        "text": "Here I'm going to offer public services and system libraries.",
        "start": 567.505,
        "duration": 3.645
    },
    {
        "text": "Let's run that through.",
        "start": 571.15,
        "duration": 1.95
    },
    {
        "text": "Once that's imported, we'll add the subscription key and",
        "start": 573.1,
        "duration": 3.375
    },
    {
        "text": "Azure endpoint. Running that through.",
        "start": 576.475,
        "duration": 3.705
    },
    {
        "text": "Here's the interesting part.",
        "start": 580.18,
        "duration": 1.44
    },
    {
        "text": "The API can take in a URL and also a local image pack.",
        "start": 581.62,
        "duration": 4.95
    },
    {
        "text": "Seth, I'm going to put you on the spot here.",
        "start": 586.57,
        "duration": 3.06
    },
    {
        "text": ">> Let's do it.",
        "start": 589.63,
        "duration": 0.87
    },
    {
        "text": ">> Bring up a couple of Bing images.",
        "start": 590.5,
        "duration": 4.449
    },
    {
        "text": "What do you think of this one,",
        "start": 594.949,
        "duration": 2.111
    },
    {
        "text": "should we go ahead with this?",
        "start": 597.06,
        "duration": 1.905
    },
    {
        "text": ">> Sure.",
        "start": 598.965,
        "duration": 2.395
    },
    {
        "text": ">> We're copying the URL of the image and pasting it here.",
        "start": 602.18,
        "duration": 7.825
    },
    {
        "text": "Now, let's run this through.",
        "start": 610.005,
        "duration": 3.475
    },
    {
        "text": "Let's check what our image captioning generated.",
        "start": 614.7,
        "duration": 4.3
    },
    {
        "text": "Briana Banks holding the chart. Pretty accurate.",
        "start": 619.0,
        "duration": 3.54
    },
    {
        "text": ">> I don't even know who Briana Banks is.",
        "start": 622.54,
        "duration": 2.175
    },
    {
        "text": ">> Now, you know.",
        "start": 624.715,
        "duration": 0.945
    },
    {
        "text": ">> Now, I know.",
        "start": 625.66,
        "duration": 1.84
    },
    {
        "text": ">> Sorry, go ahead.",
        "start": 629.16,
        "duration": 2.305
    },
    {
        "text": ">> Fantastic. I just think this is great.",
        "start": 631.465,
        "duration": 2.295
    },
    {
        "text": "Do we have any other pictures?",
        "start": 633.76,
        "duration": 1.455
    },
    {
        "text": ">> Yeah. Let's try out a few more actually.",
        "start": 635.215,
        "duration": 3.715
    },
    {
        "text": "Do you want to give this a go?",
        "start": 639.33,
        "duration": 2.725
    },
    {
        "text": ">> Yeah, let's do it.",
        "start": 642.055,
        "duration": 2.095
    },
    {
        "text": ">> Copying the URL of this picture now.",
        "start": 644.73,
        "duration": 5.03
    },
    {
        "text": "Let's change the URL part.",
        "start": 649.76,
        "duration": 5.36
    },
    {
        "text": "Perfect.",
        "start": 656.06,
        "duration": 2.23
    },
    {
        "text": "Let's run that through and pull off.",
        "start": 658.29,
        "duration": 5.78
    },
    {
        "text": "Group of people and fireworks.",
        "start": 664.07,
        "duration": 2.48
    },
    {
        "text": ">> Let's try one more. Let's search for",
        "start": 666.55,
        "duration": 1.2
    },
    {
        "text": "Seth Juarez. Here, I'll spell my name.",
        "start": 667.75,
        "duration": 1.41
    },
    {
        "text": "Let's see if it knows who I am. Computers are funny.",
        "start": 669.16,
        "duration": 3.51
    },
    {
        "text": "Just, S-E-T-H and J-U-A-R-E.",
        "start": 672.67,
        "duration": 3.76
    },
    {
        "text": "There we go. Let's see if it finds a picture of me.",
        "start": 679.2,
        "duration": 4.315
    },
    {
        "text": "If I'm doing something crazy.",
        "start": 683.515,
        "duration": 2.935
    },
    {
        "text": ">> We do have a picture of you exactly.",
        "start": 687.24,
        "duration": 3.37
    },
    {
        "text": ">> Yeah, it do. Old school picture. Let's see what we got.",
        "start": 690.61,
        "duration": 3.93
    },
    {
        "text": "It's obviously we going to break the computer.",
        "start": 694.54,
        "duration": 3.0
    },
    {
        "text": "I'm sorry, I broke cognitive services. Let's try it.",
        "start": 697.54,
        "duration": 4.635
    },
    {
        "text": ">> Let's try this one more time.",
        "start": 702.175,
        "duration": 3.355
    },
    {
        "text": "It did actually, that's interesting.",
        "start": 706.14,
        "duration": 4.04
    },
    {
        "text": "Let me open up.",
        "start": 710.19,
        "duration": 3.41
    },
    {
        "text": ">> How about the one with me talking to some dude?",
        "start": 715.2,
        "duration": 4.494
    },
    {
        "text": "Go back one. Back one.",
        "start": 719.694,
        "duration": 3.776
    },
    {
        "text": "The one to the right of that.",
        "start": 724.77,
        "duration": 2.365
    },
    {
        "text": "Because that's the one to the right.",
        "start": 727.135,
        "duration": 1.725
    },
    {
        "text": "There you go. Try that one. Let's try that picture.",
        "start": 728.86,
        "duration": 2.58
    },
    {
        "text": "Let's see if it knows me or this guy.",
        "start": 731.44,
        "duration": 3.285
    },
    {
        "text": "Because maybe I was talking to a really famous guy",
        "start": 734.725,
        "duration": 1.875
    },
    {
        "text": "and I just had no idea.",
        "start": 736.6,
        "duration": 2.17
    },
    {
        "text": ">> Let's do it. I see what's happening. Sorry, give me a minute.",
        "start": 739.08,
        "duration": 7.82
    },
    {
        "text": ">> Oh, I didn't get the whole thing.",
        "start": 751.89,
        "duration": 2.485
    },
    {
        "text": ">> Yeah, I was just adding too big URL.",
        "start": 754.375,
        "duration": 3.355
    },
    {
        "text": "Let's try this.",
        "start": 757.89,
        "duration": 3.26
    },
    {
        "text": ">> Perfect. JPEG.",
        "start": 763.29,
        "duration": 7.81
    },
    {
        "text": ">> Not true.",
        "start": 771.1,
        "duration": 1.74
    },
    {
        "text": "I wish you don't know what's happening.",
        "start": 772.84,
        "duration": 4.93
    },
    {
        "text": ">> Yeah, I'll tell you. Go at the end,",
        "start": 779.16,
        "duration": 3.01
    },
    {
        "text": "and then there's a single quote you missed.",
        "start": 782.17,
        "duration": 3.46
    },
    {
        "text": "At the end of the string. Hit there, \"Single\".",
        "start": 785.82,
        "duration": 4.96
    },
    {
        "text": "There you go. Now, let's try it.",
        "start": 790.78,
        "duration": 3.645
    },
    {
        "text": ">> Perfect.",
        "start": 794.425,
        "duration": 0.75
    },
    {
        "text": ">> Here we go.",
        "start": 795.175,
        "duration": 1.605
    },
    {
        "text": ">> Men sitting at the table,",
        "start": 796.78,
        "duration": 1.995
    },
    {
        "text": "looking at a laptop.",
        "start": 798.775,
        "duration": 1.185
    },
    {
        "text": ">> Yes.",
        "start": 799.96,
        "duration": 1.38
    },
    {
        "text": "It knows that I'm sitting at a table, looking at a laptop.",
        "start": 801.34,
        "duration": 4.685
    },
    {
        "text": "Look at that. How cool is that?",
        "start": 806.025,
        "duration": 3.33
    },
    {
        "text": "This is cool. It seems pretty easy.",
        "start": 809.355,
        "duration": 2.385
    },
    {
        "text": "You set up the service,",
        "start": 811.74,
        "duration": 3.995
    },
    {
        "text": "you put in the key, and you just",
        "start": 815.735,
        "duration": 1.715
    },
    {
        "text": "start sending pictures to the thing.",
        "start": 817.45,
        "duration": 1.59
    },
    {
        "text": "It can be a URL or it could be",
        "start": 819.04,
        "duration": 1.5
    },
    {
        "text": "something you push up local. Is that right?",
        "start": 820.54,
        "duration": 1.995
    },
    {
        "text": ">> Yeah, exactly.",
        "start": 822.535,
        "duration": 1.62
    },
    {
        "text": ">> That's awesome. What else is new with computer vision?",
        "start": 824.155,
        "duration": 3.645
    },
    {
        "text": "Do you have anything new that we should be thinking about as well?",
        "start": 827.8,
        "duration": 3.495
    },
    {
        "text": ">> Yes. We've had a ton of updates this fall.",
        "start": 831.295,
        "duration": 3.105
    },
    {
        "text": "We've 3.1 version of computer vision APIs generally available.",
        "start": 834.4,
        "duration": 3.975
    },
    {
        "text": "We've updated SDKs for Python,",
        "start": 838.375,
        "duration": 2.115
    },
    {
        "text": "Java, C#, and JavaScript.",
        "start": 840.49,
        "duration": 2.19
    },
    {
        "text": "These leaders AI capabilities are",
        "start": 842.68,
        "duration": 2.04
    },
    {
        "text": "integrated with Intel Microsoft products,",
        "start": 844.72,
        "duration": 2.115
    },
    {
        "text": "and available to our customers using Office products.",
        "start": 846.835,
        "duration": 3.465
    },
    {
        "text": ">> I didn't think about that. Office is",
        "start": 850.3,
        "duration": 3.39
    },
    {
        "text": "using these things internally right now?",
        "start": 853.69,
        "duration": 2.43
    },
    {
        "text": ">> Yes. The next slide",
        "start": 856.12,
        "duration": 4.86
    },
    {
        "text": "I'll show the integrations that are",
        "start": 860.98,
        "duration": 1.59
    },
    {
        "text": "currently in our Office products.",
        "start": 862.57,
        "duration": 2.61
    },
    {
        "text": ">> Let's take a look.",
        "start": 865.18,
        "duration": 1.47
    },
    {
        "text": ">> The enhanced image captioning model",
        "start": 866.65,
        "duration": 1.68
    },
    {
        "text": "is already in our production service,",
        "start": 868.33,
        "duration": 1.47
    },
    {
        "text": "and is available in latest version of our Computer Vision API.",
        "start": 869.8,
        "duration": 3.24
    },
    {
        "text": "The service provides a flexible deployment option",
        "start": 873.04,
        "duration": 2.355
    },
    {
        "text": "enabling developers to use",
        "start": 875.395,
        "duration": 1.605
    },
    {
        "text": "this capability to both improve",
        "start": 877.0,
        "duration": 1.83
    },
    {
        "text": "accessibility in their own applications, and inter-phases.",
        "start": 878.83,
        "duration": 3.495
    },
    {
        "text": "The AI capability is already being used in Seeing AI,",
        "start": 882.325,
        "duration": 3.285
    },
    {
        "text": "Microsoft's Assistant mobile app for visually impaired.",
        "start": 885.61,
        "duration": 3.495
    },
    {
        "text": "A user can take a snapshot of their surroundings,",
        "start": 889.105,
        "duration": 2.505
    },
    {
        "text": "and Computer Vision Describe API will return",
        "start": 891.61,
        "duration": 2.43
    },
    {
        "text": "the description to help the users understand the scene,",
        "start": 894.04,
        "duration": 3.06
    },
    {
        "text": "and participate in the conversations.",
        "start": 897.1,
        "duration": 2.61
    },
    {
        "text": "Image captioning is also being used for",
        "start": 899.71,
        "duration": 2.55
    },
    {
        "text": "tasks like creating all text for images,",
        "start": 902.26,
        "duration": 2.415
    },
    {
        "text": "and this is available through",
        "start": 904.675,
        "duration": 1.485
    },
    {
        "text": "our Microsoft Office products like Office,",
        "start": 906.16,
        "duration": 2.73
    },
    {
        "text": "Outlook, Word, and PowerPoint.",
        "start": 908.89,
        "duration": 3.03
    },
    {
        "text": ">> This is really cool.",
        "start": 911.92,
        "duration": 2.295
    },
    {
        "text": "Where can people go to find out",
        "start": 914.215,
        "duration": 1.845
    },
    {
        "text": "more to get started on using this good stuff?",
        "start": 916.06,
        "duration": 3.52
    },
    {
        "text": ">> To get started, we have a bunch of resources available online.",
        "start": 920.1,
        "duration": 3.7
    },
    {
        "text": "You can visit Azure Cognitive Services portal or",
        "start": 923.8,
        "duration": 2.85
    },
    {
        "text": "directly go to Developer Documentation on Microsoft docs.com.",
        "start": 926.65,
        "duration": 3.57
    },
    {
        "text": "Look through API references,",
        "start": 930.22,
        "duration": 1.71
    },
    {
        "text": "concepts, quick-start guides, and tutorials.",
        "start": 931.93,
        "duration": 2.58
    },
    {
        "text": "If you have questions or feedback,",
        "start": 934.51,
        "duration": 2.13
    },
    {
        "text": "you can also contact us through",
        "start": 936.64,
        "duration": 1.485
    },
    {
        "text": "Azure Cognitive Services customer support.",
        "start": 938.125,
        "duration": 3.12
    },
    {
        "text": ">> Well, this is absolutely amazing.",
        "start": 941.245,
        "duration": 3.675
    },
    {
        "text": "I've enjoyed looking at this.",
        "start": 944.92,
        "duration": 1.44
    },
    {
        "text": "Like I said, I love computer vision,",
        "start": 946.36,
        "duration": 1.74
    },
    {
        "text": "is one of my favorite things.",
        "start": 948.1,
        "duration": 1.05
    },
    {
        "text": "It's cool to see it actually generating captions,",
        "start": 949.15,
        "duration": 2.67
    },
    {
        "text": "and doing all this good stuff.",
        "start": 951.82,
        "duration": 1.11
    },
    {
        "text": "Thank you so much for spending some time with us, my friend.",
        "start": 952.93,
        "duration": 2.085
    },
    {
        "text": ">> Thank you.",
        "start": 955.015,
        "duration": 1.275
    },
    {
        "text": ">> Also, thank you so much for watching.",
        "start": 956.29,
        "duration": 2.22
    },
    {
        "text": "We're learning all about the new image",
        "start": 958.51,
        "duration": 1.65
    },
    {
        "text": "captioning inside of computer vision.",
        "start": 960.16,
        "duration": 2.52
    },
    {
        "text": "Thank you so much for watching, and",
        "start": 962.68,
        "duration": 1.2
    },
    {
        "text": "hopefully we'll see you next time. Take care.",
        "start": 963.88,
        "duration": 1.35
    },
    {
        "text": "[MUSIC]",
        "start": 965.23,
        "duration": 14.77
    }
]