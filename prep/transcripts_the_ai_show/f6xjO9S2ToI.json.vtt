[
    {
        "text": "you're not going to want to miss this",
        "start": 0.24,
        "duration": 3.199
    },
    {
        "text": "episode of the ai show where dimitro",
        "start": 1.439,
        "duration": 4.801
    },
    {
        "text": "a core contributor to pie torch walks us",
        "start": 3.439,
        "duration": 3.521
    },
    {
        "text": "through",
        "start": 6.24,
        "duration": 3.6
    },
    {
        "text": "how it actually works hands-on you're",
        "start": 6.96,
        "duration": 4.08
    },
    {
        "text": "not going to want to miss it make sure",
        "start": 9.84,
        "duration": 6.1
    },
    {
        "text": "you tune in",
        "start": 11.04,
        "duration": 8.8
    },
    {
        "text": "[Music]",
        "start": 15.94,
        "duration": 5.339
    },
    {
        "text": "hello and welcome to this special",
        "start": 19.84,
        "duration": 3.199
    },
    {
        "text": "episode of the ai show where we've",
        "start": 21.279,
        "duration": 2.961
    },
    {
        "text": "invited dimitro",
        "start": 23.039,
        "duration": 3.841
    },
    {
        "text": "back because i'm a huge fan of pie torch",
        "start": 24.24,
        "duration": 4.0
    },
    {
        "text": "dmitry why don't you introduce yourself",
        "start": 26.88,
        "duration": 3.6
    },
    {
        "text": "tell us who you are and what you do",
        "start": 28.24,
        "duration": 4.8
    },
    {
        "text": "hi chad uh thanks thanks again for you",
        "start": 30.48,
        "duration": 3.599
    },
    {
        "text": "know inviting me back",
        "start": 33.04,
        "duration": 4.24
    },
    {
        "text": "it's it's been a lot of fun so today i",
        "start": 34.079,
        "duration": 4.241
    },
    {
        "text": "basically wanted to",
        "start": 37.28,
        "duration": 4.32
    },
    {
        "text": "talk about hands-on experience and show",
        "start": 38.32,
        "duration": 4.239
    },
    {
        "text": "you some code with",
        "start": 41.6,
        "duration": 3.2
    },
    {
        "text": "pytorch and a little bit about myself",
        "start": 42.559,
        "duration": 5.121
    },
    {
        "text": "i'm one of the core developers of python",
        "start": 44.8,
        "duration": 3.759
    },
    {
        "text": "framework",
        "start": 47.68,
        "duration": 2.8
    },
    {
        "text": "uh pretty much working on a lot of",
        "start": 48.559,
        "duration": 3.441
    },
    {
        "text": "abstractions and bits and pieces and",
        "start": 50.48,
        "duration": 2.8
    },
    {
        "text": "trying to make all this awesome",
        "start": 52.0,
        "duration": 2.64
    },
    {
        "text": "functionality fit together",
        "start": 53.28,
        "duration": 3.68
    },
    {
        "text": "and make it easy to use yeah and so last",
        "start": 54.64,
        "duration": 4.0
    },
    {
        "text": "time we had a really good discussion i",
        "start": 56.96,
        "duration": 2.88
    },
    {
        "text": "loved it but i thought",
        "start": 58.64,
        "duration": 3.759
    },
    {
        "text": "this time as you mentioned maybe we can",
        "start": 59.84,
        "duration": 4.399
    },
    {
        "text": "actually dive in to some",
        "start": 62.399,
        "duration": 3.681
    },
    {
        "text": "of what it looks like in code can we do",
        "start": 64.239,
        "duration": 3.92
    },
    {
        "text": "that yeah of course",
        "start": 66.08,
        "duration": 4.24
    },
    {
        "text": "and i have i have actually i paid a",
        "start": 68.159,
        "duration": 3.361
    },
    {
        "text": "notebook for you",
        "start": 70.32,
        "duration": 3.28
    },
    {
        "text": "so we i was thinking we can just",
        "start": 71.52,
        "duration": 3.279
    },
    {
        "text": "basically go through this and",
        "start": 73.6,
        "duration": 3.36
    },
    {
        "text": "i can try to comment on some of the",
        "start": 74.799,
        "duration": 3.121
    },
    {
        "text": "details",
        "start": 76.96,
        "duration": 3.44
    },
    {
        "text": "and again disclaimer this is more like",
        "start": 77.92,
        "duration": 3.92
    },
    {
        "text": "very",
        "start": 80.4,
        "duration": 3.44
    },
    {
        "text": "very intro level part of a better just",
        "start": 81.84,
        "duration": 4.319
    },
    {
        "text": "showcasing core functionality so kind of",
        "start": 83.84,
        "duration": 4.959
    },
    {
        "text": "showing the overall libraries and flow",
        "start": 86.159,
        "duration": 3.761
    },
    {
        "text": "and how you would",
        "start": 88.799,
        "duration": 3.36
    },
    {
        "text": "might train your actual model uh the",
        "start": 89.92,
        "duration": 3.92
    },
    {
        "text": "model here the one we're gonna take is",
        "start": 92.159,
        "duration": 2.96
    },
    {
        "text": "pretty basic which is",
        "start": 93.84,
        "duration": 3.52
    },
    {
        "text": "basically a mist but with fashionista",
        "start": 95.119,
        "duration": 3.441
    },
    {
        "text": "instead of digits",
        "start": 97.36,
        "duration": 3.119
    },
    {
        "text": "uh just so they can train it in my",
        "start": 98.56,
        "duration": 4.16
    },
    {
        "text": "browser and do it pretty quickly",
        "start": 100.479,
        "duration": 4.241
    },
    {
        "text": "but uh of course i mean the same",
        "start": 102.72,
        "duration": 3.359
    },
    {
        "text": "abstractions translate to",
        "start": 104.72,
        "duration": 2.88
    },
    {
        "text": "much more interest in cutting-edge",
        "start": 106.079,
        "duration": 3.201
    },
    {
        "text": "research so",
        "start": 107.6,
        "duration": 3.92
    },
    {
        "text": "let's get started i mean i'll start with",
        "start": 109.28,
        "duration": 3.04
    },
    {
        "text": "just",
        "start": 111.52,
        "duration": 3.12
    },
    {
        "text": "uh importing some pythagoras libraries",
        "start": 112.32,
        "duration": 3.6
    },
    {
        "text": "and as we talked last time",
        "start": 114.64,
        "duration": 4.079
    },
    {
        "text": "uh basically fighters at its core is",
        "start": 115.92,
        "duration": 4.159
    },
    {
        "text": "just a tensor library",
        "start": 118.719,
        "duration": 4.641
    },
    {
        "text": "uh you can think of it as kind of numpy",
        "start": 120.079,
        "duration": 6.32
    },
    {
        "text": "equivalent so we we might just do",
        "start": 123.36,
        "duration": 6.0
    },
    {
        "text": "random over additions for example uh",
        "start": 126.399,
        "duration": 3.441
    },
    {
        "text": "created",
        "start": 129.36,
        "duration": 3.44
    },
    {
        "text": "random metrics of numbers with different",
        "start": 129.84,
        "duration": 3.84
    },
    {
        "text": "data types",
        "start": 132.8,
        "duration": 3.36
    },
    {
        "text": "just how you how you would do numpy you",
        "start": 133.68,
        "duration": 3.279
    },
    {
        "text": "can",
        "start": 136.16,
        "duration": 3.76
    },
    {
        "text": "you know add tensors together you can do",
        "start": 136.959,
        "duration": 6.081
    },
    {
        "text": "all the kind of fun operations in them",
        "start": 139.92,
        "duration": 5.28
    },
    {
        "text": "similarly to dampai we even support kind",
        "start": 143.04,
        "duration": 3.76
    },
    {
        "text": "of fancy python style index",
        "start": 145.2,
        "duration": 4.08
    },
    {
        "text": "indexing so you can slice and dice and",
        "start": 146.8,
        "duration": 3.36
    },
    {
        "text": "do views",
        "start": 149.28,
        "duration": 3.28
    },
    {
        "text": "and do mutable stuff like that and the",
        "start": 150.16,
        "duration": 3.439
    },
    {
        "text": "best part is that",
        "start": 152.56,
        "duration": 4.319
    },
    {
        "text": "the standard library is interoperable",
        "start": 153.599,
        "duration": 3.841
    },
    {
        "text": "with",
        "start": 156.879,
        "duration": 3.44
    },
    {
        "text": "numpy so for example if i create python",
        "start": 157.44,
        "duration": 3.68
    },
    {
        "text": "tensor",
        "start": 160.319,
        "duration": 3.841
    },
    {
        "text": "i can call dot numpy on it and get back",
        "start": 161.12,
        "duration": 6.399
    },
    {
        "text": "basically the nd array fro from numpy",
        "start": 164.16,
        "duration": 5.68
    },
    {
        "text": "it's a zero copy view so it will not",
        "start": 167.519,
        "duration": 3.921
    },
    {
        "text": "actually go and copy the memory",
        "start": 169.84,
        "duration": 3.2
    },
    {
        "text": "uh which is where it comes very handy if",
        "start": 171.44,
        "duration": 3.2
    },
    {
        "text": "you have like some you know third party",
        "start": 173.04,
        "duration": 2.8
    },
    {
        "text": "library or if you want to",
        "start": 174.64,
        "duration": 3.2
    },
    {
        "text": "call some some library from numpy",
        "start": 175.84,
        "duration": 3.84
    },
    {
        "text": "ecosystem like psychic learn in the",
        "start": 177.84,
        "duration": 3.36
    },
    {
        "text": "middle of your model you can actually do",
        "start": 179.68,
        "duration": 2.08
    },
    {
        "text": "that",
        "start": 181.2,
        "duration": 3.039
    },
    {
        "text": "without paying any performance penalty",
        "start": 181.76,
        "duration": 3.36
    },
    {
        "text": "so question here",
        "start": 184.239,
        "duration": 4.241
    },
    {
        "text": "these aren't actually numpy tensors",
        "start": 185.12,
        "duration": 4.479
    },
    {
        "text": "these are there's like a",
        "start": 188.48,
        "duration": 3.679
    },
    {
        "text": "full like tensor library that",
        "start": 189.599,
        "duration": 4.161
    },
    {
        "text": "interoperates with numpy",
        "start": 192.159,
        "duration": 2.641
    },
    {
        "text": "i think that's an important distinction",
        "start": 193.76,
        "duration": 2.88
    },
    {
        "text": "is that is that the case yeah that's",
        "start": 194.8,
        "duration": 3.28
    },
    {
        "text": "correct for example if i do",
        "start": 196.64,
        "duration": 5.28
    },
    {
        "text": "type a and type b",
        "start": 198.08,
        "duration": 5.76
    },
    {
        "text": "that one of them would be class storage",
        "start": 201.92,
        "duration": 3.2
    },
    {
        "text": "tensor another one will be",
        "start": 203.84,
        "duration": 2.88
    },
    {
        "text": "numpy and the array but they would",
        "start": 205.12,
        "duration": 3.199
    },
    {
        "text": "actually share the same memory in this",
        "start": 206.72,
        "duration": 3.36
    },
    {
        "text": "case because it's a cpu tensor",
        "start": 208.319,
        "duration": 4.881
    },
    {
        "text": "got it numpy of course runs just in cpu",
        "start": 210.08,
        "duration": 4.56
    },
    {
        "text": "the fun part about",
        "start": 213.2,
        "duration": 4.8
    },
    {
        "text": "uh about pythorg it can run",
        "start": 214.64,
        "duration": 5.519
    },
    {
        "text": "run on code as well so i can basically",
        "start": 218.0,
        "duration": 4.08
    },
    {
        "text": "specify device equals cuda",
        "start": 220.159,
        "duration": 4.961
    },
    {
        "text": "or just just as a stream in this case",
        "start": 222.08,
        "duration": 6.32
    },
    {
        "text": "let's say we have only one one gpu and",
        "start": 225.12,
        "duration": 5.119
    },
    {
        "text": "boom i can basically create exactly the",
        "start": 228.4,
        "duration": 3.6
    },
    {
        "text": "same same tensor",
        "start": 230.239,
        "duration": 4.401
    },
    {
        "text": "now it now though leaves on gpu so this",
        "start": 232.0,
        "duration": 4.48
    },
    {
        "text": "operation actually runs on gpu",
        "start": 234.64,
        "duration": 3.76
    },
    {
        "text": "and of course in order to print results",
        "start": 236.48,
        "duration": 3.759
    },
    {
        "text": "i can copy it back to cpu",
        "start": 238.4,
        "duration": 4.96
    },
    {
        "text": "and visualize so that's uh pretty much",
        "start": 240.239,
        "duration": 4.241
    },
    {
        "text": "the core of",
        "start": 243.36,
        "duration": 2.72
    },
    {
        "text": "python library again if you want to use",
        "start": 244.48,
        "duration": 4.319
    },
    {
        "text": "just just the tensor library itself and",
        "start": 246.08,
        "duration": 4.799
    },
    {
        "text": "do kind of accelerated operations on",
        "start": 248.799,
        "duration": 4.321
    },
    {
        "text": "cpus gpus or other devices",
        "start": 250.879,
        "duration": 4.481
    },
    {
        "text": "uh you pretty much can use it more or",
        "start": 253.12,
        "duration": 4.239
    },
    {
        "text": "less as a dripping replacement for numpy",
        "start": 255.36,
        "duration": 4.08
    },
    {
        "text": "and we have pretty high compatibility of",
        "start": 257.359,
        "duration": 3.521
    },
    {
        "text": "apis and actually there's",
        "start": 259.44,
        "duration": 3.52
    },
    {
        "text": "uh effort to kind of close that and have",
        "start": 260.88,
        "duration": 3.36
    },
    {
        "text": "like complete",
        "start": 262.96,
        "duration": 3.2
    },
    {
        "text": "compatibility in terms of api based",
        "start": 264.24,
        "duration": 3.519
    },
    {
        "text": "numpy but",
        "start": 266.16,
        "duration": 3.2
    },
    {
        "text": "we actually came here to do some deep",
        "start": 267.759,
        "duration": 3.121
    },
    {
        "text": "learning and again",
        "start": 269.36,
        "duration": 3.2
    },
    {
        "text": "can answer questions in theory you could",
        "start": 270.88,
        "duration": 4.4
    },
    {
        "text": "use tor uh pytorch tensors",
        "start": 272.56,
        "duration": 4.88
    },
    {
        "text": "to do any kind of numerical computation",
        "start": 275.28,
        "duration": 4.56
    },
    {
        "text": "but then just move it to the gpu is that",
        "start": 277.44,
        "duration": 3.52
    },
    {
        "text": "right",
        "start": 279.84,
        "duration": 4.079
    },
    {
        "text": "yeah so i mean uh basically linear",
        "start": 280.96,
        "duration": 4.08
    },
    {
        "text": "algebra operations work",
        "start": 283.919,
        "duration": 3.041
    },
    {
        "text": "usually very fast in gpus because of",
        "start": 285.04,
        "duration": 3.76
    },
    {
        "text": "it's highly parallel power so yeah you",
        "start": 286.96,
        "duration": 3.12
    },
    {
        "text": "can you can think of it as",
        "start": 288.8,
        "duration": 2.72
    },
    {
        "text": "pretty much a drop in replacement for",
        "start": 290.08,
        "duration": 3.04
    },
    {
        "text": "the tensor library for gpus",
        "start": 291.52,
        "duration": 5.2
    },
    {
        "text": "or i mean for for other devices",
        "start": 293.12,
        "duration": 6.56
    },
    {
        "text": "uh i mean there are some surprise",
        "start": 296.72,
        "duration": 4.24
    },
    {
        "text": "projects which kind of integrate in",
        "start": 299.68,
        "duration": 2.079
    },
    {
        "text": "similar",
        "start": 300.96,
        "duration": 2.88
    },
    {
        "text": "uh similarly to gpu like other devices",
        "start": 301.759,
        "duration": 3.041
    },
    {
        "text": "like accelerators",
        "start": 303.84,
        "duration": 4.48
    },
    {
        "text": "into into better g days",
        "start": 304.8,
        "duration": 5.92
    },
    {
        "text": "and so for this demo let's grab kind of",
        "start": 308.32,
        "duration": 3.12
    },
    {
        "text": "a classic",
        "start": 310.72,
        "duration": 3.28
    },
    {
        "text": "uh example for neural network",
        "start": 311.44,
        "duration": 3.599
    },
    {
        "text": "recognition which is",
        "start": 314.0,
        "duration": 4.08
    },
    {
        "text": "which is a mini data set however instead",
        "start": 315.039,
        "duration": 3.921
    },
    {
        "text": "of digits we'll",
        "start": 318.08,
        "duration": 2.88
    },
    {
        "text": "get the more fun version of it which is",
        "start": 318.96,
        "duration": 4.16
    },
    {
        "text": "fashionabnist so similar to original",
        "start": 320.96,
        "duration": 4.64
    },
    {
        "text": "and useful like handwriting this is",
        "start": 323.12,
        "duration": 3.44
    },
    {
        "text": "recognition",
        "start": 325.6,
        "duration": 3.92
    },
    {
        "text": "it's basically a set of like 28 by 28",
        "start": 326.56,
        "duration": 4.16
    },
    {
        "text": "grayscale images",
        "start": 329.52,
        "duration": 2.72
    },
    {
        "text": "but instead of 10 digits they have like",
        "start": 330.72,
        "duration": 3.919
    },
    {
        "text": "10 different types of closes",
        "start": 332.24,
        "duration": 4.959
    },
    {
        "text": "uh so it's more fun to look at compared",
        "start": 334.639,
        "duration": 3.761
    },
    {
        "text": "with that",
        "start": 337.199,
        "duration": 3.361
    },
    {
        "text": "so as with our class as without last",
        "start": 338.4,
        "duration": 3.12
    },
    {
        "text": "time uh",
        "start": 340.56,
        "duration": 3.04
    },
    {
        "text": "pythagoras also comes with kind of a set",
        "start": 341.52,
        "duration": 3.2
    },
    {
        "text": "of libraries which",
        "start": 343.6,
        "duration": 2.8
    },
    {
        "text": "uh help you to kind of get started with",
        "start": 344.72,
        "duration": 3.039
    },
    {
        "text": "data reading and",
        "start": 346.4,
        "duration": 3.68
    },
    {
        "text": "kind of like preparing your data or even",
        "start": 347.759,
        "duration": 4.321
    },
    {
        "text": "have like bindings to common data sets",
        "start": 350.08,
        "duration": 4.24
    },
    {
        "text": "so for something like uh amnesia fashion",
        "start": 352.08,
        "duration": 4.32
    },
    {
        "text": "list or image network",
        "start": 354.32,
        "duration": 3.92
    },
    {
        "text": "are the most popular public data sets",
        "start": 356.4,
        "duration": 3.359
    },
    {
        "text": "pretty much have like one line",
        "start": 358.24,
        "duration": 4.88
    },
    {
        "text": "apis in part of pathogen",
        "start": 359.759,
        "duration": 6.88
    },
    {
        "text": "so by uh by running this we can",
        "start": 363.12,
        "duration": 5.359
    },
    {
        "text": "basically initialize data set like if",
        "start": 366.639,
        "duration": 2.481
    },
    {
        "text": "it's not",
        "start": 368.479,
        "duration": 2.641
    },
    {
        "text": "already downloaded my machine is gonna",
        "start": 369.12,
        "duration": 4.24
    },
    {
        "text": "go and fetch it from from the internet",
        "start": 371.12,
        "duration": 4.639
    },
    {
        "text": "uh we can specify which transformations",
        "start": 373.36,
        "duration": 3.6
    },
    {
        "text": "we want to images",
        "start": 375.759,
        "duration": 3.681
    },
    {
        "text": "uh to do on images to prepare them and",
        "start": 376.96,
        "duration": 4.16
    },
    {
        "text": "kind of convert them to tensors",
        "start": 379.44,
        "duration": 3.599
    },
    {
        "text": "in this in this case we just do",
        "start": 381.12,
        "duration": 3.28
    },
    {
        "text": "something very simple just",
        "start": 383.039,
        "duration": 4.321
    },
    {
        "text": "take the original pixel values uh kind",
        "start": 384.4,
        "duration": 4.32
    },
    {
        "text": "of like",
        "start": 387.36,
        "duration": 4.0
    },
    {
        "text": "normalize them so they become kind of in",
        "start": 388.72,
        "duration": 4.08
    },
    {
        "text": "the range from zero to one",
        "start": 391.36,
        "duration": 3.36
    },
    {
        "text": "and conversing directly to tensor like",
        "start": 392.8,
        "duration": 3.92
    },
    {
        "text": "for fancier fancy different units you",
        "start": 394.72,
        "duration": 3.36
    },
    {
        "text": "can imagine something like",
        "start": 396.72,
        "duration": 3.28
    },
    {
        "text": "some data augmentation techniques like",
        "start": 398.08,
        "duration": 3.119
    },
    {
        "text": "random cropping or",
        "start": 400.0,
        "duration": 2.96
    },
    {
        "text": "rotations etc are kind of happening here",
        "start": 401.199,
        "duration": 3.361
    },
    {
        "text": "and of course you can like write your",
        "start": 402.96,
        "duration": 2.48
    },
    {
        "text": "own transform",
        "start": 404.56,
        "duration": 3.52
    },
    {
        "text": "as a simple python class as well so by",
        "start": 405.44,
        "duration": 3.28
    },
    {
        "text": "having this",
        "start": 408.08,
        "duration": 2.959
    },
    {
        "text": "data set and as you as you mentioned",
        "start": 408.72,
        "duration": 3.039
    },
    {
        "text": "last time like",
        "start": 411.039,
        "duration": 2.641
    },
    {
        "text": "the data set is pretty much just an",
        "start": 411.759,
        "duration": 4.081
    },
    {
        "text": "abstraction for hey how many elements",
        "start": 413.68,
        "duration": 3.919
    },
    {
        "text": "are there and how do i get",
        "start": 415.84,
        "duration": 4.96
    },
    {
        "text": "uh kind of a data element from it",
        "start": 417.599,
        "duration": 4.961
    },
    {
        "text": "this is a wrapper which knows how to",
        "start": 420.8,
        "duration": 3.04
    },
    {
        "text": "index a mnist",
        "start": 422.56,
        "duration": 3.6
    },
    {
        "text": "file somewhere on my machine i'm just",
        "start": 423.84,
        "duration": 3.12
    },
    {
        "text": "gonna create",
        "start": 426.16,
        "duration": 2.879
    },
    {
        "text": "data loader which is again simple",
        "start": 426.96,
        "duration": 4.16
    },
    {
        "text": "wrapper which takes a data set",
        "start": 429.039,
        "duration": 4.88
    },
    {
        "text": "takes some page size and spans up like a",
        "start": 431.12,
        "duration": 4.479
    },
    {
        "text": "parallel set of parallel workers which",
        "start": 433.919,
        "duration": 2.641
    },
    {
        "text": "gonna pre-process",
        "start": 435.599,
        "duration": 2.961
    },
    {
        "text": "read this data convert the tensors and",
        "start": 436.56,
        "duration": 4.0
    },
    {
        "text": "give me basically back an iterator",
        "start": 438.56,
        "duration": 4.32
    },
    {
        "text": "so i can go through the data and let's",
        "start": 440.56,
        "duration": 4.56
    },
    {
        "text": "define some like visualization details",
        "start": 442.88,
        "duration": 5.439
    },
    {
        "text": "which will come handy later so uh",
        "start": 445.12,
        "duration": 4.96
    },
    {
        "text": "so in terms of data set loader this is",
        "start": 448.319,
        "duration": 3.841
    },
    {
        "text": "basically just just a python class which",
        "start": 450.08,
        "duration": 3.519
    },
    {
        "text": "i can iterate over so i can create",
        "start": 452.16,
        "duration": 2.479
    },
    {
        "text": "iterator over it",
        "start": 453.599,
        "duration": 4.16
    },
    {
        "text": "and let's get let's try to get uh",
        "start": 454.639,
        "duration": 5.201
    },
    {
        "text": "one sample sample data point from it",
        "start": 457.759,
        "duration": 3.521
    },
    {
        "text": "let's say some images",
        "start": 459.84,
        "duration": 3.44
    },
    {
        "text": "and i asked for a batch of like 64. so",
        "start": 461.28,
        "duration": 3.68
    },
    {
        "text": "it's going to be 64 images",
        "start": 463.28,
        "duration": 3.28
    },
    {
        "text": "and visual individualize them with",
        "start": 464.96,
        "duration": 3.12
    },
    {
        "text": "regular multiple sleep",
        "start": 466.56,
        "duration": 3.84
    },
    {
        "text": "looks like some fun pieces of clothes",
        "start": 468.08,
        "duration": 4.08
    },
    {
        "text": "over here and labels would be",
        "start": 470.4,
        "duration": 4.88
    },
    {
        "text": "our would be our kind of correct classes",
        "start": 472.16,
        "duration": 3.599
    },
    {
        "text": "for",
        "start": 475.28,
        "duration": 2.479
    },
    {
        "text": "you for each of those can we scroll up",
        "start": 475.759,
        "duration": 3.201
    },
    {
        "text": "for a second i wanted to",
        "start": 477.759,
        "duration": 3.761
    },
    {
        "text": "see if i understood this correctly so",
        "start": 478.96,
        "duration": 3.44
    },
    {
        "text": "basically",
        "start": 481.52,
        "duration": 4.239
    },
    {
        "text": "the in the the next two statements",
        "start": 482.4,
        "duration": 4.88
    },
    {
        "text": "you're basically loading a training set",
        "start": 485.759,
        "duration": 4.081
    },
    {
        "text": "and a test set from an inbuilt",
        "start": 487.28,
        "duration": 4.8
    },
    {
        "text": "set of data that's already available as",
        "start": 489.84,
        "duration": 3.68
    },
    {
        "text": "data sets and i'm sure there's a bunch",
        "start": 492.08,
        "duration": 3.76
    },
    {
        "text": "of other data sets as well is that right",
        "start": 493.52,
        "duration": 5.04
    },
    {
        "text": "yeah so i mean i don't know whether like",
        "start": 495.84,
        "duration": 4.16
    },
    {
        "text": "autocomplete will work here",
        "start": 498.56,
        "duration": 2.8
    },
    {
        "text": "but yeah there is like i mean this is a",
        "start": 500.0,
        "duration": 3.36
    },
    {
        "text": "number of image data sets which are",
        "start": 501.36,
        "duration": 3.679
    },
    {
        "text": "already bound in torch vision",
        "start": 503.36,
        "duration": 3.2
    },
    {
        "text": "and again like if you have your own data",
        "start": 505.039,
        "duration": 3.681
    },
    {
        "text": "sets you pretty much need to implement a",
        "start": 506.56,
        "duration": 3.68
    },
    {
        "text": "simple python class with these two",
        "start": 508.72,
        "duration": 2.4
    },
    {
        "text": "methods",
        "start": 510.24,
        "duration": 2.4
    },
    {
        "text": "like give me the lens and give me the",
        "start": 511.12,
        "duration": 3.839
    },
    {
        "text": "item or kind of another variant of it we",
        "start": 512.64,
        "duration": 4.24
    },
    {
        "text": "just even support streaming just like",
        "start": 514.959,
        "duration": 4.401
    },
    {
        "text": "implement any any iterator until",
        "start": 516.88,
        "duration": 3.68
    },
    {
        "text": "basically work with",
        "start": 519.36,
        "duration": 3.119
    },
    {
        "text": "data loader or you can use your own data",
        "start": 520.56,
        "duration": 3.44
    },
    {
        "text": "loading libraries if you want as well",
        "start": 522.479,
        "duration": 2.241
    },
    {
        "text": "and you will",
        "start": 524.0,
        "duration": 3.12
    },
    {
        "text": "you can you can find libraries for",
        "start": 524.72,
        "duration": 3.6
    },
    {
        "text": "reading from anything from like",
        "start": 527.12,
        "duration": 2.48
    },
    {
        "text": "databases to",
        "start": 528.32,
        "duration": 3.519
    },
    {
        "text": "you know buckets in like storage buckets",
        "start": 529.6,
        "duration": 4.08
    },
    {
        "text": "and public clouds and stuff like that",
        "start": 531.839,
        "duration": 3.12
    },
    {
        "text": "got it and it's cool that when you're",
        "start": 533.68,
        "duration": 3.04
    },
    {
        "text": "loading them you can actually transform",
        "start": 534.959,
        "duration": 3.041
    },
    {
        "text": "the data as it's",
        "start": 536.72,
        "duration": 3.6
    },
    {
        "text": "it's loading because the transformers go",
        "start": 538.0,
        "duration": 4.399
    },
    {
        "text": "directly into the data set",
        "start": 540.32,
        "duration": 4.24
    },
    {
        "text": "so what's the difference between what's",
        "start": 542.399,
        "duration": 4.161
    },
    {
        "text": "the job of a data loader versus a data",
        "start": 544.56,
        "duration": 3.6
    },
    {
        "text": "set versus a transformer and like a",
        "start": 546.56,
        "duration": 3.2
    },
    {
        "text": "sentence",
        "start": 548.16,
        "duration": 3.92
    },
    {
        "text": "so i mean data set is more like you can",
        "start": 549.76,
        "duration": 3.6
    },
    {
        "text": "think of it like a",
        "start": 552.08,
        "duration": 2.96
    },
    {
        "text": "dictionary semantics right like which",
        "start": 553.36,
        "duration": 3.76
    },
    {
        "text": "which data there is",
        "start": 555.04,
        "duration": 4.479
    },
    {
        "text": "so data loader is more of a kind of",
        "start": 557.12,
        "duration": 3.6
    },
    {
        "text": "multi-threaded worker",
        "start": 559.519,
        "duration": 3.041
    },
    {
        "text": "multi-process worker which would take a",
        "start": 560.72,
        "duration": 3.84
    },
    {
        "text": "data set and like try to",
        "start": 562.56,
        "duration": 4.0
    },
    {
        "text": "sample some items from it in this case",
        "start": 564.56,
        "duration": 3.6
    },
    {
        "text": "there's also with some shuffling",
        "start": 566.56,
        "duration": 4.64
    },
    {
        "text": "uh group them into into batches",
        "start": 568.16,
        "duration": 6.08
    },
    {
        "text": "of a given size and apply",
        "start": 571.2,
        "duration": 4.4
    },
    {
        "text": "and apply transforms which were",
        "start": 574.24,
        "duration": 3.2
    },
    {
        "text": "specified on the data set uh",
        "start": 575.6,
        "duration": 3.6
    },
    {
        "text": "kind of as as a part of those separate",
        "start": 577.44,
        "duration": 4.079
    },
    {
        "text": "multi multi-processing workers",
        "start": 579.2,
        "duration": 4.639
    },
    {
        "text": "so from your actual python training",
        "start": 581.519,
        "duration": 3.44
    },
    {
        "text": "script perspective you",
        "start": 583.839,
        "duration": 2.641
    },
    {
        "text": "you already get like prepared tensors",
        "start": 584.959,
        "duration": 3.041
    },
    {
        "text": "and you don't have to wait",
        "start": 586.48,
        "duration": 3.6
    },
    {
        "text": "in your main process for pre-processing",
        "start": 588.0,
        "duration": 3.04
    },
    {
        "text": "to help",
        "start": 590.08,
        "duration": 2.64
    },
    {
        "text": "to take place so this is kind of like a",
        "start": 591.04,
        "duration": 3.04
    },
    {
        "text": "simple setup which",
        "start": 592.72,
        "duration": 3.84
    },
    {
        "text": "works quite well for a lot of kind of",
        "start": 594.08,
        "duration": 4.24
    },
    {
        "text": "like getting started and",
        "start": 596.56,
        "duration": 3.76
    },
    {
        "text": "probably for like small to mid ski",
        "start": 598.32,
        "duration": 3.68
    },
    {
        "text": "mid-scale data sets i mean for like",
        "start": 600.32,
        "duration": 4.48
    },
    {
        "text": "the most uh like highly scalable use",
        "start": 602.0,
        "duration": 4.079
    },
    {
        "text": "cases i mean if you're probably",
        "start": 604.8,
        "duration": 3.12
    },
    {
        "text": "reading some like proprietary data in a",
        "start": 606.079,
        "duration": 3.2
    },
    {
        "text": "huge scale from",
        "start": 607.92,
        "duration": 3.44
    },
    {
        "text": "your own storage i mean you might as",
        "start": 609.279,
        "duration": 3.761
    },
    {
        "text": "well just implement your own",
        "start": 611.36,
        "duration": 4.8
    },
    {
        "text": "like library for data reading we do",
        "start": 613.04,
        "duration": 4.16
    },
    {
        "text": "whatever you want but",
        "start": 616.16,
        "duration": 2.799
    },
    {
        "text": "kind of data order and data sets usually",
        "start": 617.2,
        "duration": 3.36
    },
    {
        "text": "work quite well for",
        "start": 618.959,
        "duration": 3.841
    },
    {
        "text": "kind of like reasonably reasonably big",
        "start": 620.56,
        "duration": 3.76
    },
    {
        "text": "use cases and kind of give you default",
        "start": 622.8,
        "duration": 4.08
    },
    {
        "text": "performance",
        "start": 624.32,
        "duration": 2.56
    },
    {
        "text": "so let's let's move on to actually",
        "start": 628.079,
        "duration": 3.601
    },
    {
        "text": "network architecture",
        "start": 629.839,
        "duration": 4.961
    },
    {
        "text": "as i described last time we like butch",
        "start": 631.68,
        "duration": 4.48
    },
    {
        "text": "comes with basically a",
        "start": 634.8,
        "duration": 3.599
    },
    {
        "text": "library of standard neural network",
        "start": 636.16,
        "duration": 3.04
    },
    {
        "text": "layers",
        "start": 638.399,
        "duration": 2.641
    },
    {
        "text": "and in this case i'll define like very",
        "start": 639.2,
        "duration": 3.199
    },
    {
        "text": "simple network",
        "start": 641.04,
        "duration": 4.56
    },
    {
        "text": "so i inherited my kind of my class which",
        "start": 642.399,
        "duration": 3.601
    },
    {
        "text": "represents",
        "start": 645.6,
        "duration": 3.6
    },
    {
        "text": "my my network from an module and in this",
        "start": 646.0,
        "duration": 3.6
    },
    {
        "text": "case",
        "start": 649.2,
        "duration": 3.199
    },
    {
        "text": "it will be just for four layers like",
        "start": 649.6,
        "duration": 5.12
    },
    {
        "text": "multi-layer perceptron so i'll just use",
        "start": 652.399,
        "duration": 3.841
    },
    {
        "text": "fully connected linear layers",
        "start": 654.72,
        "duration": 2.88
    },
    {
        "text": "so i'm going to create them with",
        "start": 656.24,
        "duration": 3.599
    },
    {
        "text": "respective sizes and i'm going to tell",
        "start": 657.6,
        "duration": 3.679
    },
    {
        "text": "how like my network is going to be",
        "start": 659.839,
        "duration": 2.961
    },
    {
        "text": "applied to an input tensor",
        "start": 661.279,
        "duration": 3.921
    },
    {
        "text": "so my my input is going to be an image",
        "start": 662.8,
        "duration": 4.4
    },
    {
        "text": "right so it's",
        "start": 665.2,
        "duration": 3.44
    },
    {
        "text": "it's going to be it's going to be like",
        "start": 667.2,
        "duration": 3.44
    },
    {
        "text": "some some dimensions",
        "start": 668.64,
        "duration": 3.84
    },
    {
        "text": "in channel dimension and first dimension",
        "start": 670.64,
        "duration": 3.36
    },
    {
        "text": "being",
        "start": 672.48,
        "duration": 3.52
    },
    {
        "text": "the page size so i can apply my network",
        "start": 674.0,
        "duration": 3.68
    },
    {
        "text": "to multiple images so what i'm saying",
        "start": 676.0,
        "duration": 2.64
    },
    {
        "text": "here is",
        "start": 677.68,
        "duration": 3.44
    },
    {
        "text": "let's take and reshape kind of use this",
        "start": 678.64,
        "duration": 3.439
    },
    {
        "text": "input tensor",
        "start": 681.12,
        "duration": 3.76
    },
    {
        "text": "as it just every pixel hasn't kind of",
        "start": 682.079,
        "duration": 4.0
    },
    {
        "text": "having its own value because we're going",
        "start": 684.88,
        "duration": 2.8
    },
    {
        "text": "to feed it into fully connected layer",
        "start": 686.079,
        "duration": 2.88
    },
    {
        "text": "so i'm just saying that hey i want to",
        "start": 687.68,
        "duration": 3.839
    },
    {
        "text": "keep their batch dimension and all the",
        "start": 688.959,
        "duration": 4.081
    },
    {
        "text": "other dimensions i want to collapse into",
        "start": 691.519,
        "duration": 2.161
    },
    {
        "text": "one",
        "start": 693.04,
        "duration": 3.84
    },
    {
        "text": "so now the x is going to be there",
        "start": 693.68,
        "duration": 4.719
    },
    {
        "text": "like after this x is going to be just",
        "start": 696.88,
        "duration": 2.8
    },
    {
        "text": "two dimensional tensor with batch",
        "start": 698.399,
        "duration": 3.12
    },
    {
        "text": "dimension and kind of all the pixels",
        "start": 699.68,
        "duration": 3.44
    },
    {
        "text": "in one row right and i'm just going to",
        "start": 701.519,
        "duration": 3.281
    },
    {
        "text": "apply my",
        "start": 703.12,
        "duration": 4.88
    },
    {
        "text": "fully connected mlp that's it and those",
        "start": 704.8,
        "duration": 3.92
    },
    {
        "text": "uh like",
        "start": 708.0,
        "duration": 3.279
    },
    {
        "text": "soft soft max and the in the final layer",
        "start": 708.72,
        "duration": 4.64
    },
    {
        "text": "uh to kind of do classification",
        "start": 711.279,
        "duration": 3.601
    },
    {
        "text": "so i can feed it in the classification",
        "start": 713.36,
        "duration": 3.68
    },
    {
        "text": "laws so here's a question and i always i",
        "start": 714.88,
        "duration": 3.68
    },
    {
        "text": "always i'm glad you're here so i can ask",
        "start": 717.04,
        "duration": 3.039
    },
    {
        "text": "you this so basically",
        "start": 718.56,
        "duration": 3.44
    },
    {
        "text": "do you store in the init function you're",
        "start": 720.079,
        "duration": 3.361
    },
    {
        "text": "basically storing",
        "start": 722.0,
        "duration": 2.88
    },
    {
        "text": "the things that store the model",
        "start": 723.44,
        "duration": 4.0
    },
    {
        "text": "parameters and in the forward",
        "start": 724.88,
        "duration": 5.28
    },
    {
        "text": "you actually run the functional stuff",
        "start": 727.44,
        "duration": 3.68
    },
    {
        "text": "because i see that the",
        "start": 730.16,
        "duration": 2.88
    },
    {
        "text": "the activation functions are in there as",
        "start": 731.12,
        "duration": 4.24
    },
    {
        "text": "well as the final activation function",
        "start": 733.04,
        "duration": 4.56
    },
    {
        "text": "is in there as well so basically you",
        "start": 735.36,
        "duration": 3.599
    },
    {
        "text": "store the things that store",
        "start": 737.6,
        "duration": 2.799
    },
    {
        "text": "model parameters in the in it and then",
        "start": 738.959,
        "duration": 3.44
    },
    {
        "text": "the forward you run the function does",
        "start": 740.399,
        "duration": 2.321
    },
    {
        "text": "that",
        "start": 742.399,
        "duration": 2.721
    },
    {
        "text": "is that because yeah because i mean",
        "start": 742.72,
        "duration": 3.919
    },
    {
        "text": "rally in this case for example doesn't",
        "start": 745.12,
        "duration": 3.519
    },
    {
        "text": "actually have",
        "start": 746.639,
        "duration": 3.2
    },
    {
        "text": "doesn't actually have parameters so we",
        "start": 748.639,
        "duration": 3.2
    },
    {
        "text": "can just use the functional version uh",
        "start": 749.839,
        "duration": 3.601
    },
    {
        "text": "however for like something like linear",
        "start": 751.839,
        "duration": 2.961
    },
    {
        "text": "layer there is actually the state",
        "start": 753.44,
        "duration": 2.16
    },
    {
        "text": "associated there",
        "start": 754.8,
        "duration": 3.599
    },
    {
        "text": "there are weights so that's why it's it",
        "start": 755.6,
        "duration": 4.08
    },
    {
        "text": "it makes sense to",
        "start": 758.399,
        "duration": 3.12
    },
    {
        "text": "kind of like store it in a storage",
        "start": 759.68,
        "duration": 3.2
    },
    {
        "text": "explicit in the state and also",
        "start": 761.519,
        "duration": 2.801
    },
    {
        "text": "allows you like i mean if you if you",
        "start": 762.88,
        "duration": 3.199
    },
    {
        "text": "were to to do something like parameter",
        "start": 764.32,
        "duration": 3.519
    },
    {
        "text": "sharing for example so",
        "start": 766.079,
        "duration": 3.44
    },
    {
        "text": "for whatever reason i want to use let's",
        "start": 767.839,
        "duration": 3.041
    },
    {
        "text": "say the same parameters in the first",
        "start": 769.519,
        "duration": 2.641
    },
    {
        "text": "layer and the third layer of neural",
        "start": 770.88,
        "duration": 1.92
    },
    {
        "text": "network",
        "start": 772.16,
        "duration": 3.44
    },
    {
        "text": "i can basically just change it uh",
        "start": 772.8,
        "duration": 4.0
    },
    {
        "text": "something like this and call the same",
        "start": 775.6,
        "duration": 1.679
    },
    {
        "text": "layer",
        "start": 776.8,
        "duration": 2.64
    },
    {
        "text": "multiple times so in this case first",
        "start": 777.279,
        "duration": 3.441
    },
    {
        "text": "insert layer will actually share the",
        "start": 779.44,
        "duration": 2.72
    },
    {
        "text": "same parameters but they will be",
        "start": 780.72,
        "duration": 3.44
    },
    {
        "text": "applied multiple times in the network i",
        "start": 782.16,
        "duration": 3.119
    },
    {
        "text": "mean",
        "start": 784.16,
        "duration": 2.56
    },
    {
        "text": "it's not really useful in in this",
        "start": 785.279,
        "duration": 2.881
    },
    {
        "text": "particular case but might be useful in",
        "start": 786.72,
        "duration": 3.119
    },
    {
        "text": "other cases",
        "start": 788.16,
        "duration": 3.919
    },
    {
        "text": "so it's this is just a python object",
        "start": 789.839,
        "duration": 3.12
    },
    {
        "text": "right so we can",
        "start": 792.079,
        "duration": 3.041
    },
    {
        "text": "uh you can effectively just create",
        "start": 792.959,
        "duration": 3.44
    },
    {
        "text": "create our model we can",
        "start": 795.12,
        "duration": 3.76
    },
    {
        "text": "move our model to gpu and it will",
        "start": 796.399,
        "duration": 3.601
    },
    {
        "text": "actually go and like move",
        "start": 798.88,
        "duration": 2.88
    },
    {
        "text": "every parameter of the of the model to",
        "start": 800.0,
        "duration": 3.92
    },
    {
        "text": "gpu so the cl i mean the classes",
        "start": 801.76,
        "duration": 3.84
    },
    {
        "text": "class basically ends up being just a",
        "start": 803.92,
        "duration": 4.0
    },
    {
        "text": "python class but if i want to look at",
        "start": 805.6,
        "duration": 5.44
    },
    {
        "text": "uh model.fc1.wait it will be like a",
        "start": 807.92,
        "duration": 4.719
    },
    {
        "text": "tensor leaving a gpu",
        "start": 811.04,
        "duration": 4.72
    },
    {
        "text": "and similarly i can i can like define my",
        "start": 812.639,
        "duration": 6.161
    },
    {
        "text": "uh my loss and my optimizer in this case",
        "start": 815.76,
        "duration": 4.96
    },
    {
        "text": "i'm just",
        "start": 818.8,
        "duration": 3.68
    },
    {
        "text": "i'm just taking like simple gloss for",
        "start": 820.72,
        "duration": 4.0
    },
    {
        "text": "classification and i'm also taking",
        "start": 822.48,
        "duration": 4.159
    },
    {
        "text": "like the simplest possible optimizer",
        "start": 824.72,
        "duration": 3.919
    },
    {
        "text": "just vanilla sgd i mean again you have",
        "start": 826.639,
        "duration": 2.401
    },
    {
        "text": "like",
        "start": 828.639,
        "duration": 2.56
    },
    {
        "text": "all uh all the other kinds from like",
        "start": 829.04,
        "duration": 3.919
    },
    {
        "text": "adam to other great to all the",
        "start": 831.199,
        "duration": 4.801
    },
    {
        "text": "uh all the fancier options and to enter",
        "start": 832.959,
        "duration": 4.081
    },
    {
        "text": "your question",
        "start": 836.0,
        "duration": 3.76
    },
    {
        "text": "about uh visualizing parameters right we",
        "start": 837.04,
        "duration": 4.56
    },
    {
        "text": "can we can do something like",
        "start": 839.76,
        "duration": 4.879
    },
    {
        "text": "model fc1.wait",
        "start": 841.6,
        "duration": 4.64
    },
    {
        "text": "right and it will be just a tensor i",
        "start": 844.639,
        "duration": 3.44
    },
    {
        "text": "mean because we just created our model",
        "start": 846.24,
        "duration": 3.92
    },
    {
        "text": "it's going to be randomly randomly",
        "start": 848.079,
        "duration": 3.12
    },
    {
        "text": "initialized",
        "start": 850.16,
        "duration": 3.2
    },
    {
        "text": "uh even even several gpu so you can see",
        "start": 851.199,
        "duration": 3.281
    },
    {
        "text": "that it leaves on",
        "start": 853.36,
        "duration": 5.12
    },
    {
        "text": "like gpu zero and also because it's",
        "start": 854.48,
        "duration": 6.56
    },
    {
        "text": "by default it has like requires grad",
        "start": 858.48,
        "duration": 3.68
    },
    {
        "text": "equals true which means that",
        "start": 861.04,
        "duration": 2.72
    },
    {
        "text": "if i'm gonna do some operations with",
        "start": 862.16,
        "duration": 3.2
    },
    {
        "text": "this parameter the",
        "start": 863.76,
        "duration": 3.12
    },
    {
        "text": "actuator graph is gonna remember those",
        "start": 865.36,
        "duration": 3.2
    },
    {
        "text": "operations so i can do backwards later",
        "start": 866.88,
        "duration": 2.56
    },
    {
        "text": "on",
        "start": 868.56,
        "duration": 3.6
    },
    {
        "text": "so actually let's let's try that so as",
        "start": 869.44,
        "duration": 3.36
    },
    {
        "text": "before we",
        "start": 872.16,
        "duration": 3.52
    },
    {
        "text": "gonna fetch one like one batch of images",
        "start": 872.8,
        "duration": 4.24
    },
    {
        "text": "and one batch of labels",
        "start": 875.68,
        "duration": 3.44
    },
    {
        "text": "we're gonna move our images to gpu",
        "start": 877.04,
        "duration": 4.239
    },
    {
        "text": "because our model is there",
        "start": 879.12,
        "duration": 3.76
    },
    {
        "text": "you're gonna basically apply the model",
        "start": 881.279,
        "duration": 2.8
    },
    {
        "text": "to the inputs so it's called",
        "start": 882.88,
        "duration": 3.36
    },
    {
        "text": "model.forward",
        "start": 884.079,
        "duration": 5.841
    },
    {
        "text": "uh compute our like predicted uh",
        "start": 886.24,
        "duration": 6.88
    },
    {
        "text": "pre predicts of max outputs and feeds",
        "start": 889.92,
        "duration": 3.919
    },
    {
        "text": "them into",
        "start": 893.12,
        "duration": 3.12
    },
    {
        "text": "our log loss criteria which we defined",
        "start": 893.839,
        "duration": 3.601
    },
    {
        "text": "and then we're going to say",
        "start": 896.24,
        "duration": 4.48
    },
    {
        "text": "loss.backwards so effectively doing like",
        "start": 897.44,
        "duration": 4.959
    },
    {
        "text": "false operations which happened inside",
        "start": 900.72,
        "duration": 4.08
    },
    {
        "text": "the model autograph was remembering",
        "start": 902.399,
        "duration": 3.68
    },
    {
        "text": "which operations were there like which",
        "start": 904.8,
        "duration": 2.719
    },
    {
        "text": "matrix multiplication so like real",
        "start": 906.079,
        "duration": 2.56
    },
    {
        "text": "application cetera and",
        "start": 907.519,
        "duration": 3.12
    },
    {
        "text": "lost the backwards is gonna walk",
        "start": 908.639,
        "duration": 3.041
    },
    {
        "text": "backwards this",
        "start": 910.639,
        "duration": 2.961
    },
    {
        "text": "kind of tape this graph of operations",
        "start": 911.68,
        "duration": 3.36
    },
    {
        "text": "which happened and",
        "start": 913.6,
        "duration": 5.039
    },
    {
        "text": "uh assigned like compute the gradient",
        "start": 915.04,
        "duration": 5.919
    },
    {
        "text": "values and assigned for every parameter",
        "start": 918.639,
        "duration": 3.041
    },
    {
        "text": "dot grad",
        "start": 920.959,
        "duration": 2.641
    },
    {
        "text": "the actual tensorflow should be what",
        "start": 921.68,
        "duration": 3.36
    },
    {
        "text": "should be the gradients",
        "start": 923.6,
        "duration": 3.919
    },
    {
        "text": "for this input starting from this",
        "start": 925.04,
        "duration": 3.599
    },
    {
        "text": "starting from this loss",
        "start": 927.519,
        "duration": 3.521
    },
    {
        "text": "so for example if i run it i can see",
        "start": 928.639,
        "duration": 3.921
    },
    {
        "text": "that like now my",
        "start": 931.04,
        "duration": 4.0
    },
    {
        "text": "fully connected layer weight parameter",
        "start": 932.56,
        "duration": 4.0
    },
    {
        "text": "has the graph attribute which is tender",
        "start": 935.04,
        "duration": 2.88
    },
    {
        "text": "by itself of the same size",
        "start": 936.56,
        "duration": 5.04
    },
    {
        "text": "which are like some gradients of this",
        "start": 937.92,
        "duration": 5.359
    },
    {
        "text": "of the standard for this inputs and",
        "start": 941.6,
        "duration": 3.28
    },
    {
        "text": "every time when i run it on different",
        "start": 943.279,
        "duration": 2.8
    },
    {
        "text": "inputs of course gradients will change",
        "start": 944.88,
        "duration": 2.399
    },
    {
        "text": "and parameters will change because it",
        "start": 946.079,
        "duration": 1.76
    },
    {
        "text": "will be",
        "start": 947.279,
        "duration": 2.721
    },
    {
        "text": "uh i'll be calling like actual optimizer",
        "start": 947.839,
        "duration": 3.841
    },
    {
        "text": "later i'm just showing that like it's",
        "start": 950.0,
        "duration": 2.32
    },
    {
        "text": "pretty",
        "start": 951.68,
        "duration": 2.64
    },
    {
        "text": "uh componentized and you can use",
        "start": 952.32,
        "duration": 3.12
    },
    {
        "text": "individual pieces",
        "start": 954.32,
        "duration": 3.759
    },
    {
        "text": "uh independently if you wanted them",
        "start": 955.44,
        "duration": 4.0
    },
    {
        "text": "that's cool",
        "start": 958.079,
        "duration": 3.76
    },
    {
        "text": "so uh for i mean for to make our",
        "start": 959.44,
        "duration": 3.68
    },
    {
        "text": "training look a little bit more fun",
        "start": 961.839,
        "duration": 2.161
    },
    {
        "text": "let's start with some",
        "start": 963.12,
        "duration": 4.48
    },
    {
        "text": "visualization so pythagorean actually",
        "start": 964.0,
        "duration": 4.72
    },
    {
        "text": "has built-in integration with",
        "start": 967.6,
        "duration": 2.4
    },
    {
        "text": "tensorboard which is like this",
        "start": 968.72,
        "duration": 3.84
    },
    {
        "text": "great project built uh built by google",
        "start": 970.0,
        "duration": 3.68
    },
    {
        "text": "which is actually independent from",
        "start": 972.56,
        "duration": 2.079
    },
    {
        "text": "tensorflow",
        "start": 973.68,
        "duration": 3.12
    },
    {
        "text": "so you can you can use it for kind of",
        "start": 974.639,
        "duration": 4.0
    },
    {
        "text": "visualizing graphs",
        "start": 976.8,
        "duration": 4.24
    },
    {
        "text": "design kind of learning learning process",
        "start": 978.639,
        "duration": 3.921
    },
    {
        "text": "like some fancier like convenience",
        "start": 981.04,
        "duration": 3.039
    },
    {
        "text": "visualizations or images and stuff like",
        "start": 982.56,
        "duration": 2.24
    },
    {
        "text": "that",
        "start": 984.079,
        "duration": 4.081
    },
    {
        "text": "so let's set up some tender board",
        "start": 984.8,
        "duration": 5.599
    },
    {
        "text": "let's also actually look at how those",
        "start": 988.16,
        "duration": 4.4
    },
    {
        "text": "images which we fit in our",
        "start": 990.399,
        "duration": 4.0
    },
    {
        "text": "feed in our training look like i mean",
        "start": 992.56,
        "duration": 4.16
    },
    {
        "text": "pretty much uh",
        "start": 994.399,
        "duration": 3.44
    },
    {
        "text": "pretty nicely especially after",
        "start": 996.72,
        "duration": 2.239
    },
    {
        "text": "normalization you can see that like",
        "start": 997.839,
        "duration": 2.321
    },
    {
        "text": "background became white",
        "start": 998.959,
        "duration": 4.481
    },
    {
        "text": "and it's just single scale image now",
        "start": 1000.16,
        "duration": 7.2
    },
    {
        "text": "uh so here comes the interesting part",
        "start": 1003.44,
        "duration": 4.399
    },
    {
        "text": "which is",
        "start": 1007.36,
        "duration": 3.839
    },
    {
        "text": "actually our training loop and python",
        "start": 1007.839,
        "duration": 5.201
    },
    {
        "text": "core library itself basically just gives",
        "start": 1011.199,
        "duration": 2.241
    },
    {
        "text": "you",
        "start": 1013.04,
        "duration": 1.919
    },
    {
        "text": "bits and pieces so you can build your",
        "start": 1013.44,
        "duration": 2.959
    },
    {
        "text": "own training loop with whatever",
        "start": 1014.959,
        "duration": 2.641
    },
    {
        "text": "flexibility you want",
        "start": 1016.399,
        "duration": 2.8
    },
    {
        "text": "and that's what we're going to show here",
        "start": 1017.6,
        "duration": 3.599
    },
    {
        "text": "of course if you're building your",
        "start": 1019.199,
        "duration": 3.361
    },
    {
        "text": "actual application or like if you're",
        "start": 1021.199,
        "duration": 2.961
    },
    {
        "text": "using some higher level framework",
        "start": 1022.56,
        "duration": 3.04
    },
    {
        "text": "usually you would be using some of the",
        "start": 1024.16,
        "duration": 2.96
    },
    {
        "text": "kind of libraries on top",
        "start": 1025.6,
        "duration": 3.359
    },
    {
        "text": "for example peter lightning or python",
        "start": 1027.12,
        "duration": 3.6
    },
    {
        "text": "ignite are kind of two popular choices",
        "start": 1028.959,
        "duration": 2.641
    },
    {
        "text": "for",
        "start": 1030.72,
        "duration": 2.959
    },
    {
        "text": "obstructing out uh a lot of boilerplate",
        "start": 1031.6,
        "duration": 3.28
    },
    {
        "text": "from the training loop so they're kind",
        "start": 1033.679,
        "duration": 2.24
    },
    {
        "text": "of like",
        "start": 1034.88,
        "duration": 3.36
    },
    {
        "text": "the logic which we spell out here kind",
        "start": 1035.919,
        "duration": 3.92
    },
    {
        "text": "of uh",
        "start": 1038.24,
        "duration": 3.36
    },
    {
        "text": "behind there behind the scenes and",
        "start": 1039.839,
        "duration": 3.2
    },
    {
        "text": "provide a lot of",
        "start": 1041.6,
        "duration": 4.239
    },
    {
        "text": "kind of tweaks and configurations to for",
        "start": 1043.039,
        "duration": 3.92
    },
    {
        "text": "the common situations",
        "start": 1045.839,
        "duration": 3.521
    },
    {
        "text": "but in this for this demonstration let's",
        "start": 1046.959,
        "duration": 3.521
    },
    {
        "text": "actually build our own",
        "start": 1049.36,
        "duration": 3.6
    },
    {
        "text": "so what we're going to do we're going to",
        "start": 1050.48,
        "duration": 3.76
    },
    {
        "text": "take several epochs",
        "start": 1052.96,
        "duration": 2.48
    },
    {
        "text": "right so we're going to go through our",
        "start": 1054.24,
        "duration": 3.36
    },
    {
        "text": "training data multiple times",
        "start": 1055.44,
        "duration": 4.8
    },
    {
        "text": "and every time i'm going to just take my",
        "start": 1057.6,
        "duration": 3.76
    },
    {
        "text": "input loader which is going to be",
        "start": 1060.24,
        "duration": 2.64
    },
    {
        "text": "shuffled every time",
        "start": 1061.36,
        "duration": 3.92
    },
    {
        "text": "take some images and labels just as as",
        "start": 1062.88,
        "duration": 3.12
    },
    {
        "text": "we did before",
        "start": 1065.28,
        "duration": 4.16
    },
    {
        "text": "right run my model calculate loss",
        "start": 1066.0,
        "duration": 6.16
    },
    {
        "text": "uh prepare optimizer call lost the",
        "start": 1069.44,
        "duration": 4.4
    },
    {
        "text": "backwards and the optimization step",
        "start": 1072.16,
        "duration": 3.519
    },
    {
        "text": "which is going to do it sdg step",
        "start": 1073.84,
        "duration": 4.16
    },
    {
        "text": "and let's do like some of our kind of",
        "start": 1075.679,
        "duration": 4.641
    },
    {
        "text": "manual matrix computation uh",
        "start": 1078.0,
        "duration": 4.88
    },
    {
        "text": "which is uh which is just like a running",
        "start": 1080.32,
        "duration": 3.359
    },
    {
        "text": "loss",
        "start": 1082.88,
        "duration": 4.159
    },
    {
        "text": "as we run it so yeah if i run the cell",
        "start": 1083.679,
        "duration": 4.481
    },
    {
        "text": "you can see that",
        "start": 1087.039,
        "duration": 3.681
    },
    {
        "text": "it's running i'm also after every epoch",
        "start": 1088.16,
        "duration": 4.48
    },
    {
        "text": "i just spelled out that explicitly",
        "start": 1090.72,
        "duration": 4.8
    },
    {
        "text": "i'm gonna take the my test set and i'm",
        "start": 1092.64,
        "duration": 3.919
    },
    {
        "text": "gonna run",
        "start": 1095.52,
        "duration": 2.88
    },
    {
        "text": "run my model on it and compute kind of",
        "start": 1096.559,
        "duration": 3.681
    },
    {
        "text": "like maximum class which is what",
        "start": 1098.4,
        "duration": 2.32
    },
    {
        "text": "actually",
        "start": 1100.24,
        "duration": 3.2
    },
    {
        "text": "our model predicts for every image and",
        "start": 1100.72,
        "duration": 3.04
    },
    {
        "text": "try",
        "start": 1103.44,
        "duration": 2.479
    },
    {
        "text": "and just spell it out manually in python",
        "start": 1103.76,
        "duration": 4.0
    },
    {
        "text": "compute like",
        "start": 1105.919,
        "duration": 3.841
    },
    {
        "text": "whether the model got it right or not",
        "start": 1107.76,
        "duration": 3.2
    },
    {
        "text": "and compute the accuracy which is",
        "start": 1109.76,
        "duration": 2.72
    },
    {
        "text": "basically just like the ratio of images",
        "start": 1110.96,
        "duration": 3.44
    },
    {
        "text": "which our model classifies correctly",
        "start": 1112.48,
        "duration": 4.4
    },
    {
        "text": "and as you can see here i mean we our",
        "start": 1114.4,
        "duration": 3.84
    },
    {
        "text": "model trains right so",
        "start": 1116.88,
        "duration": 3.36
    },
    {
        "text": "loss training loss goes down our test",
        "start": 1118.24,
        "duration": 3.04
    },
    {
        "text": "accuracy goes up",
        "start": 1120.24,
        "duration": 2.72
    },
    {
        "text": "i mean if i keep it keep it training it",
        "start": 1121.28,
        "duration": 3.36
    },
    {
        "text": "will probably reach because it's",
        "start": 1122.96,
        "duration": 3.92
    },
    {
        "text": "the simple model maybe like around 90",
        "start": 1124.64,
        "duration": 3.36
    },
    {
        "text": "acres or something like this i mean",
        "start": 1126.88,
        "duration": 2.56
    },
    {
        "text": "again it's a very simple data set so",
        "start": 1128.0,
        "duration": 2.799
    },
    {
        "text": "even",
        "start": 1129.44,
        "duration": 3.119
    },
    {
        "text": "even a very simple multi-layer",
        "start": 1130.799,
        "duration": 4.561
    },
    {
        "text": "perceptron trains pretty well",
        "start": 1132.559,
        "duration": 4.24
    },
    {
        "text": "yeah let's hold it up here because i",
        "start": 1135.36,
        "duration": 3.6
    },
    {
        "text": "want to make sure everyone i can see",
        "start": 1136.799,
        "duration": 4.88
    },
    {
        "text": "what's going on so basically it's",
        "start": 1138.96,
        "duration": 3.28
    },
    {
        "text": "literally",
        "start": 1141.679,
        "duration": 3.601
    },
    {
        "text": "just the training loop and the",
        "start": 1142.24,
        "duration": 5.52
    },
    {
        "text": "five important things in the training is",
        "start": 1145.28,
        "duration": 4.32
    },
    {
        "text": "obviously you need to call the model",
        "start": 1147.76,
        "duration": 4.56
    },
    {
        "text": "you need to calculate the loss and then",
        "start": 1149.6,
        "duration": 4.16
    },
    {
        "text": "this is an interesting thing",
        "start": 1152.32,
        "duration": 3.52
    },
    {
        "text": "why do we have to do zero grad on the",
        "start": 1153.76,
        "duration": 4.08
    },
    {
        "text": "optum optimizer because the optimizer in",
        "start": 1155.84,
        "duration": 3.36
    },
    {
        "text": "my sense is that the optimizer has all",
        "start": 1157.84,
        "duration": 3.68
    },
    {
        "text": "of the model parameters sort of built in",
        "start": 1159.2,
        "duration": 4.8
    },
    {
        "text": "um yes yeah so i mean this is more like",
        "start": 1161.52,
        "duration": 3.279
    },
    {
        "text": "an explicit",
        "start": 1164.0,
        "duration": 2.88
    },
    {
        "text": "an inexplicit convention if you like if",
        "start": 1164.799,
        "duration": 3.841
    },
    {
        "text": "you use this kind of efforts api it",
        "start": 1166.88,
        "duration": 2.88
    },
    {
        "text": "actually",
        "start": 1168.64,
        "duration": 2.64
    },
    {
        "text": "in some cases you might want to call the",
        "start": 1169.76,
        "duration": 3.039
    },
    {
        "text": "backwards multiple times because you",
        "start": 1171.28,
        "duration": 3.12
    },
    {
        "text": "might have like multi-task learning",
        "start": 1172.799,
        "duration": 2.961
    },
    {
        "text": "you might have multiple losses so you",
        "start": 1174.4,
        "duration": 3.2
    },
    {
        "text": "want to actually add gradients for those",
        "start": 1175.76,
        "duration": 2.48
    },
    {
        "text": "together",
        "start": 1177.6,
        "duration": 2.319
    },
    {
        "text": "so that's why kind of in canonical",
        "start": 1178.24,
        "duration": 3.04
    },
    {
        "text": "patterns you need to call",
        "start": 1179.919,
        "duration": 4.0
    },
    {
        "text": "uh zero zero graph explicitly to say",
        "start": 1181.28,
        "duration": 4.0
    },
    {
        "text": "like hey i want to actually like discard",
        "start": 1183.919,
        "duration": 2.801
    },
    {
        "text": "my previous gradients and compute them",
        "start": 1185.28,
        "duration": 2.0
    },
    {
        "text": "once",
        "start": 1186.72,
        "duration": 3.36
    },
    {
        "text": "if i didn't have this then gradients",
        "start": 1187.28,
        "duration": 4.0
    },
    {
        "text": "from every dot backward call would be",
        "start": 1190.08,
        "duration": 2.719
    },
    {
        "text": "like kind of accumulated together just",
        "start": 1191.28,
        "duration": 2.8
    },
    {
        "text": "like added together",
        "start": 1192.799,
        "duration": 2.801
    },
    {
        "text": "which is useful in some cases but for",
        "start": 1194.08,
        "duration": 3.2
    },
    {
        "text": "like canonical uh",
        "start": 1195.6,
        "duration": 2.88
    },
    {
        "text": "for the canonical neural network",
        "start": 1197.28,
        "duration": 2.639
    },
    {
        "text": "training you just want to clear them up",
        "start": 1198.48,
        "duration": 2.559
    },
    {
        "text": "every time",
        "start": 1199.919,
        "duration": 3.441
    },
    {
        "text": "so where because i if we're looking at",
        "start": 1201.039,
        "duration": 3.681
    },
    {
        "text": "like for example i think you were using",
        "start": 1203.36,
        "duration": 3.199
    },
    {
        "text": "stochastic gradient descent",
        "start": 1204.72,
        "duration": 4.079
    },
    {
        "text": "where is the subtraction of the",
        "start": 1206.559,
        "duration": 4.161
    },
    {
        "text": "gradients of the model parameters",
        "start": 1208.799,
        "duration": 3.041
    },
    {
        "text": "happening",
        "start": 1210.72,
        "duration": 2.24
    },
    {
        "text": "on the model parameter you know what i'm",
        "start": 1211.84,
        "duration": 2.0
    },
    {
        "text": "talking about because you got to",
        "start": 1212.96,
        "duration": 2.24
    },
    {
        "text": "subtract a scale",
        "start": 1213.84,
        "duration": 3.199
    },
    {
        "text": "times the learning rate and the yeah so",
        "start": 1215.2,
        "duration": 3.2
    },
    {
        "text": "that's basically what like that's",
        "start": 1217.039,
        "duration": 3.361
    },
    {
        "text": "basically what optimizer optimizer.step",
        "start": 1218.4,
        "duration": 3.759
    },
    {
        "text": "does because we create i mean because we",
        "start": 1220.4,
        "duration": 3.759
    },
    {
        "text": "created i mean in this example we",
        "start": 1222.159,
        "duration": 2.841
    },
    {
        "text": "created",
        "start": 1224.159,
        "duration": 3.041
    },
    {
        "text": "optimizer.sgd right so that's what it",
        "start": 1225.0,
        "duration": 3.4
    },
    {
        "text": "does an implementation",
        "start": 1227.2,
        "duration": 2.719
    },
    {
        "text": "i mean if we wanted to do this example",
        "start": 1228.4,
        "duration": 3.279
    },
    {
        "text": "even more kind of low level",
        "start": 1229.919,
        "duration": 3.12
    },
    {
        "text": "we could have written here something",
        "start": 1231.679,
        "duration": 3.12
    },
    {
        "text": "like iterate over every",
        "start": 1233.039,
        "duration": 4.241
    },
    {
        "text": "modal parameter and basically do",
        "start": 1234.799,
        "duration": 3.601
    },
    {
        "text": "something like focus sounds like for",
        "start": 1237.28,
        "duration": 2.48
    },
    {
        "text": "param",
        "start": 1238.4,
        "duration": 5.519
    },
    {
        "text": "in model parameters",
        "start": 1239.76,
        "duration": 4.159
    },
    {
        "text": "do something like parameters equals you",
        "start": 1244.799,
        "duration": 4.481
    },
    {
        "text": "know param dot grad",
        "start": 1247.919,
        "duration": 3.601
    },
    {
        "text": "something something right like right",
        "start": 1249.28,
        "duration": 3.759
    },
    {
        "text": "times like learning rate or whatever",
        "start": 1251.52,
        "duration": 3.92
    },
    {
        "text": "so that i mean that would work that",
        "start": 1253.039,
        "duration": 3.681
    },
    {
        "text": "something like that would work too but",
        "start": 1255.44,
        "duration": 3.2
    },
    {
        "text": "in this case i'm just calling the higher",
        "start": 1256.72,
        "duration": 4.48
    },
    {
        "text": "level api which kind of abstracts it out",
        "start": 1258.64,
        "duration": 4.8
    },
    {
        "text": "awesome and so that's why for example",
        "start": 1261.2,
        "duration": 3.44
    },
    {
        "text": "when you're doing the",
        "start": 1263.44,
        "duration": 4.56
    },
    {
        "text": "the the test part you say no gradient",
        "start": 1264.64,
        "duration": 5.12
    },
    {
        "text": "because you just don't want those",
        "start": 1268.0,
        "duration": 3.2
    },
    {
        "text": "you don't want those calculations to",
        "start": 1269.76,
        "duration": 2.72
    },
    {
        "text": "even be done because it doesn't matter",
        "start": 1271.2,
        "duration": 2.24
    },
    {
        "text": "yeah so i mean actually",
        "start": 1272.48,
        "duration": 2.96
    },
    {
        "text": "i mean i don't have to call no gradient",
        "start": 1273.44,
        "duration": 3.68
    },
    {
        "text": "uh per se i mean like because if i don't",
        "start": 1275.44,
        "duration": 3.04
    },
    {
        "text": "call backwards it would",
        "start": 1277.12,
        "duration": 3.039
    },
    {
        "text": "i mean it would actually will not call",
        "start": 1278.48,
        "duration": 3.36
    },
    {
        "text": "backwards pass of the of the model",
        "start": 1280.159,
        "duration": 3.201
    },
    {
        "text": "i mean it still help i mean like it's",
        "start": 1281.84,
        "duration": 2.959
    },
    {
        "text": "still helpful to do this because it",
        "start": 1283.36,
        "duration": 3.12
    },
    {
        "text": "disables tracking of autograd so",
        "start": 1284.799,
        "duration": 4.321
    },
    {
        "text": "maybe it has like small performance",
        "start": 1286.48,
        "duration": 3.52
    },
    {
        "text": "benefit because",
        "start": 1289.12,
        "duration": 2.0
    },
    {
        "text": "better doesn't need to remember like",
        "start": 1290.0,
        "duration": 2.799
    },
    {
        "text": "which operations kind of happened and",
        "start": 1291.12,
        "duration": 2.559
    },
    {
        "text": "forward",
        "start": 1292.799,
        "duration": 2.561
    },
    {
        "text": "uh because it knows that like backwards",
        "start": 1293.679,
        "duration": 3.441
    },
    {
        "text": "will not be called uh however",
        "start": 1295.36,
        "duration": 3.92
    },
    {
        "text": "however even without it it would work",
        "start": 1297.12,
        "duration": 3.76
    },
    {
        "text": "just fine because as you can see we just",
        "start": 1299.28,
        "duration": 2.16
    },
    {
        "text": "don't call",
        "start": 1300.88,
        "duration": 3.44
    },
    {
        "text": "that backwards uh for our loss i mean we",
        "start": 1301.44,
        "duration": 4.479
    },
    {
        "text": "don't even compute loss for",
        "start": 1304.32,
        "duration": 4.16
    },
    {
        "text": "uh for for the test set evaluation we",
        "start": 1305.919,
        "duration": 4.801
    },
    {
        "text": "just i don't know why",
        "start": 1308.48,
        "duration": 3.12
    },
    {
        "text": "sorry about that and then the last",
        "start": 1310.72,
        "duration": 2.8
    },
    {
        "text": "question you explicitly set the model to",
        "start": 1311.6,
        "duration": 2.88
    },
    {
        "text": "eval",
        "start": 1313.52,
        "duration": 4.08
    },
    {
        "text": "versus train what does that do yeah so",
        "start": 1314.48,
        "duration": 3.6
    },
    {
        "text": "this is",
        "start": 1317.6,
        "duration": 2.88
    },
    {
        "text": "more like switches are set in uh kind of",
        "start": 1318.08,
        "duration": 4.0
    },
    {
        "text": "like global setting on the model so",
        "start": 1320.48,
        "duration": 2.16
    },
    {
        "text": "there's a",
        "start": 1322.08,
        "duration": 2.0
    },
    {
        "text": "parameter called like model",
        "start": 1322.64,
        "duration": 3.279
    },
    {
        "text": "model.training on like on every module",
        "start": 1324.08,
        "duration": 3.2
    },
    {
        "text": "there is like a field",
        "start": 1325.919,
        "duration": 2.561
    },
    {
        "text": "field like this so it's basically the",
        "start": 1327.28,
        "duration": 3.44
    },
    {
        "text": "same as like saying saying something",
        "start": 1328.48,
        "duration": 2.8
    },
    {
        "text": "like this",
        "start": 1330.72,
        "duration": 2.48
    },
    {
        "text": "model training equals true actually for",
        "start": 1331.28,
        "duration": 3.759
    },
    {
        "text": "this model we don't have to do it",
        "start": 1333.2,
        "duration": 4.0
    },
    {
        "text": "there are some modules which do differ",
        "start": 1335.039,
        "duration": 4.081
    },
    {
        "text": "between evaluation model",
        "start": 1337.2,
        "duration": 3.839
    },
    {
        "text": "mode and training mode i mean most",
        "start": 1339.12,
        "duration": 3.919
    },
    {
        "text": "obvious example is dropout",
        "start": 1341.039,
        "duration": 4.64
    },
    {
        "text": "or like batch norm right where where",
        "start": 1343.039,
        "duration": 3.841
    },
    {
        "text": "like for example for dropout and",
        "start": 1345.679,
        "duration": 2.641
    },
    {
        "text": "training you would want to generate like",
        "start": 1346.88,
        "duration": 3.44
    },
    {
        "text": "random masks and apply it to",
        "start": 1348.32,
        "duration": 5.04
    },
    {
        "text": "uh apply to the model uh and during the",
        "start": 1350.32,
        "duration": 4.56
    },
    {
        "text": "evaluation you actually just correct for",
        "start": 1353.36,
        "duration": 2.16
    },
    {
        "text": "that by",
        "start": 1354.88,
        "duration": 2.96
    },
    {
        "text": "multiplying by some global rate because",
        "start": 1355.52,
        "duration": 3.279
    },
    {
        "text": "you don't want to",
        "start": 1357.84,
        "duration": 3.76
    },
    {
        "text": "randomly disable some of the neurons but",
        "start": 1358.799,
        "duration": 4.641
    },
    {
        "text": "in our case we actually don't use any of",
        "start": 1361.6,
        "duration": 3.76
    },
    {
        "text": "those modules in our model so",
        "start": 1363.44,
        "duration": 3.2
    },
    {
        "text": "technically you don't have to do that",
        "start": 1365.36,
        "duration": 2.559
    },
    {
        "text": "but i'm just showing it for",
        "start": 1366.64,
        "duration": 4.0
    },
    {
        "text": "general kind of general completeness",
        "start": 1367.919,
        "duration": 3.201
    },
    {
        "text": "love it",
        "start": 1370.64,
        "duration": 2.399
    },
    {
        "text": "all right let's keep going my friend",
        "start": 1371.12,
        "duration": 3.2
    },
    {
        "text": "yeah so",
        "start": 1373.039,
        "duration": 3.681
    },
    {
        "text": "i mean we as you as you might have",
        "start": 1374.32,
        "duration": 3.76
    },
    {
        "text": "noticed we were using this like",
        "start": 1376.72,
        "duration": 2.64
    },
    {
        "text": "tensorboard writer",
        "start": 1378.08,
        "duration": 3.68
    },
    {
        "text": "along the way to add our training stereo",
        "start": 1379.36,
        "duration": 4.0
    },
    {
        "text": "statistic here so let's",
        "start": 1381.76,
        "duration": 3.2
    },
    {
        "text": "uh let's actually try to launch",
        "start": 1383.36,
        "duration": 3.28
    },
    {
        "text": "tensorboard and it can",
        "start": 1384.96,
        "duration": 3.36
    },
    {
        "text": "can be launched as a separate web page",
        "start": 1386.64,
        "duration": 3.68
    },
    {
        "text": "but also it has pretty cool integration",
        "start": 1388.32,
        "duration": 3.839
    },
    {
        "text": "inside ipython notebook",
        "start": 1390.32,
        "duration": 5.04
    },
    {
        "text": "so uh i can i can see um i can see some",
        "start": 1392.159,
        "duration": 3.921
    },
    {
        "text": "of the like our",
        "start": 1395.36,
        "duration": 2.08
    },
    {
        "text": "training graph which is not very",
        "start": 1396.08,
        "duration": 2.88
    },
    {
        "text": "interesting but it goes down which is",
        "start": 1397.44,
        "duration": 2.16
    },
    {
        "text": "good",
        "start": 1398.96,
        "duration": 3.36
    },
    {
        "text": "uh which we locked over there we also",
        "start": 1399.6,
        "duration": 3.28
    },
    {
        "text": "logged",
        "start": 1402.32,
        "duration": 2.32
    },
    {
        "text": "about some like some of the sample",
        "start": 1402.88,
        "duration": 4.24
    },
    {
        "text": "images so there's visualization for that",
        "start": 1404.64,
        "duration": 5.6
    },
    {
        "text": "uh also somewhere above we logged their",
        "start": 1407.12,
        "duration": 5.919
    },
    {
        "text": "graph of our module uh our modules so",
        "start": 1410.24,
        "duration": 5.52
    },
    {
        "text": "basically which operations like python",
        "start": 1413.039,
        "duration": 5.441
    },
    {
        "text": "implemented uh kind of we were called",
        "start": 1415.76,
        "duration": 3.36
    },
    {
        "text": "inside",
        "start": 1418.48,
        "duration": 3.439
    },
    {
        "text": "there our module 7 module kind of can be",
        "start": 1419.12,
        "duration": 4.0
    },
    {
        "text": "seen in the graph form",
        "start": 1421.919,
        "duration": 3.12
    },
    {
        "text": "uh i mean it's not very interesting for",
        "start": 1423.12,
        "duration": 3.439
    },
    {
        "text": "our model but it's probably more",
        "start": 1425.039,
        "duration": 2.64
    },
    {
        "text": "interesting if if there are",
        "start": 1426.559,
        "duration": 2.561
    },
    {
        "text": "more complicated structures and you want",
        "start": 1427.679,
        "duration": 3.201
    },
    {
        "text": "to want to see kind of",
        "start": 1429.12,
        "duration": 4.24
    },
    {
        "text": "graph representation instead of the code",
        "start": 1430.88,
        "duration": 3.76
    },
    {
        "text": "again tensorboard",
        "start": 1433.36,
        "duration": 3.439
    },
    {
        "text": "the board is awesome uh it's a it's it's",
        "start": 1434.64,
        "duration": 3.44
    },
    {
        "text": "a great project and i mean there are",
        "start": 1436.799,
        "duration": 2.721
    },
    {
        "text": "like much more visualization",
        "start": 1438.08,
        "duration": 3.599
    },
    {
        "text": "integrations there so if you have you",
        "start": 1439.52,
        "duration": 4.0
    },
    {
        "text": "know embedded visualizations videos",
        "start": 1441.679,
        "duration": 3.521
    },
    {
        "text": "something like this",
        "start": 1443.52,
        "duration": 4.56
    },
    {
        "text": "that might be a good tool",
        "start": 1445.2,
        "duration": 2.88
    },
    {
        "text": "so so i mean",
        "start": 1449.12,
        "duration": 5.039
    },
    {
        "text": "apart from tender board we can also just",
        "start": 1452.24,
        "duration": 3.76
    },
    {
        "text": "use any part of",
        "start": 1454.159,
        "duration": 4.721
    },
    {
        "text": "a regular you know python scientific",
        "start": 1456.0,
        "duration": 3.84
    },
    {
        "text": "ecosystem",
        "start": 1458.88,
        "duration": 3.12
    },
    {
        "text": "so in this case like let's try to do our",
        "start": 1459.84,
        "duration": 4.719
    },
    {
        "text": "own our own research",
        "start": 1462.0,
        "duration": 5.76
    },
    {
        "text": "this is just some multi-clip code which",
        "start": 1464.559,
        "duration": 6.961
    },
    {
        "text": "uh like visualizes predicted classes is",
        "start": 1467.76,
        "duration": 4.0
    },
    {
        "text": "a",
        "start": 1471.52,
        "duration": 3.279
    },
    {
        "text": "visa cube graph so if i want to do if i",
        "start": 1471.76,
        "duration": 4.96
    },
    {
        "text": "want to explore what how my model works",
        "start": 1474.799,
        "duration": 3.76
    },
    {
        "text": "for example i can take test test data",
        "start": 1476.72,
        "duration": 4.24
    },
    {
        "text": "set run my model just like how we did it",
        "start": 1478.559,
        "duration": 3.441
    },
    {
        "text": "before",
        "start": 1480.96,
        "duration": 3.68
    },
    {
        "text": "and uh for example pain like predicted",
        "start": 1482.0,
        "duration": 4.32
    },
    {
        "text": "class for every image so our model",
        "start": 1484.64,
        "duration": 2.399
    },
    {
        "text": "actually learned",
        "start": 1486.32,
        "duration": 3.2
    },
    {
        "text": "some stuff so it has about like almost",
        "start": 1487.039,
        "duration": 3.281
    },
    {
        "text": "90 percent",
        "start": 1489.52,
        "duration": 3.039
    },
    {
        "text": "accuracy right so you can say that it's",
        "start": 1490.32,
        "duration": 3.599
    },
    {
        "text": "pretty obvious like fans",
        "start": 1492.559,
        "duration": 3.12
    },
    {
        "text": "it does it does recognize this as",
        "start": 1493.919,
        "duration": 3.441
    },
    {
        "text": "trousers uh",
        "start": 1495.679,
        "duration": 4.561
    },
    {
        "text": "this is hard to say even for me what the",
        "start": 1497.36,
        "duration": 3.84
    },
    {
        "text": "type of clothes",
        "start": 1500.24,
        "duration": 3.679
    },
    {
        "text": "it is but for something like i mean for",
        "start": 1501.2,
        "duration": 3.68
    },
    {
        "text": "something like dress",
        "start": 1503.919,
        "duration": 3.36
    },
    {
        "text": "or forever it's pretty applicable",
        "start": 1504.88,
        "duration": 5.6
    },
    {
        "text": "classification predictions",
        "start": 1507.279,
        "duration": 6.561
    },
    {
        "text": "and as we uh as",
        "start": 1510.48,
        "duration": 6.319
    },
    {
        "text": "as you mentioned before uh basically our",
        "start": 1513.84,
        "duration": 4.319
    },
    {
        "text": "model",
        "start": 1516.799,
        "duration": 2.961
    },
    {
        "text": "all learning is kind of captured in our",
        "start": 1518.159,
        "duration": 3.361
    },
    {
        "text": "model's parameters so there is this",
        "start": 1519.76,
        "duration": 3.12
    },
    {
        "text": "helpful method",
        "start": 1521.52,
        "duration": 2.88
    },
    {
        "text": "which is basically just returns your",
        "start": 1522.88,
        "duration": 3.279
    },
    {
        "text": "dictionary of the promise inside the",
        "start": 1524.4,
        "duration": 2.72
    },
    {
        "text": "model",
        "start": 1526.159,
        "duration": 4.081
    },
    {
        "text": "so all that our model learned is",
        "start": 1527.12,
        "duration": 5.679
    },
    {
        "text": "basically those in those two tensors",
        "start": 1530.24,
        "duration": 3.439
    },
    {
        "text": "which means that",
        "start": 1532.799,
        "duration": 3.12
    },
    {
        "text": "if i for example want to save the state",
        "start": 1533.679,
        "duration": 3.201
    },
    {
        "text": "of my model",
        "start": 1535.919,
        "duration": 3.041
    },
    {
        "text": "so i can later deploy it somewhere or",
        "start": 1536.88,
        "duration": 3.36
    },
    {
        "text": "just want to kind of",
        "start": 1538.96,
        "duration": 3.28
    },
    {
        "text": "save a good model which i've obtained i",
        "start": 1540.24,
        "duration": 3.36
    },
    {
        "text": "can take this",
        "start": 1542.24,
        "duration": 3.6
    },
    {
        "text": "uh i can take this dictionary and",
        "start": 1543.6,
        "duration": 3.28
    },
    {
        "text": "basically do",
        "start": 1545.84,
        "duration": 2.959
    },
    {
        "text": "that save on it which is which you can",
        "start": 1546.88,
        "duration": 3.12
    },
    {
        "text": "think of is like",
        "start": 1548.799,
        "duration": 2.88
    },
    {
        "text": "a kind of more optimized typical",
        "start": 1550.0,
        "duration": 3.679
    },
    {
        "text": "implementation which can understand",
        "start": 1551.679,
        "duration": 4.641
    },
    {
        "text": "python sensors save it into a file and",
        "start": 1553.679,
        "duration": 3.281
    },
    {
        "text": "later on",
        "start": 1556.32,
        "duration": 4.239
    },
    {
        "text": "basically like load from uh load from",
        "start": 1556.96,
        "duration": 5.839
    },
    {
        "text": "static as well by taking like modeling",
        "start": 1560.559,
        "duration": 4.961
    },
    {
        "text": "and doing a lot from cd later on",
        "start": 1562.799,
        "duration": 5.521
    },
    {
        "text": "uh so this way of saving python models",
        "start": 1565.52,
        "duration": 4.56
    },
    {
        "text": "basically just saves the parameters so",
        "start": 1568.32,
        "duration": 3.92
    },
    {
        "text": "if you if you do that and you kind of",
        "start": 1570.08,
        "duration": 3.52
    },
    {
        "text": "want to later on your model you still",
        "start": 1572.24,
        "duration": 2.88
    },
    {
        "text": "need to have the source code of your",
        "start": 1573.6,
        "duration": 3.679
    },
    {
        "text": "model basically that",
        "start": 1575.12,
        "duration": 4.48
    },
    {
        "text": "class definition which we have had about",
        "start": 1577.279,
        "duration": 4.241
    },
    {
        "text": "which works in majority of cases",
        "start": 1579.6,
        "duration": 3.439
    },
    {
        "text": "but for cases when you actually want to",
        "start": 1581.52,
        "duration": 3.039
    },
    {
        "text": "package together both",
        "start": 1583.039,
        "duration": 4.561
    },
    {
        "text": "model and source codes and for example",
        "start": 1584.559,
        "duration": 5.921
    },
    {
        "text": "deployed for some embedded inference for",
        "start": 1587.6,
        "duration": 4.4
    },
    {
        "text": "deploying some environment which might",
        "start": 1590.48,
        "duration": 3.76
    },
    {
        "text": "not even have like python uh there is",
        "start": 1592.0,
        "duration": 6.4
    },
    {
        "text": "this whole uh part of pytorch called",
        "start": 1594.24,
        "duration": 4.16
    },
    {
        "text": "also so you can you can basically to do",
        "start": 1600.88,
        "duration": 5.039
    },
    {
        "text": "that script on the model so what it's",
        "start": 1604.32,
        "duration": 3.12
    },
    {
        "text": "going to do it's going to like",
        "start": 1605.919,
        "duration": 4.481
    },
    {
        "text": "go find all the source source code bits",
        "start": 1607.44,
        "duration": 3.839
    },
    {
        "text": "which you needed",
        "start": 1610.4,
        "duration": 2.32
    },
    {
        "text": "in order to run your model not ext",
        "start": 1611.279,
        "duration": 2.88
    },
    {
        "text": "initialize it but just translate the",
        "start": 1612.72,
        "duration": 2.559
    },
    {
        "text": "forward parts",
        "start": 1614.159,
        "duration": 2.961
    },
    {
        "text": "uh take all the state of the model so",
        "start": 1615.279,
        "duration": 3.841
    },
    {
        "text": "basically all the kind of classes",
        "start": 1617.12,
        "duration": 3.919
    },
    {
        "text": "uh which and the module classes which",
        "start": 1619.12,
        "duration": 3.28
    },
    {
        "text": "your model consists of",
        "start": 1621.039,
        "duration": 3.281
    },
    {
        "text": "and basically saves them in our own like",
        "start": 1622.4,
        "duration": 3.279
    },
    {
        "text": "self-contained package",
        "start": 1624.32,
        "duration": 4.239
    },
    {
        "text": "so that that package is basically it",
        "start": 1625.679,
        "duration": 4.561
    },
    {
        "text": "doesn't depend on python at all so you",
        "start": 1628.559,
        "duration": 4.081
    },
    {
        "text": "can even use cpus api to deploy it",
        "start": 1630.24,
        "duration": 4.64
    },
    {
        "text": "uh so if you later on want to you know",
        "start": 1632.64,
        "duration": 3.919
    },
    {
        "text": "run it on your mobile phone or you want",
        "start": 1634.88,
        "duration": 2.399
    },
    {
        "text": "to",
        "start": 1636.559,
        "duration": 3.12
    },
    {
        "text": "run it on your embedded into some",
        "start": 1637.279,
        "duration": 3.28
    },
    {
        "text": "application",
        "start": 1639.679,
        "duration": 2.081
    },
    {
        "text": "you might have and you don't want to",
        "start": 1640.559,
        "duration": 3.041
    },
    {
        "text": "kind of carry pythonx system is that",
        "start": 1641.76,
        "duration": 3.76
    },
    {
        "text": "that's a pretty useful way uh kind of",
        "start": 1643.6,
        "duration": 3.6
    },
    {
        "text": "useful for deployment",
        "start": 1645.52,
        "duration": 3.039
    },
    {
        "text": "and that's that's really cool right",
        "start": 1647.2,
        "duration": 2.959
    },
    {
        "text": "because like basically in the state",
        "start": 1648.559,
        "duration": 2.881
    },
    {
        "text": "dictionary you have all of the",
        "start": 1650.159,
        "duration": 2.721
    },
    {
        "text": "parameters",
        "start": 1651.44,
        "duration": 2.88
    },
    {
        "text": "and everything that it learned in there",
        "start": 1652.88,
        "duration": 2.88
    },
    {
        "text": "so you can do other things with it too",
        "start": 1654.32,
        "duration": 3.92
    },
    {
        "text": "if you like",
        "start": 1655.76,
        "duration": 4.72
    },
    {
        "text": "you know i mean there are a lot of",
        "start": 1658.24,
        "duration": 3.2
    },
    {
        "text": "libraries for you know like",
        "start": 1660.48,
        "duration": 2.0
    },
    {
        "text": "understanding",
        "start": 1661.44,
        "duration": 2.88
    },
    {
        "text": "in your neural network someone else",
        "start": 1662.48,
        "duration": 3.919
    },
    {
        "text": "ecosystem some of them kind of once",
        "start": 1664.32,
        "duration": 4.4
    },
    {
        "text": "so there is you can basically look at",
        "start": 1666.399,
        "duration": 3.441
    },
    {
        "text": "that you can",
        "start": 1668.72,
        "duration": 2.72
    },
    {
        "text": "transfer it to a different model and",
        "start": 1669.84,
        "duration": 3.52
    },
    {
        "text": "sometimes people even if you do",
        "start": 1671.44,
        "duration": 4.719
    },
    {
        "text": "kind of fine tuning or if you if you are",
        "start": 1673.36,
        "duration": 3.439
    },
    {
        "text": "doing",
        "start": 1676.159,
        "duration": 2.24
    },
    {
        "text": "you know like transfer learning so you",
        "start": 1676.799,
        "duration": 3.841
    },
    {
        "text": "basically want to take some parameters",
        "start": 1678.399,
        "duration": 3.76
    },
    {
        "text": "from from the base model and you know",
        "start": 1680.64,
        "duration": 3.68
    },
    {
        "text": "like add a few layers and train it",
        "start": 1682.159,
        "duration": 3.76
    },
    {
        "text": "once more on some data set which is more",
        "start": 1684.32,
        "duration": 3.68
    },
    {
        "text": "specialized people do",
        "start": 1685.919,
        "duration": 3.441
    },
    {
        "text": "a lot of that to positive image or",
        "start": 1688.0,
        "duration": 3.36
    },
    {
        "text": "especially in nlp it's like fine-tuning",
        "start": 1689.36,
        "duration": 2.72
    },
    {
        "text": "transformers",
        "start": 1691.36,
        "duration": 3.12
    },
    {
        "text": "now it is now very popular it's very",
        "start": 1692.08,
        "duration": 4.0
    },
    {
        "text": "natural how to do that because",
        "start": 1694.48,
        "duration": 3.04
    },
    {
        "text": "you can basically just see what the",
        "start": 1696.08,
        "duration": 3.28
    },
    {
        "text": "parameters of your model are this is a",
        "start": 1697.52,
        "duration": 3.36
    },
    {
        "text": "python dictionary you can like",
        "start": 1699.36,
        "duration": 4.0
    },
    {
        "text": "go tweak it if you want so that kind of",
        "start": 1700.88,
        "duration": 3.76
    },
    {
        "text": "fits into general",
        "start": 1703.36,
        "duration": 3.039
    },
    {
        "text": "uh point of view being like very",
        "start": 1704.64,
        "duration": 3.68
    },
    {
        "text": "explicit and kind of being very",
        "start": 1706.399,
        "duration": 4.561
    },
    {
        "text": "kind of embedded in the python python",
        "start": 1708.32,
        "duration": 3.599
    },
    {
        "text": "like a system and",
        "start": 1710.96,
        "duration": 2.88
    },
    {
        "text": "that's intuitive to use kind of as",
        "start": 1711.919,
        "duration": 3.441
    },
    {
        "text": "another demonstration for that like if",
        "start": 1713.84,
        "duration": 4.16
    },
    {
        "text": "we go back for example and",
        "start": 1715.36,
        "duration": 4.24
    },
    {
        "text": "go back to our model definition i mean",
        "start": 1718.0,
        "duration": 3.44
    },
    {
        "text": "this model is just a",
        "start": 1719.6,
        "duration": 4.16
    },
    {
        "text": "vital object right so i can do if i if",
        "start": 1721.44,
        "duration": 3.839
    },
    {
        "text": "i'm trying to debug something right i",
        "start": 1723.76,
        "duration": 2.08
    },
    {
        "text": "can",
        "start": 1725.279,
        "duration": 3.201
    },
    {
        "text": "basically go and like do a print",
        "start": 1725.84,
        "duration": 5.04
    },
    {
        "text": "statement here i can for example print",
        "start": 1728.48,
        "duration": 6.4
    },
    {
        "text": "shape of my shape of my tensor over here",
        "start": 1730.88,
        "duration": 6.159
    },
    {
        "text": "and shape my tensor over here and maybe",
        "start": 1734.88,
        "duration": 3.12
    },
    {
        "text": "like print",
        "start": 1737.039,
        "duration": 2.801
    },
    {
        "text": "whatever the result was right so now if",
        "start": 1738.0,
        "duration": 4.24
    },
    {
        "text": "i rerun my",
        "start": 1739.84,
        "duration": 5.28
    },
    {
        "text": "if i run my model and for example this",
        "start": 1742.24,
        "duration": 3.6
    },
    {
        "text": "part",
        "start": 1745.12,
        "duration": 2.48
    },
    {
        "text": "you can see that whatever like print",
        "start": 1745.84,
        "duration": 3.04
    },
    {
        "text": "statements we put",
        "start": 1747.6,
        "duration": 3.6
    },
    {
        "text": "there i just run as a part of like",
        "start": 1748.88,
        "duration": 3.919
    },
    {
        "text": "regular applications there is no like",
        "start": 1751.2,
        "duration": 3.68
    },
    {
        "text": "there's no magic this is basically just",
        "start": 1752.799,
        "duration": 3.12
    },
    {
        "text": "a python program",
        "start": 1754.88,
        "duration": 2.72
    },
    {
        "text": "and this is really powerful because i",
        "start": 1755.919,
        "duration": 3.921
    },
    {
        "text": "can i could put like",
        "start": 1757.6,
        "duration": 4.48
    },
    {
        "text": "a debugger here i could invoke pdb here",
        "start": 1759.84,
        "duration": 3.12
    },
    {
        "text": "i can involve",
        "start": 1762.08,
        "duration": 3.44
    },
    {
        "text": "any kind of party library if i wanted to",
        "start": 1762.96,
        "duration": 4.48
    },
    {
        "text": "and which really makes my torch kind of",
        "start": 1765.52,
        "duration": 3.6
    },
    {
        "text": "a pluggable library with the rest of the",
        "start": 1767.44,
        "duration": 5.68
    },
    {
        "text": "system and there are people kind of",
        "start": 1769.12,
        "duration": 4.0
    },
    {
        "text": "exploiting exploiting this functionality",
        "start": 1773.2,
        "duration": 3.04
    },
    {
        "text": "for pretty much",
        "start": 1774.72,
        "duration": 2.959
    },
    {
        "text": "connecting it to all the different",
        "start": 1776.24,
        "duration": 2.88
    },
    {
        "text": "domains or all the different libraries",
        "start": 1777.679,
        "duration": 3.12
    },
    {
        "text": "from other parts",
        "start": 1779.12,
        "duration": 3.279
    },
    {
        "text": "ranging from you know some more like",
        "start": 1780.799,
        "duration": 3.041
    },
    {
        "text": "domains for deployment to like",
        "start": 1782.399,
        "duration": 4.241
    },
    {
        "text": "optimizing differential equations and",
        "start": 1783.84,
        "duration": 3.52
    },
    {
        "text": "stuff",
        "start": 1786.64,
        "duration": 4.08
    },
    {
        "text": "and stuff like that in kind of more",
        "start": 1787.36,
        "duration": 4.24
    },
    {
        "text": "scientific compute",
        "start": 1790.72,
        "duration": 2.16
    },
    {
        "text": "areas all right so let's go down all the",
        "start": 1791.6,
        "duration": 2.88
    },
    {
        "text": "way to the end just to make sure and",
        "start": 1792.88,
        "duration": 2.159
    },
    {
        "text": "just to",
        "start": 1794.48,
        "duration": 2.799
    },
    {
        "text": "wrap this up basically if you want to",
        "start": 1795.039,
        "duration": 4.401
    },
    {
        "text": "save and load the model you have those",
        "start": 1797.279,
        "duration": 3.681
    },
    {
        "text": "uh you basically save the state",
        "start": 1799.44,
        "duration": 4.08
    },
    {
        "text": "dictionary it does a checkpoint",
        "start": 1800.96,
        "duration": 5.04
    },
    {
        "text": "and then you can load it up uh load up a",
        "start": 1803.52,
        "duration": 6.32
    },
    {
        "text": "saved model as well right",
        "start": 1806.0,
        "duration": 3.84
    },
    {
        "text": "from dick state dictionary and that's",
        "start": 1809.919,
        "duration": 4.0
    },
    {
        "text": "pretty much would be it",
        "start": 1812.64,
        "duration": 3.2
    },
    {
        "text": "and uh in this case i actually this is",
        "start": 1813.919,
        "duration": 3.281
    },
    {
        "text": "actually a bundle which i can",
        "start": 1815.84,
        "duration": 3.839
    },
    {
        "text": "load back and it will uh it will be just",
        "start": 1817.2,
        "duration": 3.199
    },
    {
        "text": "self-contained",
        "start": 1819.679,
        "duration": 2.161
    },
    {
        "text": "which i can do in separate you know",
        "start": 1820.399,
        "duration": 3.361
    },
    {
        "text": "python process or even cpu process",
        "start": 1821.84,
        "duration": 4.48
    },
    {
        "text": "without even having having to have the",
        "start": 1823.76,
        "duration": 3.279
    },
    {
        "text": "source",
        "start": 1826.32,
        "duration": 2.64
    },
    {
        "text": "of the model available because also the",
        "start": 1827.039,
        "duration": 3.281
    },
    {
        "text": "model is actually packaged in the same",
        "start": 1828.96,
        "duration": 2.48
    },
    {
        "text": "archive",
        "start": 1830.32,
        "duration": 3.2
    },
    {
        "text": "so that's that's i mean that's pretty",
        "start": 1831.44,
        "duration": 3.359
    },
    {
        "text": "much wraps up our",
        "start": 1833.52,
        "duration": 3.6
    },
    {
        "text": "short demo uh if you want to learn if",
        "start": 1834.799,
        "duration": 3.36
    },
    {
        "text": "you want to learn more",
        "start": 1837.12,
        "duration": 3.36
    },
    {
        "text": "go to python.org it has a lot of",
        "start": 1838.159,
        "duration": 3.841
    },
    {
        "text": "tutorials and vlogs and",
        "start": 1840.48,
        "duration": 3.6
    },
    {
        "text": "recommendations and pretty much all the",
        "start": 1842.0,
        "duration": 3.919
    },
    {
        "text": "tutorials are structured as",
        "start": 1844.08,
        "duration": 4.959
    },
    {
        "text": "also i fight on notebooks so you can",
        "start": 1845.919,
        "duration": 5.36
    },
    {
        "text": "run them you can run them it's easy to",
        "start": 1849.039,
        "duration": 3.041
    },
    {
        "text": "kind of",
        "start": 1851.279,
        "duration": 3.12
    },
    {
        "text": "tweak fix them and get started uh also i",
        "start": 1852.08,
        "duration": 4.0
    },
    {
        "text": "mean i talked a little bit about service",
        "start": 1854.399,
        "duration": 3.841
    },
    {
        "text": "for example that's pro that's probably",
        "start": 1856.08,
        "duration": 3.199
    },
    {
        "text": "for another demo",
        "start": 1858.24,
        "duration": 3.439
    },
    {
        "text": "but you can go to python surf and like",
        "start": 1859.279,
        "duration": 3.12
    },
    {
        "text": "grab kind of",
        "start": 1861.679,
        "duration": 3.041
    },
    {
        "text": "model server implementation where we can",
        "start": 1862.399,
        "duration": 3.361
    },
    {
        "text": "take this model",
        "start": 1864.72,
        "duration": 2.559
    },
    {
        "text": "and pretty much with a few commands like",
        "start": 1865.76,
        "duration": 3.44
    },
    {
        "text": "wrap it in a",
        "start": 1867.279,
        "duration": 4.481
    },
    {
        "text": "in in a simple like a simple server",
        "start": 1869.2,
        "duration": 3.52
    },
    {
        "text": "which will expose",
        "start": 1871.76,
        "duration": 3.039
    },
    {
        "text": "rest and points so we can for example",
        "start": 1872.72,
        "duration": 3.04
    },
    {
        "text": "from our application",
        "start": 1874.799,
        "duration": 2.561
    },
    {
        "text": "start sending like requests and to",
        "start": 1875.76,
        "duration": 3.36
    },
    {
        "text": "influence our model somewhere",
        "start": 1877.36,
        "duration": 3.76
    },
    {
        "text": "in applications well dimitrov this has",
        "start": 1879.12,
        "duration": 3.679
    },
    {
        "text": "been really awesome i love going i love",
        "start": 1881.12,
        "duration": 3.36
    },
    {
        "text": "how we went through everything",
        "start": 1882.799,
        "duration": 3.281
    },
    {
        "text": "step by step thanks so much for spending",
        "start": 1884.48,
        "duration": 3.28
    },
    {
        "text": "some time with us and again those of you",
        "start": 1886.08,
        "duration": 2.8
    },
    {
        "text": "that are watching thank you so much for",
        "start": 1887.76,
        "duration": 2.32
    },
    {
        "text": "being with us you've been learning all",
        "start": 1888.88,
        "duration": 2.56
    },
    {
        "text": "about pytorch",
        "start": 1890.08,
        "duration": 3.52
    },
    {
        "text": "hands on thanks again to dimitro and",
        "start": 1891.44,
        "duration": 3.359
    },
    {
        "text": "hopefully we'll see you next time take",
        "start": 1893.6,
        "duration": 14.24
    },
    {
        "text": "care my friends",
        "start": 1894.799,
        "duration": 13.041
    },
    {
        "text": "you",
        "start": 1908.48,
        "duration": 2.079
    }
]